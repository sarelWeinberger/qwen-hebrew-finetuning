{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:33:01.764234Z",
     "start_time": "2025-08-26T09:32:57.945111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Block 1: Install dependencies and imports\n",
    "!pip install datasets\n",
    "!pip install openpyxl\n",
    "!pip install -q -U google-genai\n",
    "!pip install google-generativeai\n",
    "\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "from langdetect import detect\n",
    "\n",
    "\n"
   ],
   "id": "e6a5bdb52e9ddab3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (4.0.0)\r\n",
      "Requirement already satisfied: filelock in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from datasets) (3.18.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from datasets) (2.0.2)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from datasets) (20.0.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from datasets) (2.3.1)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from datasets) (2.32.4)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from datasets) (4.67.1)\r\n",
      "Requirement already satisfied: xxhash in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from datasets) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from datasets) (0.33.4)\r\n",
      "Requirement already satisfied: packaging in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from datasets) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from datasets) (6.0.2)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\r\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.14.1)\r\n",
      "Requirement already satisfied: idna>=2.0 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.4.2)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (1.26.20)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2025.7.14)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from pandas->datasets) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from pandas->datasets) (2025.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\r\n",
      "Requirement already satisfied: openpyxl in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (3.1.5)\r\n",
      "Requirement already satisfied: et-xmlfile in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from openpyxl) (2.0.0)\r\n",
      "Requirement already satisfied: google-generativeai in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (0.8.5)\r\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from google-generativeai) (0.6.15)\r\n",
      "Requirement already satisfied: google-api-core in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from google-generativeai) (2.25.1)\r\n",
      "Requirement already satisfied: google-api-python-client in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from google-generativeai) (2.179.0)\r\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from google-generativeai) (2.40.3)\r\n",
      "Requirement already satisfied: protobuf in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from google-generativeai) (5.29.5)\r\n",
      "Requirement already satisfied: pydantic in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from google-generativeai) (2.11.7)\r\n",
      "Requirement already satisfied: tqdm in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from google-generativeai) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from google-generativeai) (4.14.1)\r\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\r\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from google-api-core->google-generativeai) (1.70.0)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from google-api-core->google-generativeai) (2.32.4)\r\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\r\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (1.26.20)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.7.14)\r\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\r\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from google-api-python-client->google-generativeai) (0.22.0)\r\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from google-api-python-client->google-generativeai) (0.2.0)\r\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from google-api-python-client->google-generativeai) (4.2.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from pydantic->google-generativeai) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from pydantic->google-generativeai) (2.33.2)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/oribar-joseph/PycharmProjects/qwen-hebrew-finetuning1/.venv/lib/python3.9/site-packages (from pydantic->google-generativeai) (0.4.1)\r\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:36:42.704278Z",
     "start_time": "2025-08-26T09:36:40.924094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Stream the dataset (no full download)\n",
    "dataset = load_dataset(\"allenai/tulu-3-sft-mixture\", split=\"train\", streaming=True)\n",
    "\n",
    "from langdetect import detect_langs\n",
    "\n",
    "def is_english(messages, min_prob=0.90):\n",
    "    # Join all message content into one string\n",
    "    text = \" \".join([m.get(\"content\", \"\") for m in messages]).strip()\n",
    "    if not text:\n",
    "        return False\n",
    "    try:\n",
    "        # detect_langs returns a list like [en:0.99, es:0.01]\n",
    "        langs = detect_langs(text)\n",
    "        for lang in langs:\n",
    "            if lang.lang == \"en\" and lang.prob >= min_prob:\n",
    "                return True\n",
    "        return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Filter as a generator\n",
    "ds = (example for example in dataset if is_english(example[\"messages\"]))\n",
    "\n",
    "print(\"English filtering applied successfully\")"
   ],
   "id": "c281c57f3fc545c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English filtering applied successfully\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:36:42.711170Z",
     "start_time": "2025-08-26T09:36:42.707743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Block 3: Extract questions and answers from Tulu dataset\n",
    "def extract_qa_from_tulu(item):\n",
    "    \"\"\"Extract question and answer from Tulu dataset item\"\"\"\n",
    "    try:\n",
    "        # Tulu dataset typically has 'messages' field with conversation format\n",
    "        if 'messages' in item:\n",
    "            messages = item['messages']\n",
    "            question = \"\"\n",
    "            answer = \"\"\n",
    "\n",
    "            for msg in messages:\n",
    "                if msg.get('role') == 'user':\n",
    "                    question = msg.get('content', '')\n",
    "                elif msg.get('role') == 'assistant':\n",
    "                    answer = msg.get('content', '')\n",
    "                    break  # Take first assistant response\n",
    "\n",
    "            return question, answer\n",
    "\n",
    "        # Alternative structure - check for other common fields\n",
    "        elif 'prompt' in item and 'completion' in item:\n",
    "            return item['prompt'], item['completion']\n",
    "        elif 'input' in item and 'output' in item:\n",
    "            return item['input'], item['output']\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting Q&A: {e}\")\n",
    "        return None, None"
   ],
   "id": "62dd111940e92701",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:36:42.730762Z",
     "start_time": "2025-08-26T09:36:42.725903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_diverse_tulu_examples(ds, n_samples=5, total_samples=5000):\n",
    "    \"\"\"Get diverse examples from Tulu dataset\"\"\"\n",
    "\n",
    "    # Initialize buckets for different content lengths\n",
    "    buckets = {\n",
    "        'short': [],   # 0-300 chars\n",
    "        'medium': [],  # 300-1000 chars\n",
    "        'long': [],    # 1000+ chars\n",
    "    }\n",
    "\n",
    "    count = 0\n",
    "    for item in ds:\n",
    "        if count >= total_samples:\n",
    "            break\n",
    "\n",
    "        question, answer = extract_qa_from_tulu(item)\n",
    "\n",
    "        if question and answer:\n",
    "            content_length = len(question) + len(answer)\n",
    "\n",
    "            # Determine bucket\n",
    "            if content_length < 300:\n",
    "                bucket_name = 'short'\n",
    "            elif content_length < 1000:\n",
    "                bucket_name = 'medium'\n",
    "            else:\n",
    "                bucket_name = 'long'\n",
    "\n",
    "            # Add to bucket (keep max 100 per bucket)\n",
    "            bucket = buckets[bucket_name]\n",
    "            if len(bucket) < 100:\n",
    "                bucket.append((question, answer))\n",
    "            else:\n",
    "                # Replace random item (reservoir sampling)\n",
    "                replace_idx = random.randint(0, len(bucket))\n",
    "                if replace_idx < len(bucket):\n",
    "                    bucket[replace_idx] = (question, answer)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    # Select examples from buckets for diversity\n",
    "    diverse_examples = []\n",
    "    for bucket_name in ['short', 'medium', 'long']:\n",
    "        if buckets[bucket_name]:\n",
    "            samples_from_bucket = min(len(buckets[bucket_name]), max(1, n_samples // 3))\n",
    "            diverse_examples.extend(random.sample(buckets[bucket_name], samples_from_bucket))\n",
    "\n",
    "    # Fill remaining slots\n",
    "    while len(diverse_examples) < n_samples:\n",
    "        for bucket in buckets.values():\n",
    "            if bucket and len(diverse_examples) < n_samples:\n",
    "                remaining = [ex for ex in bucket if ex not in diverse_examples]\n",
    "                if remaining:\n",
    "                    diverse_examples.append(random.choice(remaining))\n",
    "\n",
    "    return diverse_examples[:n_samples]"
   ],
   "id": "885694f642cdfdff",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:37:26.790995Z",
     "start_time": "2025-08-26T09:36:42.736004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Block 5: Get examples and separate into questions/answers\n",
    "examples = get_diverse_tulu_examples(ds, n_samples=5, total_samples=5000)\n",
    "\n",
    "print(f\"Selected {len(examples)} diverse examples:\")\n",
    "for i, (q, a) in enumerate(examples):\n",
    "    print(f\"Example {i + 1}: Q length = {len(q)} chars, A length = {len(a)} chars\")\n",
    "\n",
    "questions = [ex[0] for ex in examples]\n",
    "answers = [ex[1] for ex in examples]\n"
   ],
   "id": "95e5b4382ae98060",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 5 diverse examples:\n",
      "Example 1: Q length = 255 chars, A length = 19 chars\n",
      "Example 2: Q length = 829 chars, A length = 131 chars\n",
      "Example 3: Q length = 1364 chars, A length = 7 chars\n",
      "Example 4: Q length = 161 chars, A length = 28 chars\n",
      "Example 5: Q length = 709 chars, A length = 59 chars\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:37:26.862703Z",
     "start_time": "2025-08-26T09:37:26.860409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Block 6: Setup Gemini API\n",
    "GEMINI_API_KEY = \"AIzaSyCtsfF_f6isWzN3B5wrUEDhaE1IujdoOnQ\"  # Replace with your actual API key\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "model = genai.GenerativeModel('gemini-2.5-pro')"
   ],
   "id": "4f450559eb4e7a22",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:37:40.231553Z",
     "start_time": "2025-08-26T09:37:40.228110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Block 7: Translation functions\n",
    "def translate_to_hebrew(text, text_type=\"text\"):\n",
    "    \"\"\"Translate text to Hebrew using Gemini API\"\"\"\n",
    "    try:\n",
    "        if text_type == \"question\":\n",
    "            prompt = f\"Translate the following question to Hebrew. Keep any mathematical expressions, code, or technical terms unchanged. Only translate the natural language portions:\\n\\n{text}\"\n",
    "        elif text_type == \"answer\":\n",
    "            prompt = f\"Translate the following answer to Hebrew. Keep any mathematical expressions, code, formulas, or technical terms unchanged. Only translate the explanatory text:\\n\\n{text}\"\n",
    "        else:\n",
    "            prompt = f\"Translate the following to Hebrew:\\n\\n{text}\"\n",
    "\n",
    "        response = model.generate_content(prompt)\n",
    "        time.sleep(0.5)  # Rate limiting\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return text"
   ],
   "id": "974c2a1e63cd4433",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:37:41.163059Z",
     "start_time": "2025-08-26T09:37:41.158868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_hebrew_cot_prompt(english_questions, english_answers, new_english_question):\n",
    "    \"\"\"Create prompt for generating Hebrew CoT explanations\"\"\"\n",
    "\n",
    "    prompt = \"\"\"You are an expert tutor who provides detailed step-by-step explanations in Hebrew. Given questions and their answers in English, you create comprehensive Hebrew explanations that show the complete reasoning process.\n",
    "\n",
    "Your Hebrew explanations should be:\n",
    "- Detailed and show all reasoning steps\n",
    "- Clear and easy to follow\n",
    "- Use proper Hebrew terminology\n",
    "- Keep mathematical expressions, code, and technical terms in their original form\n",
    "- Provide step-by-step logical progression\n",
    "\n",
    "Here are examples:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Add few-shot examples\n",
    "    for i in range(len(english_questions)):\n",
    "        prompt += f\"\"\"Example {i+1}:\n",
    "English Question: {english_questions[i]}\n",
    "\n",
    "English Answer: {english_answers[i]}\n",
    "\n",
    "Hebrew Step-by-Step Explanation:\n",
    "[This would be filled with a detailed Hebrew explanation showing the reasoning process]\n",
    "\n",
    "---\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    prompt += f\"\"\"Now provide a detailed Hebrew step-by-step explanation for this question:\n",
    "\n",
    "English Question: {new_english_question}\n",
    "\n",
    "Hebrew Step-by-Step Explanation:\"\"\"\n",
    "\n",
    "    return prompt\n"
   ],
   "id": "f9c5e931b7bab45",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:40:52.197449Z",
     "start_time": "2025-08-26T09:40:52.189953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_tulu_hebrew_dataset(english_questions, english_answers, num_questions=10):\n",
    "    \"\"\"Generate Hebrew CoT dataset from Tulu examples\"\"\"\n",
    "\n",
    "    # Load fresh dataset stream\n",
    "    ds_fresh = load_dataset(\"allenai/tulu-3-sft-mixture\", split=\"train\", streaming=True)\n",
    "    ds_fresh = (example for example in ds_fresh if is_english(example[\"messages\"]))\n",
    "    results = []\n",
    "    count = 0\n",
    "    processed = 0\n",
    "\n",
    "    print(f\"Processing {num_questions} questions from Tulu dataset...\")\n",
    "\n",
    "    for item in ds_fresh:\n",
    "        if processed >= num_questions:\n",
    "            break\n",
    "\n",
    "        question, answer = extract_qa_from_tulu(item)\n",
    "\n",
    "        if question and answer and len(question.strip()) > 10 and len(answer.strip()) > 10:\n",
    "            try:\n",
    "                print(f\"Processing question {processed+1}/{num_questions}\")\n",
    "                print(f\"Question preview: {question[:100]}...\")\n",
    "\n",
    "                # Generate Hebrew CoT using few-shot learning\n",
    "                cot_prompt = create_hebrew_cot_prompt(english_questions, english_answers, question)\n",
    "                response = model.generate_content(cot_prompt)\n",
    "                hebrew_cot = response.text\n",
    "\n",
    "                # Store result\n",
    "                result = {\n",
    "                    'question_english': question,\n",
    "                    'answer_english': answer,\n",
    "                    'cot_hebrew': hebrew_cot,\n",
    "                    'question_hebrew': '',  # Will fill later\n",
    "                    'answer_hebrew': ''     # Will fill later\n",
    "                }\n",
    "\n",
    "                results.append(result)\n",
    "                print(f\"✓ Generated Hebrew CoT for question {processed+1}\")\n",
    "\n",
    "                processed += 1\n",
    "                time.sleep(1)  # Rate limiting\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error processing question {processed+1}: {e}\")\n",
    "                continue\n",
    "\n",
    "        count += 1\n",
    "        if count >= 10000:  # Safety limit\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(results)"
   ],
   "id": "31baa56415cbeb8b",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:47:11.116382Z",
     "start_time": "2025-08-26T09:40:53.371727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hebrew_dataset = generate_tulu_hebrew_dataset(\n",
    "    english_questions=questions,\n",
    "    english_answers=answers,\n",
    "    num_questions=10\n",
    ")"
   ],
   "id": "e202e81f2f462bd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10 questions from Tulu dataset...\n",
      "Processing question 1/10\n",
      "Question preview: Create a snippet of Terraform HCL code that create an AWS autoscaling group, and an ALB in front to ...\n",
      "✓ Generated Hebrew CoT for question 1\n",
      "Processing question 2/10\n",
      "Question preview: Which languages are spoken in Costa Rica?...\n",
      "✓ Generated Hebrew CoT for question 2\n",
      "Processing question 3/10\n",
      "Question preview: Can u summarize me story from the book Harry Potter and the Philosopher's Stone?...\n",
      "✓ Generated Hebrew CoT for question 3\n",
      "Processing question 4/10\n",
      "Question preview: Did Jesus realy exist...\n",
      "✓ Generated Hebrew CoT for question 4\n",
      "Processing question 5/10\n",
      "Question preview: ¿Puedes darme un ejemplo del patrón de diseño Factory en el lenguaje de programación Java?...\n",
      "✓ Generated Hebrew CoT for question 5\n",
      "Processing question 6/10\n",
      "Question preview: Write five lines of iambic pentameter about a subject of your choosing. Do not use any trochaic subs...\n",
      "✓ Generated Hebrew CoT for question 6\n",
      "Processing question 7/10\n",
      "Question preview: You will create a table with macronutrients, micronutrients and kcal of the following foods: 100g oa...\n",
      "✓ Generated Hebrew CoT for question 7\n",
      "Processing question 8/10\n",
      "Question preview: Write me POSIX-shellscript to convert Celsius to Kelvin....\n",
      "✓ Generated Hebrew CoT for question 8\n",
      "Processing question 9/10\n",
      "Question preview: How do I compile a Java application to native with GraalVM inside the linux terminal?...\n",
      "✓ Generated Hebrew CoT for question 9\n",
      "Processing question 10/10\n",
      "Question preview: In JavaScript, why is the value of \"this\" keyword inside an arrow function determined by the context...\n",
      "✓ Generated Hebrew CoT for question 10\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:56:05.818539Z",
     "start_time": "2025-08-26T09:47:34.721907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Translating questions to Hebrew...\")\n",
    "hebrew_questions = []\n",
    "for i, question in enumerate(hebrew_dataset['question_english']):\n",
    "    print(f\"Translating question {i+1}/{len(hebrew_dataset)}\")\n",
    "    hebrew_questions.append(translate_to_hebrew(question, \"question\"))\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(\"Translating answers to Hebrew...\")\n",
    "hebrew_answers = []\n",
    "for i, answer in enumerate(hebrew_dataset['answer_english']):\n",
    "    print(f\"Translating answer {i+1}/{len(hebrew_dataset)}\")\n",
    "    hebrew_answers.append(translate_to_hebrew(answer, \"answer\"))\n",
    "    time.sleep(0.5)\n"
   ],
   "id": "22ad384df8a0bf11",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating questions to Hebrew...\n",
      "Translating question 1/10\n",
      "Translating question 2/10\n",
      "Translating question 3/10\n",
      "Translating question 4/10\n",
      "Translating question 5/10\n",
      "Translating question 6/10\n",
      "Translating question 7/10\n",
      "Translating question 8/10\n",
      "Translating question 9/10\n",
      "Translating question 10/10\n",
      "Translating answers to Hebrew...\n",
      "Translating answer 1/10\n",
      "Translating answer 2/10\n",
      "Translating answer 3/10\n",
      "Translating answer 4/10\n",
      "Translating answer 5/10\n",
      "Translating answer 6/10\n",
      "Translating answer 7/10\n",
      "Translating answer 8/10\n",
      "Translating answer 9/10\n",
      "Translating answer 10/10\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:56:12.653566Z",
     "start_time": "2025-08-26T09:56:12.650244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hebrew_dataset['question_hebrew'] = hebrew_questions\n",
    "hebrew_dataset['answer_hebrew'] = hebrew_answers"
   ],
   "id": "6df60e6dfaba0a8c",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:56:13.510148Z",
     "start_time": "2025-08-26T09:56:13.504249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "final_dataset = hebrew_dataset[[\n",
    "    'question_english',\n",
    "    'answer_english',\n",
    "    'question_hebrew',\n",
    "    'cot_hebrew',\n",
    "    'answer_hebrew'\n",
    "]]\n",
    "\n",
    "print(\"Final dataset structure:\")\n",
    "print(final_dataset.head())"
   ],
   "id": "27de09e30b180439",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset structure:\n",
      "                                    question_english  \\\n",
      "0  Create a snippet of Terraform HCL code that cr...   \n",
      "1          Which languages are spoken in Costa Rica?   \n",
      "2  Can u summarize me story from the book Harry P...   \n",
      "3                              Did Jesus realy exist   \n",
      "4  ¿Puedes darme un ejemplo del patrón de diseño ...   \n",
      "\n",
      "                                      answer_english  \\\n",
      "0  Sure, here's an example Terraform HCL code tha...   \n",
      "1  The primary language spoken in Costa Rica is S...   \n",
      "2  Harry Potter, an orphan, discovers he is a wiz...   \n",
      "3  As an AI language model, I cannot provide a de...   \n",
      "4  interface Shape {\\n   void draw();\\n}\\n\\nclass...   \n",
      "\n",
      "                                     question_hebrew  \\\n",
      "0  צור קטע קוד Terraform HCL שיוצר AWS autoscalin...   \n",
      "1                     אילו שפות מדוברות בקוסטה ריקה?   \n",
      "2  האם תוכל/י לסכם לי את הסיפור מהספר Harry Potte...   \n",
      "3                               האם ישו באמת התקיים?   \n",
      "4  ?תוכל לתת לי דוגמה לתבנית העיצוב Factory בשפת ...   \n",
      "\n",
      "                                          cot_hebrew  \\\n",
      "0  להלן הסבר מפורט, שלב אחר שלב, ליצירת קוד Terra...   \n",
      "1  להלן הסבר מפורט, צעד אחר צעד, בנוגע לשפות המדו...   \n",
      "2  כמובן, הנה סיכום מפורט, צעד אחר צעד, של עלילת ...   \n",
      "3  להלן הסבר מפורט, שלב אחר שלב, בנושא ההיסטוריות...   \n",
      "4  כמובן, הנה דוגמה והסבר מפורט בעברית על תבנית ה...   \n",
      "\n",
      "                                       answer_hebrew  \n",
      "0  בטח, הנה דוגמת קוד Terraform HCL היוצרת AWS Au...  \n",
      "1  השפה העיקרית המדוברת בקוסטה ריקה היא ספרדית, ע...  \n",
      "2  הארי פוטר, יתום, מגלה שהוא קוסם ומתחיל ללמוד ב...  \n",
      "3  בתור מודל שפה של בינה מלאכותית, אין באפשרותי ל...  \n",
      "4  ```java\\ninterface Shape {\\n   void draw();\\n}...  \n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:56:15.082195Z",
     "start_time": "2025-08-26T09:56:15.075575Z"
    }
   },
   "cell_type": "code",
   "source": "hebrew_dataset.to_csv('/Users/oribar-joseph/Downloads/cot_tulu.csv')",
   "id": "4a9dbc4657519955",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7fa70c8d9839c235"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:08:40.690978Z",
     "start_time": "2025-08-26T09:08:36.961916Z"
    }
   },
   "cell_type": "code",
   "source": "\n",
   "id": "b64856a2f6048aa9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'oasst1_5921', 'messages': [{'content': 'Create a snippet of Terraform HCL code that create an AWS autoscaling group, and an ALB in front to expose an application to internet.', 'role': 'user'}, {'content': 'Sure, here\\'s an example Terraform HCL code that creates an AWS Autoscaling Group and an Application Load Balancer to expose an application to the internet:\\n``` \\n# Configure the AWS provider\\nprovider \"aws\" {\\n  region = \"us-east-1\"\\n}\\n\\n# Create a security group to allow traffic to the ALB\\nresource \"aws_security_group\" \"alb_sg\" {\\n  name_prefix = \"alb_sg\"\\n  ingress {\\n    from_port = 80\\n    to_port = 80\\n    protocol = \"tcp\"\\n    cidr_blocks = [\"0.0.0.0/0\"]\\n  }\\n}\\n\\n# Create an ALB and target group\\nresource \"aws_lb\" \"alb\" {\\n  name               = \"example-alb\"\\n  internal           = false\\n  load_balancer_type = \"application\"\\n\\n  subnets = [\"subnet-12345678\", \"subnet-87654321\"]\\n\\n  security_groups = [aws_security_group.alb_sg.id]\\n\\n  tags = {\\n    Environment = \"production\"\\n  }\\n}\\n\\nresource \"aws_lb_target_group\" \"target_group\" {\\n  name        = \"example-target-group\"\\n  port        = 80\\n  protocol    = \"HTTP\"\\n  vpc_id      = \"vpc-12345678\"\\n\\n  health_check {\\n    path = \"/health\"\\n  }\\n\\n  tags = {\\n    Environment = \"production\"\\n  }\\n}\\n\\n# Create an autoscaling group\\nresource \"aws_launch_configuration\" \"launch_configuration\" {\\n  name_prefix   = \"example-lc-\"\\n  image_id      = \"ami-12345678\"\\n  instance_type = \"t2.micro\"\\n\\n  # Other settings here (e.g. security groups, userdata, etc.)\\n}\\n\\nresource \"aws_autoscaling_group\" \"autoscaling_group\" {\\n  name                      = \"example-asg\"\\n  launch_configuration      = aws_launch_configuration.launch_configuration.id\\n  target_group_arns         = [aws_lb_target_group.target_group.arn]\\n  health_check_type         = \"EC2\"\\n  min_size                  = 1\\n  max_size                  = 3\\n  desired_capacity          = 2\\n  vpc_zone_identifier       = [\"subnet-12345678\", \"subnet-87654321\"]\\n  termination_policies      = [\"Default\"]\\n  wait_for_capacity_timeout = \"10m\"\\n\\n  tags = {\\n    Environment = \"production\"\\n  }\\n}\\n``` \\nNote that you will need to customize the settings for your specific use case (e.g. specify your own AMI ID, VPC ID, subnet IDs, etc.). This is just an example to give you an idea of how to create an autoscaling group with an ALB in Terraform.', 'role': 'assistant'}], 'source': 'ai2-adapt-dev/oasst1_converted'}\n",
      "{'id': 'oasst1_137', 'messages': [{'content': 'Which languages are spoken in Costa Rica?', 'role': 'user'}, {'content': 'The primary language spoken in Costa Rica is Spanish, with some Central American features specific to the country.\\nCosta Rica is home to at least five living local indigenous languages spoken by the descendants of pre-Columbian peoples: Maléku, Cabécar, Bribri, Guaymí, and Buglere. Some Afro-Carib immigrants also speak Jamaican patois  (or Mekatelyu), a Creole-English language.\\nIn addition, many Costa Rican adults speak a second language such as English, French,  Portuguese or German.', 'role': 'assistant'}], 'source': 'ai2-adapt-dev/oasst1_converted'}\n",
      "{'id': 'oasst1_6476', 'messages': [{'content': \"Can u summarize me story from the book Harry Potter and the Philosopher's Stone?\", 'role': 'user'}, {'content': 'Harry Potter, an orphan, discovers he is a wizard and attends Hogwarts School of Witchcraft and Wizardry. He learns of a powerful stone that can grant immortality and ultimate power, guarded by the evil wizard Voldemort. With the help of friends, Harry must overcome challenges and defeat Voldemort to prevent him from obtaining the stone.', 'role': 'assistant'}, {'content': 'Could you recommend me a few books similar to \"The Hitchhiker\\'s Guide to the Galaxy\" by Douglas Adams?', 'role': 'user'}, {'content': 'Certainly! If you enjoy \"The Hitchhiker\\'s Guide to the Galaxy\" by Douglas Adams, you may also like these books:\\n\\n\"Good Omens\" by Terry Pratchett and Neil Gaiman\\n\"Red Dwarf\" by Grant Naylor\\n\"So Long, and Thanks for All the Fish\" by Douglas Adams\\n\"The Long Earth\" by Terry Pratchett and Stephen Baxter\\n\"Snow Crash\" by Neal Stephenson\\n\"Journey to the Center of the Earth\" by Jules Verne\\n\"The Restaurant at the End of the Universe\" by Douglas Adams\\n\"The Man in the High Castle\" by Philip K. Dick\\n\"Dirk Gently\\'s Holistic Detective Agency\" by Douglas Adams\\n\"The Moon is a Harsh Mistress\" by Robert A. Heinlein\\nThese books blend elements of science fiction, humor, and satire to create unique and entertaining stories.', 'role': 'assistant'}], 'source': 'ai2-adapt-dev/oasst1_converted'}\n",
      "{'id': 'oasst1_5569', 'messages': [{'content': 'Did Jesus realy exist', 'role': 'user'}, {'content': \"As an AI language model, I cannot provide a definitive answer to this question. However, it is widely accepted among historians that Jesus of Nazareth was a real historical figure who lived in the first century CE. The existence of Jesus is mentioned in many ancient sources, including the Gospels of the New Testament, as well as non-Christian sources such as the Jewish historian Josephus and the Roman historian Tacitus. While there is some debate among scholars about the details of Jesus' life and teachings, his existence as a historical figure is generally accepted.\", 'role': 'assistant'}], 'source': 'ai2-adapt-dev/oasst1_converted'}\n",
      "{'id': 'oasst1_7230', 'messages': [{'content': 'Write five lines of iambic pentameter about a subject of your choosing. Do not use any trochaic substitutions. The lines do not need to rhyme.', 'role': 'user'}, {'content': 'The big brown cat jumped down the tallest tree\\nas everything up there was such a bore\\nThe feline only knew that down below\\nthat when he reached the ground from high above\\nhe could eat some mice - fluffy paradise!', 'role': 'assistant'}], 'source': 'ai2-adapt-dev/oasst1_converted'}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6c895254ec8f59b2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
