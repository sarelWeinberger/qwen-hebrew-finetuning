#!/usr/bin/env python3
"""
Gepeta Project - Geektime Files Processor
×¢×™×‘×•×“ ×§×‘×¦×™ Geektime ×¢× Google Gemini API + ××¢×§×‘ ××™×œ×™×

Usage:
1. pip install google-generativeai boto3 pandas python-dotenv
2. ×”×’×“×¨ GOOGLE_API_KEY ×•-AWS credentials
3. python geektime_processor.py
"""

import google.generativeai as genai
import boto3
import pandas as pd
from io import StringIO
import time
import os
import re
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from dotenv import load_dotenv
import json

# =============================================================================
# ×”×’×“×¨×•×ª
# =============================================================================

load_dotenv()

# API Keys
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY_SANDBOX_2", "YOUR_API_KEY_HERE")

# S3 Settings
SOURCE_BUCKET = "gepeta-datasets"
SOURCE_PREFIX = "partly-processed/regex-and-dedup"
TARGET_BUCKET = "gepeta-datasets"
TARGET_PREFIX = "processed/"

# Processing Settings
PREFIX_FILTER = "Geektime"  # ×”×§×™×“×•××ª ×©×× ×—× ×• ××¢×‘×“×™× (× ×™×ª×Ÿ ×œ×©×™× ×•×™)
MAX_WORKERS = 10  # ××¡×¤×¨ threads ××§×‘×™×œ×™× (×”×’×“×¨×” ××™×˜×‘×™×ª ×©××¦×× ×•)
BATCH_SIZE = 50  # ×’×•×“×œ ×‘××¦' ×œ×¢×™×‘×•×“
TEST_LIMIT = 100  # ××¡×¤×¨ ×§×‘×¦×™× ×œ×‘×“×™×§×” (None ×œ×›×œ ×”×§×‘×¦×™×)


class GeektimeProcessor:
    """××¢×‘×“ ×§×‘×¦×™ Geektime ×¢× Google API"""

    def __init__(self, api_key):
        """××ª×—×•×œ ×”××¢×‘×“"""
        if api_key == "YOUR_API_KEY_HERE":
            raise ValueError("âŒ ×¢×“×›×Ÿ ××ª GOOGLE_API_KEY!")

        # Google AI Setup
        genai.configure(api_key=api_key)
        self.model_name = 'gemini-2.0-flash'  # ×©×“×¨×•×’ ×œ××•×“×œ 2.0

        # S3 Setup
        self.s3_client = boto3.client('s3')

        # Statistics - ×”×•×¡×¤×ª ××¢×§×‘ ××™×œ×™×
        self.stats = {
            'files_processed': 0,
            'texts_processed': 0,
            'total_time': 0,
            'errors': [],
            'total_original_words': 0,  # ×—×“×©: ×¡×™×›×•× ××™×œ×™× ××§×•×¨×™×•×ª
            'total_cleaned_words': 0,  # ×—×“×©: ×¡×™×›×•× ××™×œ×™× × ×§×™×•×ª
            'start_time': datetime.now()
        }

        print("ğŸš€ Gepeta Generic Prefix Processor ××•×›×Ÿ")
        print(f"ğŸ¤– ××•×“×œ: {self.model_name}")
        print(f"ğŸ“ ××§×•×¨: s3://{SOURCE_BUCKET}/{SOURCE_PREFIX}")
        print(f"ğŸ¯ ×™×¢×“: s3://{TARGET_BUCKET}/{TARGET_PREFIX}{PREFIX_FILTER.lower()}/")
        print(f"ğŸ” ×§×™×“×•××ª: {PREFIX_FILTER}")
        print(f"ğŸ‘¥ Workers: {MAX_WORKERS}")
        print(f"â° ×”×ª×—×œ×”: {self.stats['start_time'].strftime('%H:%M:%S')}")

    def count_words(self, text):
        """×¡×¤×™×¨×ª ××™×œ×™× ×‘×˜×§×¡×˜"""
        if pd.isna(text) or text == '':
            return 0
        words = str(text).split()
        return len(words)

    def extract_part_number(self, filename):
        """×—×™×œ×•×¥ ××¡×¤×¨ part ××”×©× ×§×•×‘×¥"""
        # ×“×•×’××”: GeektimeCorpus-Combined-Deduped.forgpt_part-1_cleaned.csv
        # ××—×¤×© ×ª×‘× ×™×ª: part-{number}
        match = re.search(r'part-(\d+)', filename)
        if match:
            return int(match.group(1))
        return None

    def list_geektime_files(self, limit=None, part_start=None, part_end=None):
        """××¦×™××ª ×›×œ ×§×‘×¦×™ Geektime ×¢× ×¡×™× ×•×Ÿ ×œ×¤×™ part range"""
        print(f"ğŸ” ××—×¤×© ×§×‘×¦×™ {PREFIX_FILTER}...")

        if part_start is not None and part_end is not None:
            print(f"ğŸ¯ ××¡× ×Ÿ: part-{part_start} ×¢×“ part-{part_end}")
        elif part_start is not None:
            print(f"ğŸ¯ ××¡× ×Ÿ: ×”×—×œ ×-part-{part_start}")
        elif part_end is not None:
            print(f"ğŸ¯ ××¡× ×Ÿ: ×¢×“ part-{part_end}")

        files = []
        paginator = self.s3_client.get_paginator('list_objects_v2')

        page_count = 0
        for page in paginator.paginate(Bucket=SOURCE_BUCKET, Prefix=SOURCE_PREFIX):
            page_count += 1
            print(f"  ×¡×•×¨×§ ×“×£ {page_count}...")

            if 'Contents' not in page:
                continue

            page_files = []
            for obj in page['Contents']:
                key = obj['Key']
                filename = os.path.basename(key)

                # ×‘×“×™×§×•×ª ×‘×¡×™×¡×™×•×ª
                if not (key.endswith('.csv') and
                        PREFIX_FILTER in filename and
                        obj['Size'] > 0):
                    continue

                # ×‘×“×™×§×ª part number ×× ×™×© ×¡×™× ×•×Ÿ
                if part_start is not None or part_end is not None:
                    part_num = self.extract_part_number(filename)
                    if part_num is None:
                        continue  # ×“×œ×’ ×¢×œ ×§×‘×¦×™× ×‘×œ×™ part number

                    # ×‘×“×™×§×ª ×˜×•×•×—
                    if part_start is not None and part_num < part_start:
                        continue
                    if part_end is not None and part_num > part_end:
                        continue

                page_files.append(key)

            files.extend(page_files)
            print(f"  ×“×£ {page_count}: × ××¦××• {len(page_files)} ×§×‘×¦×™ {PREFIX_FILTER}")

            # ×”×’×‘×œ×” ×œ×‘×“×™×§×”
            if limit and len(files) >= limit:
                files = files[:limit]
                break

        # ××™×•×Ÿ ×œ×¤×™ part number
        def sort_key(file_path):
            filename = os.path.basename(file_path)
            part_num = self.extract_part_number(filename)
            return part_num if part_num is not None else -1

        files.sort(key=sort_key)

        print(f"âœ… × ××¦××• {len(files)} ×§×‘×¦×™ {PREFIX_FILTER} ×›×•×œ×œ")

        # ×”×“×¤×¡×ª ××¡×¤×¨ parts ×©× ××¦××•
        if files:
            parts = [self.extract_part_number(os.path.basename(f)) for f in files]
            parts = [p for p in parts if p is not None]
            if parts:
                print(f"ğŸ“Š ×˜×•×•×— parts: {min(parts)} - {max(parts)}")

        return files

    def read_csv_from_s3(self, bucket, key):
        """×§×¨×™××ª CSV ×-S3"""
        try:
            response = self.s3_client.get_object(Bucket=bucket, Key=key)
            content = response['Body'].read().decode('utf-8')
            df = pd.read_csv(StringIO(content))
            return df
        except Exception as e:
            print(f"âŒ ×©×’×™××” ×‘×§×¨×™××ª {key}: {e}")
            return None

    def save_csv_to_s3(self, df, bucket, key):
        """×©××™×¨×ª CSV ×œ-S3"""
        try:
            csv_buffer = StringIO()
            df.to_csv(csv_buffer, index=False, encoding='utf-8')

            self.s3_client.put_object(
                Bucket=bucket,
                Key=key,
                Body=csv_buffer.getvalue(),
                ContentType='text/csv'
            )
            return True
        except Exception as e:
            print(f"âŒ ×©×’×™××” ×‘×©××™×¨×ª {key}: {e}")
            return False

    def clean_text_with_api(self, text):
        """× ×™×§×•×™ ×˜×§×¡×˜ ×™×—×™×“ ×¢× Google API - ×œ×œ× ×—×™×ª×•×š"""
        model = genai.GenerativeModel(self.model_name)

        prompt = f"""× ×§×” ××ª ×”×˜×§×¡×˜ ×”×¢×‘×¨×™ ×”×‘× ××¤×’××™ ×§×™×“×•×“, ×ª×’×™×•×ª HTML, ×¤×¨×¡×•××•×ª ×•×ª×‘× ×™×•×ª. ×”×—×–×¨ ×¨×§ ×˜×§×¡×˜ × ×§×™ ×‘×¢×‘×¨×™×ª:

{text}

×˜×§×¡×˜ × ×§×™:"""

        try:
            response = model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            return f"[API_ERROR] {str(e)}"

    def process_texts_parallel(self, texts):
        """×¢×™×‘×•×“ ××§×‘×™×œ×™ ×©×œ ×¨×©×™××ª ×˜×§×¡×˜×™×"""
        results = [''] * len(texts)  # ×©××™×¨×ª ×¡×“×¨

        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # ×©×œ×™×—×ª ×‘×§×©×•×ª ×¢× index
            future_to_index = {
                executor.submit(self.clean_text_with_api, text): i
                for i, text in enumerate(texts)
            }

            # ××™×¡×•×£ ×ª×•×¦××•×ª
            for future in as_completed(future_to_index):
                index = future_to_index[future]
                try:
                    result = future.result()
                    results[index] = result
                except Exception as e:
                    results[index] = f"[ERROR] {str(e)}"

        return results

    def process_single_file(self, file_key):
        """×¢×™×‘×•×“ ×§×•×‘×¥ ×™×—×™×“"""
        file_name = os.path.basename(file_key)
        part_num = self.extract_part_number(file_name)
        part_info = f" (part-{part_num})" if part_num is not None else ""

        print(f"\nğŸ“ ××¢×‘×“: {file_name}{part_info}")

        file_start_time = time.time()

        # ×§×¨×™××ª ×§×•×‘×¥
        df = self.read_csv_from_s3(SOURCE_BUCKET, file_key)
        if df is None:
            self.stats['errors'].append(f"×œ× ×”×¦×œ×™×— ×œ×§×¨×•×: {file_name}")
            return False

        print(f"ğŸ“Š × ×˜×¢×Ÿ: {len(df)} ×©×•×¨×•×ª")

        # ×‘×“×™×§×ª ×¢××•×“×•×ª × ×“×¨×©×•×ª
        if 'text' not in df.columns:
            print(f"âŒ ××™×Ÿ ×¢××•×“×ª 'text' ×‘-{file_name}")
            self.stats['errors'].append(f"××™×Ÿ ×¢××•×“×ª text: {file_name}")
            return False

        if 'n_count' not in df.columns:
            print(f"âŒ ××™×Ÿ ×¢××•×“×ª 'n_count' ×‘-{file_name}")
            self.stats['errors'].append(f"××™×Ÿ ×¢××•×“×ª n_count: {file_name}")
            return False

        # ×”×›× ×ª ×˜×§×¡×˜×™× ×œ×¢×™×‘×•×“
        texts = df['text'].dropna().tolist()
        if not texts:
            print(f"âš ï¸ ××™×Ÿ ×˜×§×¡×˜×™× ×‘-{file_name}")
            return False

        print(f"ğŸ”„ ××¢×‘×“ {len(texts)} ×˜×§×¡×˜×™× ×‘×‘××¦'×™×...")

        # ×¢×™×‘×•×“ ×‘×‘××¦'×™×
        all_cleaned_texts = []
        total_batches = (len(texts) + BATCH_SIZE - 1) // BATCH_SIZE

        for batch_idx in range(0, len(texts), BATCH_SIZE):
            batch_num = (batch_idx // BATCH_SIZE) + 1
            batch = texts[batch_idx:batch_idx + BATCH_SIZE]

            print(f"  ğŸ“¦ ×‘××¦' {batch_num}/{total_batches} ({len(batch)} ×˜×§×¡×˜×™×)")

            batch_start = time.time()
            cleaned_batch = self.process_texts_parallel(batch)
            batch_time = time.time() - batch_start

            all_cleaned_texts.extend(cleaned_batch)

            print(f"  âœ… ×”×•×©×œ×: {batch_time:.1f}s ({batch_time / len(batch):.2f}s/text)")

            # ×”××ª× ×” ×§×˜× ×” ×œ×× ×™×¢×ª rate limiting
            if batch_num < total_batches:
                time.sleep(0.5)

        # ×™×¦×™×¨×ª DataFrame ×¢× ×ª×•×¦××•×ª
        df_result = df.copy()
        df_result['cleaned_text'] = all_cleaned_texts[:len(df)]

        # ×—×™×©×•×‘ ×¡×˜×˜×™×¡×˜×™×§×•×ª ××™×œ×™× ×œ×§×•×‘×¥ ×”× ×•×›×—×™
        file_original_words = (df_result['n_count'].sum() - 1)  # ××™× ×•×¡ 1 ××”×¡×š ×”×›×œ
        file_cleaned_words = df_result['cleaned_text'].apply(self.count_words).sum()

        print(f"ğŸ“ ××™×œ×™× ××§×•×¨×™×•×ª: {file_original_words:,}")
        print(f"âœ¨ ××™×œ×™× × ×§×™×•×ª: {file_cleaned_words:,}")
        if file_original_words > 0:
            reduction = ((file_original_words - file_cleaned_words) / file_original_words * 100)
            print(f"ğŸ“‰ ×”×¤×—×ª×”: {reduction:.1f}%")

        # ×©××™×¨×” ×œ-S3
        target_key = f"{TARGET_PREFIX}{PREFIX_FILTER.lower()}/{file_name}"
        success = self.save_csv_to_s3(df_result, TARGET_BUCKET, target_key)

        file_time = time.time() - file_start_time

        if success:
            print(f"âœ… × ×©××¨: s3://{TARGET_BUCKET}/{target_key}")
            print(f"â° ×–××Ÿ ×§×•×‘×¥: {file_time / 60:.1f} ×“×§×•×ª")

            # ×¢×“×›×•×Ÿ ×¡×˜×˜×™×¡×˜×™×§×•×ª ×›×•×œ×œ×•×ª
            self.stats['files_processed'] += 1
            self.stats['texts_processed'] += len(texts)
            self.stats['total_time'] += file_time
            self.stats['total_original_words'] += file_original_words  # ×—×“×©
            self.stats['total_cleaned_words'] += file_cleaned_words  # ×—×“×©

            return True
        else:
            self.stats['errors'].append(f"×©×’×™××” ×‘×©××™×¨×ª: {file_name}")
            return False

    def print_progress(self, current, total, start_time):
        """×”×“×¤×¡×ª ×”×ª×§×“××•×ª"""
        elapsed = time.time() - start_time
        if current > 0:
            avg_time_per_file = elapsed / current
            estimated_remaining = (total - current) * avg_time_per_file

            print(f"ğŸ“Š ×”×ª×§×“××•×ª: {current}/{total} ({current / total * 100:.1f}%)")
            print(f"â° ×–××Ÿ ×©×—×œ×£: {elapsed / 60:.1f} ×“×§×•×ª")
            print(f"ğŸ”® ×–××Ÿ ××©×•×¢×¨ ×œ×¡×™×•×: {estimated_remaining / 60:.1f} ×“×§×•×ª")
            print(f"ğŸ“ˆ ×§×¦×‘: {self.stats['texts_processed'] / elapsed:.1f} ×˜×§×¡×˜×™×/×©× ×™×™×”")

            # ×”×•×¡×¤×ª ××™×“×¢ ×¢×œ ××™×œ×™×
            if self.stats['total_original_words'] > 0:
                reduction = ((self.stats['total_original_words'] - self.stats['total_cleaned_words']) /
                             self.stats['total_original_words'] * 100)
                print(
                    f"ğŸ“ ××™×œ×™× ×¢×“ ×›×”: {self.stats['total_original_words']:,} â†’ {self.stats['total_cleaned_words']:,} ({reduction:.1f}% ×”×¤×—×ª×”)")

    def save_dataset_summary(self):
        """×©××™×¨×ª ×¡×™×›×•× ××™×œ×™× ×©×œ ×”×“×˜××¡×˜ ×”× ×•×›×—×™"""
        if self.stats['total_original_words'] == 0:
            return

        try:
            # ×™×¦×™×¨×ª ×©×•×¨×ª ×¡×™×›×•×
            summary_data = {
                'Dataset': PREFIX_FILTER,
                'text_words': self.stats['total_original_words'],
                'clean_text_words': self.stats['total_cleaned_words']
            }

            summary_df = pd.DataFrame([summary_data])

            # ×©××™×¨×” ×œ-S3
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            summary_key = f"summaries/{PREFIX_FILTER.lower()}_word_summary_{timestamp}.csv"

            csv_buffer = StringIO()
            summary_df.to_csv(csv_buffer, index=False, encoding='utf-8')

            self.s3_client.put_object(
                Bucket=TARGET_BUCKET,
                Key=summary_key,
                Body=csv_buffer.getvalue(),
                ContentType='text/csv'
            )

            print(f"ğŸ“Š ×¡×™×›×•× ××™×œ×™× × ×©××¨ ×‘: s3://{TARGET_BUCKET}/{summary_key}")

        except Exception as e:
            print(f"âš ï¸ ×©×’×™××” ×‘×©××™×¨×ª ×¡×™×›×•× ××™×œ×™×: {e}")

    def print_final_stats(self):
        """×”×“×¤×¡×ª ×¡×˜×˜×™×¡×˜×™×§×•×ª ×¡×•×¤×™×•×ª"""
        total_time = time.time() - self.stats['start_time'].timestamp()

        print(f"\n{'=' * 60}")
        print("ğŸ“Š ×¡×™×›×•× ×¢×™×‘×•×“")
        print("=" * 60)
        print(f"âœ… ×§×‘×¦×™× ××¢×•×‘×“×™×: {self.stats['files_processed']}")
        print(f"ğŸ“ ×˜×§×¡×˜×™× ××¢×•×‘×“×™×: {self.stats['texts_processed']:,}")
        print(f"â° ×–××Ÿ ×›×•×œ×œ: {total_time / 60:.1f} ×“×§×•×ª")

        if self.stats['texts_processed'] > 0:
            avg_time = total_time / self.stats['texts_processed']
            print(f"âš¡ ×–××Ÿ ×××•×¦×¢ ×œ×˜×§×¡×˜: {avg_time:.2f} ×©× ×™×•×ª")
            print(f"ğŸš€ ×§×¦×‘ ×¢×™×‘×•×“: {self.stats['texts_processed'] / total_time:.1f} ×˜×§×¡×˜×™×/×©× ×™×™×”")

        # ×”×¦×’×ª ××™×“×¢ ×¢×œ ××™×œ×™×
        if self.stats['total_original_words'] > 0:
            print(f"\nğŸ“ ×¡×™×›×•× ××™×œ×™×:")
            print(f"   ××™×œ×™× ××§×•×¨×™×•×ª: {self.stats['total_original_words']:,}")
            print(f"   ××™×œ×™× × ×§×™×•×ª: {self.stats['total_cleaned_words']:,}")
            reduction = ((self.stats['total_original_words'] - self.stats['total_cleaned_words']) /
                         self.stats['total_original_words'] * 100)
            print(f"   ×”×¤×—×ª×”: {reduction:.1f}%")

        if self.stats['errors']:
            print(f"\nâŒ ×©×’×™××•×ª ({len(self.stats['errors'])}):")
            for error in self.stats['errors'][:5]:  # ×¨×§ 5 ×¨××©×•× ×•×ª
                print(f"  â€¢ {error}")
            if len(self.stats['errors']) > 5:
                print(f"  ... ×•×¢×•×“ {len(self.stats['errors']) - 5} ×©×’×™××•×ª")

        print(f"\nğŸ’¾ ×§×‘×¦×™× × ×©××¨×• ×‘: s3://{TARGET_BUCKET}/{TARGET_PREFIX}{PREFIX_FILTER.lower()}/")

        # ×©××™×¨×ª ×¡×™×›×•× ××™×œ×™×
        self.save_dataset_summary()

    def get_part_range_input(self):
        """×§×‘×œ×ª ×˜×•×•×— parts ××”××©×ª××©"""
        print(f"\nğŸ¯ ×”×’×“×¨×ª ×˜×•×•×— Parts ×œ×¢×™×‘×•×“:")
        print(f"ğŸ’¡ ×¤×•×¨××˜: part-0, part-1, part-2, ...")
        print(f"ğŸ“ ×‘×¨×™×¨×ª ××—×“×œ: ×›×œ ×”-parts")

        try:
            start_input = input("ğŸ”¢ Part ×”×ª×—×œ×” (Enter ×œ×›×œ ×”×˜×•×•×—): ").strip()
            part_start = None if start_input == "" else int(start_input)

            end_input = input("ğŸ”¢ Part ×¡×™×•× (Enter ×œ×›×œ ×”×˜×•×•×—): ").strip()
            part_end = None if end_input == "" else int(end_input)

            # ×•×œ×™×“×¦×™×”
            if part_start is not None and part_end is not None and part_start > part_end:
                print("âŒ ×©×’×™××”: Part ×”×ª×—×œ×” ×’×“×•×œ ×-Part ×”×¡×™×•×")
                return self.get_part_range_input()  # × ×¡×” ×©×•×‘

            if part_start is not None and part_start < 0:
                print("âŒ ×©×’×™××”: Part ×—×™×™×‘ ×œ×”×™×•×ª ××¡×¤×¨ ×—×™×•×‘×™")
                return self.get_part_range_input()  # × ×¡×” ×©×•×‘

            if part_end is not None and part_end < 0:
                print("âŒ ×©×’×™××”: Part ×—×™×™×‘ ×œ×”×™×•×ª ××¡×¤×¨ ×—×™×•×‘×™")
                return self.get_part_range_input()  # × ×¡×” ×©×•×‘

            # ×”×“×¤×¡×ª ×‘×—×™×¨×”
            if part_start is not None and part_end is not None:
                print(f"âœ… × ×‘×—×¨ ×˜×•×•×—: part-{part_start} ×¢×“ part-{part_end}")
            elif part_start is not None:
                print(f"âœ… × ×‘×—×¨: ×”×—×œ ×-part-{part_start}")
            elif part_end is not None:
                print(f"âœ… × ×‘×—×¨: ×¢×“ part-{part_end}")
            else:
                print(f"âœ… × ×‘×—×¨: ×›×œ ×”-parts")

            return part_start, part_end

        except ValueError:
            print("âŒ ×©×’×™××”: ×™×© ×œ×”×–×™×Ÿ ××¡×¤×¨ ×©×œ× ×‘×œ×‘×“")
            return self.get_part_range_input()  # × ×¡×” ×©×•×‘

    def run_processing(self, test_mode=True):
        """×”×¨×¦×ª ×¢×™×‘×•×“ ××œ×"""
        print(f"ğŸš€ ××ª×—×™×œ ×¢×™×‘×•×“ ×§×‘×¦×™ {PREFIX_FILTER}")

        # ×§×‘×œ×ª ×˜×•×•×— parts
        part_start, part_end = self.get_part_range_input()

        # ××¦×™××ª ×§×‘×¦×™×
        limit = TEST_LIMIT if test_mode else None
        files = self.list_geektime_files(limit, part_start, part_end)

        if not files:
            print(f"âŒ ×œ× × ××¦××• ×§×‘×¦×™ {PREFIX_FILTER} ×‘×˜×•×•×— ×©× ×‘×—×¨")
            return

        print(f"\nğŸ¯ ××¦×‘: {'×‘×“×™×§×”' if test_mode else '×™×™×¦×•×¨'}")
        print(f"ğŸ“ ××¡×¤×¨ ×§×‘×¦×™× ×œ×¢×™×‘×•×“: {len(files)}")

        # ××™×©×•×¨ ××©×ª××©
        if not test_mode:
            response = input(f"\nâ“ ×œ×”×ª×—×™×œ ×¢×™×‘×•×“ {len(files)} ×§×‘×¦×™×? (y/N): ")
            if response.lower() != 'y':
                print("âŒ ×¢×™×‘×•×“ ×‘×•×˜×œ")
                return

        # ×¢×™×‘×•×“ ×§×‘×¦×™×
        start_time = time.time()
        successful_files = 0

        for i, file_key in enumerate(files, 1):
            print(f"\n{'=' * 60}")
            print(f"ğŸ“ ×§×•×‘×¥ {i}/{len(files)}: {os.path.basename(file_key)}")

            success = self.process_single_file(file_key)
            if success:
                successful_files += 1

            # ×”×ª×§×“××•×ª ×›×œ 5 ×§×‘×¦×™×
            if i % 5 == 0 or i == len(files):
                self.print_progress(i, len(files), start_time)

        # ×¡×™×›×•×
        self.print_final_stats()

        print(f"\nğŸ‰ ×¢×™×‘×•×“ ×”×•×©×œ×!")
        print(f"âœ… {successful_files}/{len(files)} ×§×‘×¦×™× ×¢×•×‘×“×• ×‘×”×¦×œ×—×”")


def main():
    """×¤×•× ×§×¦×™×” ×¨××©×™×ª"""
    print("ğŸš€ Gepeta Processor (No Truncation)")
    print("=" * 60)

    try:
        # ×™×¦×™×¨×ª ××¢×‘×“
        processor = GeektimeProcessor(GOOGLE_API_KEY)

        # ×‘×—×™×¨×ª ××¦×‘
        print(f"\nğŸ¯ ××¤×©×¨×•×™×•×ª:")
        print(f"1. ×‘×“×™×§×” ({TEST_LIMIT} ×§×‘×¦×™×)")
        print(f"2. ×™×™×¦×•×¨ (×›×œ ×”×§×‘×¦×™×)")

        choice = input("×‘×—×¨ (1/2): ").strip()

        if choice == "1":
            print(f"\nğŸ§ª ××ª×—×™×œ ×‘×“×™×§×” ×¢× {TEST_LIMIT} ×§×‘×¦×™×...")
            processor.run_processing(test_mode=True)
        elif choice == "2":
            print(f"\nğŸ­ ××ª×—×™×œ ×¢×™×‘×•×“ ×™×™×¦×•×¨...")
            processor.run_processing(test_mode=False)
        else:
            print("âŒ ×‘×—×™×¨×” ×œ× ×ª×§×™× ×”")
            return

    except ValueError as e:
        print(f"âŒ {e}")
        print("ğŸ’¡ ×¢×“×›×Ÿ ××ª GOOGLE_API_KEY ×‘×ª×—×™×œ×ª ×”×§×•×‘×¥ ××• ×‘×§×•×‘×¥ .env")
    except Exception as e:
        print(f"âŒ ×©×’×™××”: {e}")

    print(f"\nâ° ×¡×™×•×: {datetime.now().strftime('%H:%M:%S')}")


if __name__ == "__main__":
    main()