#!/usr/bin/env python3
"""
Gepeta EC2 Worker - ×¢×•×‘×“ ×™×—×™×“ ×œ×§×•×‘×¥ ××—×“ ×¢× ×©×™××•×© ×‘×‘××¦'×™×
Usage: python worker.py --prefix Geektime --part 0 --dataset geektime

NOTE: This script expects to run in a virtual environment at /opt/venv
"""

from google import genai
from google.genai.types import CreateBatchJobConfig
import boto3
import pandas as pd
from io import StringIO
import time
import os
import argparse
import json
from datetime import datetime
import jsonlines
import fsspec
from dotenv import load_dotenv

load_dotenv()

# ×”×’×“×¨×•×ª S3
SOURCE_BUCKET = "gepeta-datasets"
SOURCE_PREFIX = "partly-processed/regex-and-dedup"
TARGET_BUCKET = "gepeta-datasets"
TARGET_PREFIX = "processed/"
STATUS_BUCKET = "gepeta-datasets"
STATUS_PREFIX = "worker-status/"

# ×”×’×“×¨×•×ª GCP
PROJECT_ID = "pwcnext-sandbox01"
LOCATION = "us-central1"
BATCH_BUCKET = "gepeta-batches"

# ×”×’×“×¨×•×ª ××•×“×œ
MODEL_ID = "gemini-2.0-flash-001"


class SingleFileProcessor:
    def __init__(self, api_key, prefix, part_number, dataset_name):
        self.prefix = prefix
        self.part_number = part_number
        self.dataset_name = dataset_name.lower()

        # Initialize GCP Vertex AI client
        self.client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)

        self.s3_client = boto3.client('s3')
        self.worker_id = f"{prefix}_part-{part_number}"

        self.stats = {
            'worker_id': self.worker_id,
            'prefix': prefix,
            'part_number': part_number,
            'dataset': dataset_name,
            'status': 'starting',
            'progress_percent': 0,
            'start_time': datetime.now().isoformat(),
            'total_rows': 0,
            'rows_already_clean': 0,
            'rows_processed_now': 0,
            'rate_limit_errors_found': 0,
            'rows_skipped_so_far': 0,
            'rows_processed_so_far': 0,
            'new_rate_limit_errors': 0
        }

    def update_status(self, status, **kwargs):
        """×¢×“×›×•×Ÿ ×¡×˜×˜×•×¡ ×•×“×™×•×•×—"""
        self.stats['status'] = status
        self.stats['last_update'] = datetime.now().isoformat()

        for key, value in kwargs.items():
            self.stats[key] = value

        try:
            status_key = f"{STATUS_PREFIX}{self.worker_id}.json"

            # ×”××¨×” ×©×œ numpy types ×œ-Python types ×œ×¤× ×™ JSON
            safe_stats = {}
            for key, value in self.stats.items():
                if hasattr(value, 'item'):  # numpy scalar
                    safe_stats[key] = value.item()
                elif isinstance(value, (int, float)):
                    safe_stats[key] = int(value) if isinstance(value, int) else float(value)
                else:
                    safe_stats[key] = value

            self.s3_client.put_object(
                Bucket=STATUS_BUCKET,
                Key=status_key,
                Body=json.dumps(safe_stats, ensure_ascii=False, indent=2),
                ContentType='application/json'
            )
        except Exception as e:
            print(f"âš ï¸ ×©×’×™××” ×‘×¢×“×›×•×Ÿ ×¡×˜×˜×•×¡: {e}")

        percent = self.stats.get('progress_percent', 0)
        print(f"ğŸ“Š {self.worker_id}: {status} - {percent:.1f}%")

    def find_target_file(self):
        """××¦×™××ª ×”×§×•×‘×¥ ×”××¢×•×‘×“ ×‘-processed/"""
        self.update_status("searching_file")

        # ×—×™×¤×•×© ×™×©×¨ ×‘×ª×™×§×™×™×ª processed
        processed_prefix = f"{TARGET_PREFIX}{self.dataset_name}/"

        print(f"ğŸ” ××—×¤×© ×§×•×‘×¥ ××¢×•×‘×“ ×‘-{processed_prefix}")

        paginator = self.s3_client.get_paginator('list_objects_v2')

        for page in paginator.paginate(Bucket=TARGET_BUCKET, Prefix=processed_prefix):
            if 'Contents' not in page:
                continue

            for obj in page['Contents']:
                key = obj['Key']
                filename = os.path.basename(key)

                if (filename.startswith(self.prefix) and
                        f"part-{self.part_number}" in filename and
                        filename.endswith('.csv') and
                        obj['Size'] > 0):
                    print(f"âœ… × ××¦× ×§×•×‘×¥ ××¢×•×‘×“: {filename}")
                    return TARGET_BUCKET, key

        # ×× ×œ× × ××¦× ×‘processed, ×—×¤×© ×‘××§×•× ×”××§×•×¨×™
        print(f"âš ï¸ ×œ× × ××¦× ×§×•×‘×¥ ××¢×•×‘×“, ××—×¤×© ×‘××§×•×¨...")

        for page in paginator.paginate(Bucket=SOURCE_BUCKET, Prefix=SOURCE_PREFIX):
            if 'Contents' not in page:
                continue

            for obj in page['Contents']:
                key = obj['Key']
                filename = os.path.basename(key)

                if (filename.startswith(self.prefix) and
                        f"part-{self.part_number}" in filename and
                        filename.endswith('.csv') and
                        obj['Size'] > 0):
                    print(f"âœ… × ××¦× ×§×•×‘×¥ ××§×•×¨×™: {filename}")
                    return SOURCE_BUCKET, key

        raise FileNotFoundError(f"×œ× × ××¦× ×§×•×‘×¥ ×¢×‘×•×¨ {self.prefix} part-{self.part_number}")

    def count_words(self, text):
        """×¡×¤×™×¨×ª ××™×œ×™× ×‘×˜×§×¡×˜"""
        if pd.isna(text) or not isinstance(text, str):
            return 0
        return len(str(text).split())

    def is_valid_clean_text(self, text):
        """×‘×“×™×§×” ×× cleaned_text ×ª×§×™×Ÿ (×œ× rate limit error)"""
        if pd.isna(text) or not isinstance(text, str):
            return False

        # Debug print
        if text.startswith("[API_ERROR]"):
            print(f"ğŸ” DEBUG: × ××¦× API_ERROR: {text[:100]}...")
            return False

        # ×× ×–×” ×¨×™×§ ××• ×§×¦×¨ ××“×™, ×œ× ×ª×§×™×Ÿ
        if len(text.strip()) < 3:
            print(f"ğŸ” DEBUG: ×˜×§×¡×˜ ×§×¦×¨ ××“×™: '{text}'")
            return False

        return True

    def is_rate_limit_error(self, text):
        """×‘×“×™×§×” ×× ×”×˜×§×¡×˜ ×”×•× ×©×’×™××ª rate limit"""
        if not isinstance(text, str):
            return False

        is_error = (text.startswith("[API_ERROR] 429") or
                    "RATE_LIMIT_EXCEEDED" in text or
                    "Quota exceeded" in text)

        if is_error:
            print(f"ğŸ” DEBUG: × ××¦× Rate Limit Error: {text[:100]}...")

        return is_error

    def create_batch_jsonl(self, texts):
        """×™×¦×™×¨×ª ×§×•×‘×¥ JSONL ×¢×‘×•×¨ ×”×‘××¦'"""
        jsonl_filename = f"{self.worker_id}_batch.jsonl"

        prompt = """× ×§×” ××ª ×”×˜×§×¡×˜ ×”×¢×‘×¨×™ ×”×‘× ××¤×’××™ ×§×™×“×•×“, ×ª×’×™×•×ª HTML, ×¤×¨×¡×•××•×ª ×•×ª×‘× ×™×•×ª. ×”×—×–×¨ ×¨×§ ×˜×§×¡×˜ × ×§×™ ×‘×¢×‘×¨×™×ª:

{text}

×˜×§×¡×˜ × ×§×™:"""

        with jsonlines.open(jsonl_filename, mode="w") as writer:
            for i, text in enumerate(texts):
                generationConfig = {
                    "temperature": 0,
                    "maxOutputTokens": 8192
                }

                writer.write({
                    "request": {
                        "contents": [
                            {
                                "role": "user",
                                "parts": [
                                    {"text": prompt.format(text=text)}
                                ]
                            }
                        ],
                        "generationConfig": generationConfig
                    }
                })

        return jsonl_filename

    def upload_batch_to_gcs(self, jsonl_filename):
        """×”×¢×œ××ª ×§×•×‘×¥ ×”×‘××¦' ×œ-GCS"""
        from google.cloud import storage

        storage_client = storage.Client(project=PROJECT_ID)
        bucket = storage_client.bucket(BATCH_BUCKET)
        blob = bucket.blob(jsonl_filename)
        blob.upload_from_filename(jsonl_filename)

        return f"gs://{BATCH_BUCKET}/{jsonl_filename}"

    def process_texts_with_batch(self, texts):
        """×¢×™×‘×•×“ ×˜×§×¡×˜×™× ×¢× batch API"""
        print(f"ğŸ”„ ××¢×‘×“ {len(texts)} ×˜×§×¡×˜×™× ×¢× batch API...")

        # ×™×¦×™×¨×ª JSONL
        jsonl_filename = self.create_batch_jsonl(texts)

        # ×”×¢×œ××” ×œ-GCS
        input_uri = self.upload_batch_to_gcs(jsonl_filename)

        # ×™×¦×™×¨×ª batch job
        dest_uri = f"gs://{BATCH_BUCKET}/results/{self.worker_id}"

        batch_job = self.client.batches.create(
            model=MODEL_ID,
            src=input_uri,
            config=CreateBatchJobConfig(dest=dest_uri),
        )

        print(f"ğŸ“‹ Batch job × ×•×¦×¨: {batch_job.name}")

        # ×”××ª× ×” ×œ×¡×™×•×
        while not batch_job.state in ["JOB_STATE_SUCCEEDED", "JOB_STATE_FAILED", "JOB_STATE_UNEXECUTED"]:
            time.sleep(10)
            batch_job = self.client.batches.get(name=batch_job.name)
            print(f"â³ ××—×›×” ×œ×¡×™×•× batch... ×¡×˜×˜×•×¡: {batch_job.state}")

        if batch_job.state == "JOB_STATE_SUCCEEDED":
            print("âœ… Batch job ×”×•×©×œ× ×‘×”×¦×œ×—×”!")

            # ×§×¨×™××ª ×ª×•×¦××•×ª
            results = self.load_batch_results(batch_job.dest.gcs_uri)

            # × ×™×§×•×™
            os.remove(jsonl_filename)
            self.client.batches.delete(name=batch_job.name)

            return results
        else:
            print(f"âŒ Batch job × ×›×©×œ: {batch_job.error}")
            return [f"[API_ERROR] Batch failed: {batch_job.error}"] * len(texts)

    def load_batch_results(self, dest_uri):
        """×˜×¢×™× ×ª ×ª×•×¦××•×ª ×”×‘××¦'"""
        fs = fsspec.filesystem("gcs")
        file_paths = fs.glob(f"{dest_uri}/*/predictions.jsonl")

        if not file_paths:
            print("âŒ ×œ× × ××¦××• ×ª×•×¦××•×ª")
            return []

        # ×˜×¢×™× ×ª ×”×ª×•×¦××•×ª
        df = pd.read_json(f"gs://{file_paths[-1]}", lines=True)

        results = []
        for _, row in df.iterrows():
            try:
                response = row['response']
                if 'candidates' in response and response['candidates']:
                    candidate = response['candidates'][0]
                    if 'content' in candidate and 'parts' in candidate['content']:
                        parts = candidate['content']['parts']
                        if parts and 'text' in parts[0]:
                            results.append(parts[0]['text'].strip())
                        else:
                            results.append("[API_ERROR] No text in response")
                    else:
                        results.append("[API_ERROR] No content in candidate")
                else:
                    results.append("[API_ERROR] No candidates in response")
            except Exception as e:
                results.append(f"[API_ERROR] {str(e)}")

        return results

    def process_file(self):
        """×¢×™×‘×•×“ ×”×§×•×‘×¥ ×”×¨××©×™ - ×¢×•×‘×“ ×¢×œ ×§×‘×¦×™× ××¢×•×‘×“×™×"""
        try:
            # ×—×™×¤×•×© ×•×˜×¢×™× ×ª ×§×•×‘×¥ (××¢×•×‘×“ ××• ××§×•×¨×™)
            source_bucket, file_key = self.find_target_file()
            filename = os.path.basename(file_key)
            is_processed_file = source_bucket == TARGET_BUCKET

            self.update_status("loading_file")

            response = self.s3_client.get_object(Bucket=source_bucket, Key=file_key)
            content = response['Body'].read().decode('utf-8')
            df = pd.read_csv(StringIO(content))

            print(f"ğŸ“Š × ×˜×¢×Ÿ: {len(df)} ×©×•×¨×•×ª ×-{'processed' if is_processed_file else 'source'}")

            if 'text' not in df.columns or 'n_count' not in df.columns:
                raise ValueError(f"×—×¡×¨×•×ª ×¢××•×“×•×ª × ×“×¨×©×•×ª ×‘-{filename}")

            # ×‘×“×™×§×” ×× ×™×© ×¢××•×“×ª cleaned_text
            has_cleaned_text = 'cleaned_text' in df.columns
            print(f"ğŸ“‹ ×™×© ×¢××•×“×ª cleaned_text: {'âœ…' if has_cleaned_text else 'âŒ'}")

            if has_cleaned_text:
                # ×–×” ×§×•×‘×¥ ××¢×•×‘×“ - × ×ª×§×Ÿ ×¨×§ rate limit errors
                total_rows = len(df)
                valid_clean = df['cleaned_text'].apply(self.is_valid_clean_text).sum()
                rate_limit_errors = df['cleaned_text'].apply(self.is_rate_limit_error).sum()

                # ×¢×“×›×•×Ÿ stats
                self.stats['total_rows'] = total_rows
                self.stats['rows_already_clean'] = valid_clean
                self.stats['rows_processed_now'] = 0
                self.stats['rate_limit_errors_found'] = rate_limit_errors

                print(f"ğŸ“ˆ × ×™×ª×•×— ×§×•×‘×¥ ××¢×•×‘×“:")
                print(f"   â€¢ ×¡×”\"×› ×©×•×¨×•×ª: {total_rows:,}")
                print(f"   â€¢ ×›×‘×¨ × ×§×™×•×ª: {valid_clean:,}")
                print(f"   â€¢ ×©×’×™××•×ª rate limit: {rate_limit_errors:,}")

                if rate_limit_errors > 0:
                    # ××™×¡×•×£ ×˜×§×¡×˜×™× ×œ×¢×™×‘×•×“
                    texts_to_process = []
                    indices_to_process = []

                    for idx, row in df.iterrows():
                        if self.is_rate_limit_error(row.get('cleaned_text')):
                            text = row['text']
                            if pd.notna(text) and len(str(text).strip()) > 0:
                                texts_to_process.append(str(text))
                                indices_to_process.append(idx)

                    print(f"ğŸ”„ ××ª×§×Ÿ {len(texts_to_process)} ×©×’×™××•×ª rate limit...")

                    self.stats['rows_processed_now'] = len(texts_to_process)
                    self.update_status("processing")

                    if texts_to_process:
                        # ×¢×™×‘×•×“ ×¢× batch API
                        self.update_status("processing", progress_percent=25)
                        processed_results = self.process_texts_with_batch(texts_to_process)
                        self.update_status("processing", progress_percent=75)

                        # ×”×—×œ×¤×ª ×©×’×™××•×ª rate limit ×‘×˜×§×¡×˜ × ×§×™
                        for i, idx in enumerate(indices_to_process):
                            if i < len(processed_results):
                                df.loc[idx, 'cleaned_text'] = processed_results[i]

                        self.stats['rows_processed_so_far'] = len(processed_results)
                        self.update_status("processing", progress_percent=90)

                    self.stats['rows_skipped_so_far'] = valid_clean
                else:
                    print("âœ… ×›×œ ×”×˜×§×¡×˜×™× ×›×‘×¨ × ×§×™×™× - ×¨×§ ×¡×•×¤×¨ ××™×œ×™×")
                    self.stats['rows_skipped_so_far'] = total_rows

                df_result = df.copy()
            else:
                # ×§×•×‘×¥ ××§×•×¨×™ - ×¢×™×‘×•×“ ××œ×
                print("ğŸ”„ ×§×•×‘×¥ ××§×•×¨×™ - ××¢×‘×“ ×”×›×œ")

                texts = df['text'].dropna().tolist()
                if not texts:
                    raise ValueError(f"××™×Ÿ ×˜×§×¡×˜×™× ×‘-{filename}")

                # ×¢×“×›×•×Ÿ stats ×œ×¢×™×‘×•×“ ××œ×
                self.stats['total_rows'] = len(texts)
                self.stats['rows_already_clean'] = 0
                self.stats['rows_processed_now'] = len(texts)
                self.stats['rate_limit_errors_found'] = 0

                self.update_status("processing", progress_percent=10)

                # ×¢×™×‘×•×“ ×¢× batch API
                all_cleaned_texts = self.process_texts_with_batch(texts)

                self.stats['rows_processed_so_far'] = len(all_cleaned_texts)
                self.update_status("processing", progress_percent=80)

                df_result = df.copy()
                df_result['cleaned_text'] = all_cleaned_texts[:len(df)]

            # ×”×•×¡×¤×ª/×¢×“×›×•×Ÿ ×¡×¤×™×¨×ª ××™×œ×™×
            print("ğŸ“Š ××—×©×‘ ××¡×¤×¨ ××™×œ×™×...")
            self.update_status("calculating_words")
            df_result['cleaned_text_words'] = df_result['cleaned_text'].apply(self.count_words)

            # ×—×™×©×•×‘ ×¡×˜×˜×™×¡×˜×™×§×•×ª
            original_words = (df_result['n_count'].sum() - 1)
            cleaned_words = df_result['cleaned_text_words'].sum()

            self.update_status("saving")

            target_key = f"{TARGET_PREFIX}{self.dataset_name}/{filename}"

            csv_buffer = StringIO()
            df_result.to_csv(csv_buffer, index=False, encoding='utf-8')

            self.s3_client.put_object(
                Bucket=TARGET_BUCKET,
                Key=target_key,
                Body=csv_buffer.getvalue(),
                ContentType='text/csv'
            )

            self.update_status("completed",
                               progress_percent=100,
                               total_original_words=int(original_words),
                               total_cleaned_words=int(cleaned_words),
                               target_path=f"s3://{TARGET_BUCKET}/{target_key}")

            print(f"âœ… ×”×•×©×œ×: {self.worker_id}")
            print(f"ğŸ’¾ × ×©××¨ ×‘: s3://{TARGET_BUCKET}/{target_key}")
            print(f"ğŸ“Š ××™×œ×™×: {original_words:,} â†’ {cleaned_words:,}")
            return True

        except Exception as e:
            self.update_status("error", error_message=str(e))
            print(f"âŒ ×©×’×™××” ×‘-{self.worker_id}: {e}")
            return False


def main():
    parser = argparse.ArgumentParser(description='Gepeta Single File Worker')
    parser.add_argument('--prefix', required=True, help='Dataset prefix')
    parser.add_argument('--part', type=int, required=True, help='Part number')
    parser.add_argument('--dataset', required=True, help='Dataset name')

    args = parser.parse_args()

    try:
        processor = SingleFileProcessor(
            api_key=None,  # ×œ× × ×“×¨×© ×¢×•×“ ×¢×‘×•×¨ batch API
            prefix=args.prefix,
            part_number=args.part,
            dataset_name=args.dataset
        )

        success = processor.process_file()
        return success

    except Exception as e:
        print(f"âŒ ×©×’×™××” ×§×¨×™×˜×™×ª: {e}")
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)