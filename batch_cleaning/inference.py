# 1. inference.py - ×§×•×“ ×”-inference ×©×™×›× ×¡ ×œ-container
"""
×§×•×‘×¥: inference.py
××˜×¨×”: ×”×§×•×“ ×©×™×¢×‘×•×“ ×‘×ª×•×š ×”-container ×©×œ SageMaker
"""

import json
import torch
import os
from transformers import AutoProcessor, Gemma3ForConditionalGeneration, BitsAndBytesConfig
from huggingface_hub import login


class TextCleaningModel:
    def __init__(self):
        self.model = None
        self.processor = None
        self.loaded = False

    def load_model(self):
        """×˜×¢×™× ×ª ×”××•×“×œ - ×™×§×¨× ×¤×¢× ××—×ª ×‘×”×ª×—×œ×ª ×”container"""
        if self.loaded:
            return

        print("ğŸ”„ Loading model...")

        # ×§×¨×™××ª HF token ×-environment variable
        hf_token = os.environ.get('HF_TOKEN')
        if hf_token:
            login(hf_token)

        model_name = "google/gemma-3-27b-it"

        # ×”×’×“×¨×ª quantization
        quant_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_quant_type="nf4"
        )

        # ×˜×¢×™× ×ª ×”××•×“×œ
        self.model = Gemma3ForConditionalGeneration.from_pretrained(
            model_name,
            quantization_config=quant_config,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True,
            attn_implementation="eager"
        ).eval()

        # ×˜×¢×™× ×ª ×”processor
        self.processor = AutoProcessor.from_pretrained(model_name)

        self.loaded = True
        print("âœ… Model loaded successfully")

    def clean_text(self, text, max_new_tokens=200):
        """× ×™×§×•×™ ×˜×§×¡×˜ ×‘×•×“×“"""
        if not self.loaded:
            self.load_model()

        messages = [
            {
                "role": "system",
                "content": [{"type": "text",
                             "text": "××ª×” ×¢×•×–×¨ ×œ× ×™×§×•×™ ×˜×§×¡×˜×™× ×¢×‘×¨×™×™×. ×§×‘×œ ×˜×§×¡×˜×™× ×¢×‘×¨×™×™× ×¨×•×¢×©×™× ×©×¢×©×•×™×™× ×œ×”×›×™×œ ×¤×’××™ ×§×™×“×•×“ (&quot;), ×§×˜×¢×™ HTML, ×˜×œ×¤×•×Ÿ/××™××™×™×œ, ××™××•×’'×™×, ×¤×¨×¡×•××•×ª, ××• ×ª×‘× ×™×•×ª. ×”×—×–×¨ ×¨×§ ××ª ×”×˜×§×¡×˜ ×”×× ×•×§×”, ×ª×•×š ×©××™×¨×” ×¢×œ ×”××©××¢×•×ª, ×‘×¢×‘×¨×™×ª, ×œ×œ× ×”×¡×‘×¨×™×."}]
            },
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": f"""× ×§×” ×˜×§×¡×˜×™× ×¢×‘×¨×™×™× ××¤×’××™ ×§×™×“×•×“, ×ª×‘× ×™×•×ª HTML, ×¤×¨×¡×•××•×ª, ××™×“×¢ ××™×•×ª×¨ ×•×ª×’×™×•×ª.

×“×•×’××” 1:
×§×œ×˜: ×“×™×•×•×—: ×ª× ×“××¢×© ×©× ×—×©×£ ×‘×™×¨×“×Ÿ ×ª×›× ×Ÿ ×œ×¤×’×•×¢ ×‘×× ×©×™ ×¢×¡×§×™× ×™×©×¨××œ×™×™×
Â© ×¡×•×¤×§ ×¢×œ ×™×“×™ ××¢×¨×™×‘ ×ª× ×“××¢×©... ____________________________________________________________ ×¡×¨×˜×•× ×™× ×©×•×•×™× ×‘-MSN (BuzzVideos)
×¤×œ×˜: ×“×™×•×•×—: ×ª× ×“××¢×© ×©× ×—×©×£ ×‘×™×¨×“×Ÿ ×ª×›× ×Ÿ ×œ×¤×’×•×¢ ×‘×× ×©×™ ×¢×¡×§×™× ×™×©×¨××œ×™×™×
×ª× ×“××¢×© ×©× ×—×©×£ ×‘× ×•×‘××‘×¨ ×”××—×¨×•×Ÿ ×‘×™×¨×“×Ÿ, ×ª×›× ×Ÿ ×‘×™×Ÿ ×”×™×ª×¨ ×œ×¤×’×•×¢ ×‘×× ×©×™ ×¢×¡×§×™× ×™×©×¨××œ×™× ×‘×¨×‘×ª ×¢××•×Ÿ...

×“×•×’××” 2:
×§×œ×˜: ×¡×•×—×¨ ×©×”×¤×™×¥ × ×¤×¦×™× ×‘××©×“×•×“ ×•×¢×¨×™× ××—×¨×•×ª ×”×•×¤×œ×œ ×‘×•×•××˜×¡××¤
××œ×” ×¨×•×–× ×‘×œ×˜... ×”×™×™, ×‘×œ×•×— ×”×—×“×© ×©×œ ××©×“×•×“ × ×˜ ×›×‘×¨ ×‘×™×§×¨×ª? ×›×œ ×”×“×™×¨×•×ª ×œ××›×™×¨×”/×”×©×›×¨×” ×‘××©×“×•×“... ××•×œ×™ ×™×¢× ×™×™×Ÿ ××•×ª×š ×’×
×¤×œ×˜: ×¡×•×—×¨ ×©×”×¤×™×¥ × ×¤×¦×™× ×‘××©×“×•×“ ×•×¢×¨×™× ××—×¨×•×ª ×”×•×¤×œ×œ ×‘×•×•××˜×¡××¤
××œ×” ×¨×•×–× ×‘×œ×˜
××—×™×¨×•×Ÿ ×œ× ×¤×¦×™× ×©×”×•×¤×¥ ×‘××¤×œ×™×§×¦×™×” ×¢"×™ ×¦×¢×™×¨ ×™×¨×•×©×œ××™ ×”×‘×™× ×œ×ª×¤×™×¡×ª×• ×‘×¢×ª ×‘×™×¦×•×¢ ×”×¢×¡×§×”...

×¢×›×©×™×• × ×§×” ××ª ×”×˜×§×¡×˜ ×”×‘×:
{text}

×”×©×‘ ×¨×§ ×¢× ×”×˜×§×¡×˜ ×”×× ×•×§×”:"""}
                ]
            }
        ]

        try:
            # ×¢×™×‘×•×“ ×”×”×•×“×¢×•×ª
            inputs = self.processor.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_dict=True,
                return_tensors="pt"
            )

            # ×”×¢×‘×¨×” ×œ××›×©×™×¨ ×”× ×›×•×Ÿ
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

            input_len = inputs["input_ids"].shape[-1]

            # ×™×¦×™×¨×ª ×ª×’×•×‘×”
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=True,
                    temperature=0.8,
                    top_p=0.95,
                    repetition_penalty=1.1,
                    pad_token_id=self.processor.tokenizer.eos_token_id,
                    eos_token_id=self.processor.tokenizer.eos_token_id,
                    use_cache=True
                )

            # ×¤×¢× ×•×— ×¨×§ ×”×—×œ×§ ×”×—×“×©
            generated_ids = outputs[0][input_len:]
            decoded = self.processor.tokenizer.decode(generated_ids, skip_special_tokens=True)

            return decoded.strip()

        except Exception as e:
            print(f"Error in generation: {e}")
            return f"[ERROR] {str(e)}"


# ×™×¦×™×¨×ª instance ×’×œ×•×‘×œ×™
model_instance = TextCleaningModel()


def model_fn(model_dir):
    """SageMaker model loading function"""
    model_instance.load_model()
    return model_instance


def input_fn(request_body, request_content_type):
    """SageMaker input processing function"""
    if request_content_type == 'application/json':
        data = json.loads(request_body)
        return data
    else:
        raise ValueError(f"Unsupported content type: {request_content_type}")


def predict_fn(data, model):
    """SageMaker prediction function"""
    if isinstance(data, dict):
        # batch ×©×œ ×˜×§×¡×˜×™×
        if 'texts' in data:
            texts = data['texts']
            results = []
            for i, text_item in enumerate(texts):
                text = text_item.get('text', '') if isinstance(text_item, dict) else text_item
                print(f"Processing text {i + 1}/{len(texts)}")
                cleaned = model.clean_text(text)
                results.append({
                    'index': i,
                    'original': text,
                    'cleaned': cleaned
                })
            return {'results': results}

        # ×˜×§×¡×˜ ×‘×•×“×“
        elif 'text' in data:
            text = data['text']
            cleaned = model.clean_text(text)
            return {'cleaned': cleaned}

    raise ValueError("Invalid input format. Expected 'text' or 'texts' key.")


def output_fn(prediction, accept):
    """SageMaker output processing function"""
    if accept == 'application/json':
        return json.dumps(prediction, ensure_ascii=False, indent=2)
    else:
        raise ValueError(f"Unsupported accept type: {accept}")


# ×œ×‘×“×™×§×” ××§×•××™×ª
if __name__ == "__main__":
    # ×˜×¡×˜ ××§×•××™
    test_data = {
        "texts": [
            {"text": "×“×¨×¢×™: ××™×Ÿ ×¡×™×‘×” ×©× ×™×›× ×¡ ×œ×¢×™××•×ª×™× ×‘×§×•××œ×™×¦×™×” ×¡×‘×™×‘ ×—×•×§ ×”×’×™×•×¡. ×ª×’×™×•×ª: ×“×¨×¢×™ ×—×•×§ ×’×™×•×¡"},
            {"text": "×—×“×©×•×ª ×¡×¤×•×¨×˜: ×”×¤×•×¢×œ × ×™×¦×—×”!!! Â© ×›×œ ×”×–×›×•×™×•×ª ×©××•×¨×•×ª... Follow @sport"}
        ]
    }

    model = model_fn("/opt/ml/model")
    prediction = predict_fn(test_data, model)
    result = output_fn(prediction, 'application/json')
    print(result)