
        # ×”×¢×œ××” ×œ-ECR
        push_cmd = f"docker push {image_uri}"
        print(f"ğŸ“¤ ××¢×œ×” ×œ-ECR: {push_cmd}")
        result = subprocess.run(push_cmd, shell=True, capture_output=True, text=True)

        if result.returncode == 0:
            print(f"âœ… ×”×•×¢×œ×” ×‘×”×¦×œ×—×”: {image_uri}")
            return image_uri
        else:
            print(f"âŒ ×©×’×™××” ×‘×”×¢×œ××”: {result.stderr}")
            return None

    def create_model(self, image_uri, model_name=None):
        """
        ×™×¦×™×¨×ª SageMaker Model
        """
        if model_name is None:
            timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
            model_name = f"text-cleaning-model-{timestamp}"

        model_config = {
            'ModelName': model_name,
            'ExecutionRoleArn': self.role_arn,
            'PrimaryContainer': {
                'Image': image_uri,
                'Mode': 'SingleModel',
                'Environment': {
                    'HF_TOKEN': self.hf_token or '',
                    'SAGEMAKER_PROGRAM': 'inference.py',
                    'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/code'
                }
            }
        }

        try:
            response = self.sagemaker.create_model(**model_config)
            print(f"âœ… × ×•×¦×¨ ××•×“×œ: {model_name}")
            return model_name
        except Exception as e:
            print(f"âŒ ×©×’×™××” ×‘×™×¦×™×¨×ª ××•×“×œ: {e}")
            return None

    def upload_input_data(self, texts, bucket_name, key_prefix="text-cleaning-input"):
        """
        ×”×¢×œ××ª × ×ª×•× ×™ ×§×œ×˜ ×œ-S3
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        input_key = f"{key_prefix}/batch_{timestamp}.jsonl"

        # ×”×›× ×ª × ×ª×•× ×™× ×‘×¤×•×¨××˜ JSONL (×©×•×¨×” ×œ×›×œ ×˜×§×¡×˜)
        jsonl_data = []
        for i, text in enumerate(texts):
            jsonl_data.append(json.dumps({
                "texts": [{"index": i, "text": text}]
            }, ensure_ascii=False))

        # ×”×¢×œ××” ×œ-S3
        self.s3.put_object(
            Bucket=bucket_name,
            Key=input_key,
            Body='\n'.join(jsonl_data),
            ContentType='application/jsonl'
        )

        print(f"ğŸ“¤ ×”×•×¢×œ×” ×œS3: s3://{bucket_name}/{input_key}")
        return f"s3://{bucket_name}/{input_key}"

    def create_batch_transform_job(self,
                                   model_name,
                                   input_s3_uri,
                                   output_s3_path,
                                   job_name=None,
                                   instance_type="ml.g5.12xlarge",
                                   instance_count=1):
        """
        ×™×¦×™×¨×ª Batch Transform Job
        """
        if job_name is None:
            timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
            job_name = f"text-cleaning-job-{timestamp}"

        job_config = {
            'TransformJobName': job_name,
            'ModelName': model_name,
            'TransformInput': {
                'DataSource': {
                    'S3DataSource': {
                        'S3DataType': 'S3Prefix',
                        'S3Uri': input_s3_uri
                    }
                },
                'ContentType': 'application/jsonl',
                'SplitType': 'Line'
            },
            'TransformOutput': {
                'S3OutputPath': output_s3_path,
                'Accept': 'application/json'
            },
            'TransformResources': {
                'InstanceType': instance_type,
                'InstanceCount': instance_count
            }
        }

        try:
            response = self.sagemaker.create_transform_job(**job_config)
            print(f"ğŸš€ × ×•×¦×¨ Batch Transform Job: {job_name}")
            print(f"ğŸ“Š ×¡×˜×˜×•×¡: {response['TransformJobArn']}")
            return job_name
        except Exception as e:
            print(f"âŒ ×©×’×™××” ×‘×™×¦×™×¨×ª Job: {e}")
            return None

    def wait_for_job_completion(self, job_name, check_interval=60):
        """
        ×”××ª× ×” ×œ×¡×™×•× ×”Job
        """
        print(f"â³ ×××ª×™×Ÿ ×œ×¡×™×•× Job: {job_name}")

        while True:
            response = self.sagemaker.describe_transform_job(TransformJobName=job_name)
            status = response['TransformJobStatus']

            print(f"ğŸ“Š ×¡×˜×˜×•×¡: {status}")

            if status == 'Completed':
                print("âœ… Job ×”×•×©×œ× ×‘×”×¦×œ×—×”!")
                return True
            elif status == 'Failed':
                print(f"âŒ Job × ×›×©×œ: {response.get('FailureReason', 'Unknown error')}")
                return False
            elif status in ['Stopping', 'Stopped']:
                print(f"ğŸ›‘ Job ×”×•×¤×¡×§: {status}")
                return False

            time.sleep(check_interval)

    def download_results(self, output_s3_path, local_dir="./results"):
        """
        ×”×•×¨×“×ª ×ª×•×¦××•×ª ×S3
        """
        print(f"ğŸ“¥ ××•×¨×™×“ ×ª×•×¦××•×ª ×-{output_s3_path}")

        # Parse S3 path
        if output_s3_path.startswith('s3://'):
            bucket_and_key = output_s3_path[5:].split('/', 1)
            bucket_name = bucket_and_key[0]
            prefix = bucket_and_key[1] if len(bucket_and_key) > 1 else ""

        # Create local directory
        os.makedirs(local_dir, exist_ok=True)

        # List and download files
        response = self.s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)

        if 'Contents' not in response:
            print("âŒ ×œ× × ××¦××• ×ª×•×¦××•×ª")
            return []

        downloaded_files = []
        for obj in response['Contents']:
            key = obj['Key']
            local_file = os.path.join(local_dir, os.path.basename(key))

            self.s3.download_file(bucket_name, key, local_file)
            downloaded_files.append(local_file)
            print(f"âœ… ×”×•×¨×“: {local_file}")

        return downloaded_files

# ×“×•×’××ª ×©×™××•×©
def main():
    # ×”×’×“×¨×•×ª - ×¦×¨×™×š ×œ×¢×“×›×Ÿ!
    REGION = "us-east-1"
    ROLE_ARN = "arn:aws:iam::YOUR_ACCOUNT:role/SageMakerExecutionRole"  # ×¢×“×›×Ÿ!
    HF_TOKEN = "hf_your_token_here"  # ×¢×“×›×Ÿ!
    BUCKET_NAME = "your-s3-bucket"  # ×¢×“×›×Ÿ!

    # ×˜×§×¡×˜×™× ×œ×‘×“×™×§×”
    sample_texts = [
        "×“×¨×¢×™: ××™×Ÿ ×¡×™×‘×” ×©× ×™×›× ×¡ ×œ×¢×™××•×ª×™× ×‘×§×•××œ×™×¦×™×” ×¡×‘×™×‘ ×—×•×§ ×”×’×™×•×¡. ×ª×’×™×•×ª: ×“×¨×¢×™ ×—×•×§ ×’×™×•×¡",
        "×—×“×©×•×ª ×¡×¤×•×¨×˜: ×”×¤×•×¢×œ × ×™×¦×—×”!!! Â© ×›×œ ×”×–×›×•×™×•×ª ×©××•×¨×•×ª... Follow @sport_news",
        "×›×œ×›×œ×”: ×¢×œ×™×™×ª ×”××“×“... <phone>03-1234567</phone> ×œ×¤×¨×˜×™× ×‘××ª×¨ www.example.co.il",
        "×¤×•×œ×™×˜×™×§×”: ×™×©×™×‘×ª ×”×××©×œ×” ×”×—×œ×™×˜×”,,, ×–×” ×–×” ××™×“×¢ ×—×©×•×‘!!! _____ ×¢×•×“ ×‘××ª×¨"
    ]

    # ×™×¦×™×¨×ª ××¢×‘×“
    processor = SageMakerBatchProcessor(
        region_name=REGION,
        role_arn=ROLE_ARN,
        hf_token=HF_TOKEN
    )

    try:
        # 1. ×‘× ×™×™×ª image
        image_uri = processor.build_and_push_image()
        if not image_uri:
            return

        # 2. ×™×¦×™×¨×ª ××•×“×œ
        model_name = processor.create_model(image_uri)
        if not model_name:
            return

        # 3. ×”×¢×œ××ª × ×ª×•× ×™×
        input_s3_uri = processor.upload_input_data(sample_texts, BUCKET_NAME)
        output_s3_path = f"s3://{BUCKET_NAME}/text-cleaning-output/"

        # 4. ×™×¦×™×¨×ª Job
        job_name = processor.create_batch_transform_job(
            model_name=model_name,
            input_s3_uri=input_s3_uri,
            output_s3_path=output_s3_path,
            instance_type="ml.g5.12xlarge"
        )

        if not job_name:
            return

        # 5. ×”××ª× ×” ×œ×¡×™×•×
        if processor.wait_for_job_completion(job_name):
            # 6. ×”×•×¨×“×ª ×ª×•×¦××•×ª
            results = processor.download_results(output_s3_path)
            print(f"ğŸ“ ×ª×•×¦××•×ª × ×©××¨×• ×‘: {results}")

    except Exception as e:
        print(f"âŒ ×©×’×™××” ×›×œ×œ×™×ª: {e}")

if __name__ == "__main__":
    main()