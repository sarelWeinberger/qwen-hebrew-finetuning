{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen Hebrew Fine-tuning: SageMaker P4/P5 Instance Benchmark\n",
    "\n",
    "This notebook provides a complete workflow for:\n",
    "1. Setting up SageMaker infrastructure\n",
    "2. Running performance benchmarks across P4d, P4de, P5, P5e, and P5en instances\n",
    "3. Analyzing results and generating recommendations\n",
    "\n",
    "## Prerequisites\n",
    "- AWS credentials configured\n",
    "- SageMaker execution role with appropriate permissions\n",
    "- S3 bucket for data and model storage\n",
    "- Docker containers built and pushed to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /home/ubuntu/qwen-hebrew-finetuning/.venv/lib/python3.12/site-packages (1.39.3)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.247.1-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/qwen-hebrew-finetuning/.venv/lib/python3.12/site-packages (2.3.0)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting wandb\n",
      "  Using cached wandb-0.21.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: botocore<1.40.0,>=1.39.3 in /home/ubuntu/qwen-hebrew-finetuning/.venv/lib/python3.12/site-packages (from boto3) (1.39.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ubuntu/qwen-hebrew-finetuning/.venv/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /home/ubuntu/qwen-hebrew-finetuning/.venv/lib/python3.12/site-packages (from boto3) (0.13.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ubuntu/qwen-hebrew-finetuning/.venv/lib/python3.12/site-packages (from botocore<1.40.0,>=1.39.3->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ubuntu/qwen-hebrew-finetuning/.venv/lib/python3.12/site-packages (from botocore<1.40.0,>=1.39.3->boto3) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/qwen-hebrew-finetuning/.venv/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.40.0,>=1.39.3->boto3) (1.17.0)\n",
      "Requirement already satisfied: attrs<26,>=24 in /home/ubuntu/qwen-hebrew-finetuning/.venv/lib/python3.12/site-packages (from sagemaker) (25.3.0)\n",
      "Collecting cloudpickle>=2.2.1 (from sagemaker)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting docker (from sagemaker)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fastapi (from sagemaker)\n",
      "  Downloading fastapi-0.115.14-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting google-pasta (from sagemaker)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting graphene<4,>=3 (from sagemaker)\n",
      "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting importlib-metadata<7.0,>=1.4.0 (from sagemaker)\n",
      "  Downloading importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting jsonschema (from sagemaker)\n",
      "  Downloading jsonschema-4.24.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting numpy==1.26.4 (from sagemaker)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting omegaconf<3,>=2.2 (from sagemaker)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting packaging<25,>=23.0 (from sagemaker)\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pathos (from sagemaker)\n",
      "  Downloading pathos-0.3.4-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: platformdirs in /home/ubuntu/qwen-hebrew-finetuning/.venv/lib/python3.12/site-packages (from sagemaker) (4.3.8)\n",
      "Collecting protobuf<6.0,>=3.12 (from sagemaker)\n",
      "  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/qwen-hebrew-finetuning/.venv/lib/python3.12/site-packages (from sagemaker) (7.0.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /home/ubuntu/qwen-hebrew-finetuning/.venv/lib/python3.12/site-packages (from sagemaker) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/ubuntu/qwen-hebrew-finetuning/.venv/lib/python3.12/site-packages (from sagemaker) (2.32.4)\n",
      "Collecting sagemaker-core<2.0.0,>=1.0.17 (from sagemaker)\n",
      "  Downloading sagemaker_core-1.0.42-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting schema (from sagemaker)\n",
      "  Downloading schema-0.7.7-py2.py3-none-any.whl.metadata (34 kB)\n",
      "Collecting smdebug-rulesconfig==1.0.1 (from sagemaker)\n",
      "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting tblib<4,>=1.7.0 (from sagemaker)\n",
      "  Downloading tblib-3.1.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/qwen-hebrew-finetuning/.venv/lib/python3.12/site-packages (from sagemaker) (4.67.1)\n",
      "Collecting uvicorn (from sagemaker)\n",
      "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4,>=3->sagemaker)\n",
      "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4,>=3->sagemaker)\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7.1 in /home/ubuntu/qwen-hebrew-finetuning/.venv/lib/python3.12/site-packages (from graphene<4,>=3->sagemaker) (4.14.1)\n",
      "Collecting zipp>=0.5 (from importlib-metadata<7.0,>=1.4.0->sagemaker)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf<3,>=2.2->sagemaker)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pydantic<3.0.0,>=2.0.0 (from sagemaker-core<2.0.0,>=1.0.17->sagemaker)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting rich<15.0.0,>=14.0.0 (from sagemaker-core<2.0.0,>=1.0.17->sagemaker)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting mock<5.0,>4.0 (from sagemaker-core<2.0.0,>=1.0.17->sagemaker)\n",
      "  Downloading mock-4.0.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema->sagemaker)\n",
      "  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema->sagemaker)\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema->sagemaker)\n",
      "  Downloading rpds_py-0.26.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker)\n",
      "  Using cached pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<15.0.0,>=14.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ubuntu/qwen-hebrew-finetuning/.venv/lib/python3.12/site-packages (from rich<15.0.0,>=14.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.19.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/qwen-hebrew-finetuning/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/qwen-hebrew-finetuning/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.58.5-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (106 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting click!=8.0.0,>=7.1 (from wandb)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Using cached GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Using cached sentry_sdk-2.32.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/qwen-hebrew-finetuning/.venv/lib/python3.12/site-packages (from requests->sagemaker) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/qwen-hebrew-finetuning/.venv/lib/python3.12/site-packages (from requests->sagemaker) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/qwen-hebrew-finetuning/.venv/lib/python3.12/site-packages (from requests->sagemaker) (2025.6.15)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<15.0.0,>=14.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi->sagemaker)\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting anyio<5,>=3.6.2 (from starlette<0.47.0,>=0.40.0->fastapi->sagemaker)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting sniffio>=1.1 (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi->sagemaker)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting ppft>=1.7.7 (from pathos->sagemaker)\n",
      "  Downloading ppft-1.7.7-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting dill>=0.4.0 (from pathos->sagemaker)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pox>=0.3.6 (from pathos->sagemaker)\n",
      "  Downloading pox-0.3.6-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting multiprocess>=0.70.18 (from pathos->sagemaker)\n",
      "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
      "Collecting h11>=0.8 (from uvicorn->sagemaker)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Downloading sagemaker-2.247.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "Downloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
      "Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
      "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading sagemaker_core-1.0.42-py3-none-any.whl (415 kB)\n",
      "Downloading jsonschema-4.24.0-py3-none-any.whl (88 kB)\n",
      "Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading tblib-3.1.0-py3-none-any.whl (12 kB)\n",
      "Downloading matplotlib-3.10.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading wandb-0.21.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading contourpy-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.5-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.26.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (386 kB)\n",
      "Downloading sentry_sdk-2.32.0-py2.py3-none-any.whl (356 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Downloading fastapi-0.115.14-py3-none-any.whl (95 kB)\n",
      "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading pathos-0.3.4-py3-none-any.whl (82 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading multiprocess-0.70.18-py312-none-any.whl (150 kB)\n",
      "Downloading pox-0.3.6-py3-none-any.whl (29 kB)\n",
      "Downloading ppft-1.7.7-py3-none-any.whl (56 kB)\n",
      "Downloading schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
      "Downloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144591 sha256=d3cf56c96917c88204c8580734aff5609fd9a0292a20d1c3fc20102df9781b4e\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/1f/be/48/13754633f1d08d1fbfc60d5e80ae1e5d7329500477685286cd\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: schema, antlr4-python3-runtime, zipp, typing-inspection, tblib, sniffio, smmap, smdebug-rulesconfig, sentry-sdk, rpds-py, pyparsing, pydantic-core, protobuf, ppft, pox, pillow, packaging, omegaconf, numpy, mock, mdurl, kiwisolver, h11, graphql-core, google-pasta, fonttools, dill, cycler, cloudpickle, click, annotated-types, uvicorn, referencing, pydantic, multiprocess, markdown-it-py, importlib-metadata, graphql-relay, gitdb, docker, contourpy, anyio, starlette, rich, pathos, matplotlib, jsonschema-specifications, graphene, gitpython, wandb, seaborn, jsonschema, fastapi, sagemaker-core, sagemaker\n",
      "\u001b[2K  Attempting uninstall: packaging[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/55\u001b[0m [pillow]f]core]onfig]\n",
      "\u001b[2K    Found existing installation: packaging 25.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/55\u001b[0m [pillow]\n",
      "\u001b[2K    Uninstalling packaging-25.0:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/55\u001b[0m [pillow]\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/55\u001b[0m [pillow]\n",
      "\u001b[2K  Attempting uninstall: numpy╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/55\u001b[0m [omegaconf]\n",
      "\u001b[2K    Found existing installation: numpy 2.3.1━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/55\u001b[0m [omegaconf]\n",
      "\u001b[2K    Uninstalling numpy-2.3.1:m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/55\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.3.1━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/55\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: dill0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/55\u001b[0m [fonttools]ta]\n",
      "\u001b[2K    Found existing installation: dill 0.3.8━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/55\u001b[0m [fonttools]\n",
      "\u001b[2K    Uninstalling dill-0.3.8:0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26/55\u001b[0m [dill]]\n",
      "\u001b[2K      Successfully uninstalled dill-0.3.8━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26/55\u001b[0m [dill]\n",
      "\u001b[2K  Attempting uninstall: multiprocess\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33/55\u001b[0m [pydantic]ng]\n",
      "\u001b[2K    Found existing installation: multiprocess 0.70.16━━━━━━━━━━━━━\u001b[0m \u001b[32m34/55\u001b[0m [multiprocess]\n",
      "\u001b[2K    Uninstalling multiprocess-0.70.16:\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34/55\u001b[0m [multiprocess]\n",
      "\u001b[2K      Successfully uninstalled multiprocess-0.70.16━━━━━━━━━━━\u001b[0m \u001b[32m34/55\u001b[0m [multiprocess]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55/55\u001b[0m [sagemaker]agemaker]agemaker-core]ata]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.6.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.4.0 which is incompatible.\n",
      "datasets 3.6.0 requires multiprocess<0.70.17, but you have multiprocess 0.70.18 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.7.0 antlr4-python3-runtime-4.9.3 anyio-4.9.0 click-8.2.1 cloudpickle-3.1.1 contourpy-1.3.2 cycler-0.12.1 dill-0.4.0 docker-7.1.0 fastapi-0.115.14 fonttools-4.58.5 gitdb-4.0.12 gitpython-3.1.44 google-pasta-0.2.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 h11-0.16.0 importlib-metadata-6.11.0 jsonschema-4.24.0 jsonschema-specifications-2025.4.1 kiwisolver-1.4.8 markdown-it-py-3.0.0 matplotlib-3.10.3 mdurl-0.1.2 mock-4.0.3 multiprocess-0.70.18 numpy-1.26.4 omegaconf-2.3.0 packaging-24.2 pathos-0.3.4 pillow-11.3.0 pox-0.3.6 ppft-1.7.7 protobuf-5.29.5 pydantic-2.11.7 pydantic-core-2.33.2 pyparsing-3.2.3 referencing-0.36.2 rich-14.0.0 rpds-py-0.26.0 sagemaker-2.247.1 sagemaker-core-1.0.42 schema-0.7.7 seaborn-0.13.2 sentry-sdk-2.32.0 smdebug-rulesconfig-1.0.1 smmap-5.0.2 sniffio-1.3.1 starlette-0.46.2 tblib-3.1.0 typing-inspection-0.4.1 uvicorn-0.35.0 wandb-0.21.0 zipp-3.23.0\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install boto3 sagemaker pandas matplotlib seaborn wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the scripts directory to Python path\n",
    "sys.path.append('../scripts')\n",
    "sys.path.append('../infrastructure')\n",
    "\n",
    "from benchmark_runner import SageMakerBenchmarkRunner\n",
    "#from containers.training.scripts.sagemaker_jobs import SageMakerJobManager\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Update these variables with your AWS configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-1\n",
      "Bucket: gepeta-datasets\n",
      "Dataset: s3://gepeta-datasets/processed/wikipedia/\n",
      "Instance types: ['ml.p4d.24xlarge', 'ml.p4de.24xlarge', 'ml.p5.48xlarge']\n"
     ]
    }
   ],
   "source": [
    "# AWS Configuration\n",
    "REGION = 'us-east-1'\n",
    "BUCKET_NAME = 'gepeta-datasets'  # Update with your bucket name\n",
    "ROLE_ARN = 'arn:aws:iam::670967753077:role/SageMakerExecutionRole'  # Update with your role\n",
    "\n",
    "# Dataset Configuration\n",
    "DATASET_S3_PATH = f's3://{BUCKET_NAME}/processed/wikipedia/'  # Path to your processed Hebrew dataset\n",
    "\n",
    "# Benchmark Configuration\n",
    "INSTANCE_TYPES = ['ml.p4d.24xlarge', 'ml.p4de.24xlarge', 'ml.p5.48xlarge']\n",
    "BENCHMARK_EPOCHS = 1\n",
    "BENCHMARK_MAX_STEPS = 100  # For quick benchmarking\n",
    "\n",
    "# W&B Configuration (optional)\n",
    "WANDB_PROJECT = 'qwen-hebrew-sagemaker-benchmark'\n",
    "WANDB_ENTITY = 'your-wandb-entity'  # Update with your W&B entity\n",
    "\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"Bucket: {BUCKET_NAME}\")\n",
    "print(f\"Dataset: {DATASET_S3_PATH}\")\n",
    "print(f\"Instance types: {INSTANCE_TYPES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Account ID: 670967753077\n",
      "✓ S3 bucket gepeta-datasets is accessible\n",
      "✓ Dataset found at s3://gepeta-datasets/processed/wikipedia/\n",
      "✓ Training container image found: 670967753077.dkr.ecr.us-east-1.amazonaws.com/qwen-hebrew-training:latest\n"
     ]
    }
   ],
   "source": [
    "# Initialize AWS clients\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=REGION)\n",
    "s3_client = boto3.client('s3', region_name=REGION)\n",
    "sts_client = boto3.client('sts')\n",
    "\n",
    "# Get account information\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "print(f\"Account ID: {account_id}\")\n",
    "\n",
    "# Verify S3 bucket access\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=BUCKET_NAME)\n",
    "    print(f\"✓ S3 bucket {BUCKET_NAME} is accessible\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ S3 bucket {BUCKET_NAME} is not accessible: {e}\")\n",
    "\n",
    "# Check if dataset exists\n",
    "try:\n",
    "    dataset_key = DATASET_S3_PATH.replace(f's3://{BUCKET_NAME}/', '')\n",
    "    response = s3_client.list_objects_v2(Bucket=BUCKET_NAME, Prefix=dataset_key, MaxKeys=1)\n",
    "    if 'Contents' in response:\n",
    "        print(f\"✓ Dataset found at {DATASET_S3_PATH}\")\n",
    "    else:\n",
    "        print(f\"✗ Dataset not found at {DATASET_S3_PATH}\")\n",
    "        print(\"Please run data preparation first or update the dataset path\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error checking dataset: {e}\")\n",
    "\n",
    "# Verify ECR images\n",
    "ecr_client = boto3.client('ecr', region_name=REGION)\n",
    "training_image = f\"{account_id}.dkr.ecr.{REGION}.amazonaws.com/qwen-hebrew-training:latest\"\n",
    "\n",
    "try:\n",
    "    ecr_client.describe_images(\n",
    "        repositoryName='qwen-hebrew-training',\n",
    "        imageIds=[{'imageTag': 'latest'}]\n",
    "    )\n",
    "    print(f\"✓ Training container image found: {training_image}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Training container image not found: {e}\")\n",
    "    print(\"Please build and push the Docker container first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Benchmark Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark runner initialized successfully\n",
      "Training image URI: 670967753077.dkr.ecr.us-east-1.amazonaws.com/qwen-hebrew-training:latest\n"
     ]
    }
   ],
   "source": [
    "# Initialize benchmark runner\n",
    "benchmark_runner = SageMakerBenchmarkRunner(\n",
    "    role_arn=ROLE_ARN,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    region=REGION\n",
    ")\n",
    "\n",
    "print(\"Benchmark runner initialized successfully\")\n",
    "print(f\"Training image URI: {benchmark_runner.get_training_image_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run Performance Benchmark\n",
    "\n",
    "This will submit training jobs to all specified instance types and monitor their progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 20:36:39,893 - INFO - Starting benchmark across 3 instance types\n",
      "2025-07-06 20:36:39,895 - INFO - Instance types: ['ml.p4d.24xlarge', 'ml.p4de.24xlarge', 'ml.p5.48xlarge']\n",
      "2025-07-06 20:36:39,895 - INFO - Submitting training job: qwen-benchmark-p4d-24xlarge-20250706-203639 on ml.p4d.24xlarge\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting benchmark across 3 instance types...\n",
      "This will take approximately 1-2 hours to complete\n",
      "Instance types: ['ml.p4d.24xlarge', 'ml.p4de.24xlarge', 'ml.p5.48xlarge']\n",
      "Benchmark started at: 2025-07-06 20:36:39.893648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 20:36:40,124 - ERROR - Failed to submit job for ml.p4d.24xlarge: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.p4d.24xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please use AWS Service Quotas to request an increase for this quota. If AWS Service Quotas is not available, contact AWS support to request an increase for this quota.\n",
      "2025-07-06 20:36:40,124 - INFO - Submitting training job: qwen-benchmark-p4de-24xlarge-20250706-203640 on ml.p4de.24xlarge\n",
      "2025-07-06 20:36:40,414 - ERROR - Failed to submit job for ml.p4de.24xlarge: An error occurred (ValidationException) when calling the CreateTrainingJob operation: Could not assume role arn:aws:iam::670967753077:role/SageMakerExecutionRole. Please ensure that the role exists and allows principal 'sagemaker.amazonaws.com' to assume the role.\n",
      "2025-07-06 20:36:40,415 - INFO - Submitting training job: qwen-benchmark-p5-48xlarge-20250706-203640 on ml.p5.48xlarge\n",
      "2025-07-06 20:36:41,406 - ERROR - Failed to submit job for ml.p5.48xlarge: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.p5.48xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please use AWS Service Quotas to request an increase for this quota. If AWS Service Quotas is not available, contact AWS support to request an increase for this quota.\n",
      "2025-07-06 20:36:41,407 - ERROR - No training jobs were successfully submitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmark completed at: 2025-07-06 20:36:41.408575\n",
      "Total benchmark time: 0:00:01.514927\n",
      "Collected results for 0 jobs\n"
     ]
    }
   ],
   "source": [
    "# Start benchmark\n",
    "print(f\"Starting benchmark across {len(INSTANCE_TYPES)} instance types...\")\n",
    "print(f\"This will take approximately 1-2 hours to complete\")\n",
    "print(f\"Instance types: {INSTANCE_TYPES}\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(f\"Benchmark started at: {start_time}\")\n",
    "\n",
    "# Run the benchmark\n",
    "results = benchmark_runner.run_benchmark(\n",
    "    instance_types=INSTANCE_TYPES,\n",
    "    dataset_path=DATASET_S3_PATH,\n",
    "    epochs=BENCHMARK_EPOCHS,\n",
    "    max_steps=BENCHMARK_MAX_STEPS\n",
    ")\n",
    "\n",
    "end_time = datetime.now()\n",
    "total_time = end_time - start_time\n",
    "print(f\"\\nBenchmark completed at: {end_time}\")\n",
    "print(f\"Total benchmark time: {total_time}\")\n",
    "print(f\"Collected results for {len(results)} jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comparison report\n",
    "if results:\n",
    "    df = benchmark_runner.generate_comparison_report(results)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"BENCHMARK RESULTS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Key metrics to display\n",
    "    display_columns = [\n",
    "        'instance_type', 'status', 'training_duration_hours', \n",
    "        'actual_cost', 'avg_tokens_per_second', 'avg_gpu_utilization',\n",
    "        'cost_effectiveness', 'overall_score'\n",
    "    ]\n",
    "    \n",
    "    available_columns = [col for col in display_columns if col in df.columns]\n",
    "    display_df = df[available_columns].round(3)\n",
    "    \n",
    "    print(display_df.to_string(index=False))\n",
    "    \n",
    "    # Save results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"benchmark_results_{timestamp}.csv\"\n",
    "    df.to_csv(results_file, index=False)\n",
    "    print(f\"\\nDetailed results saved to: {results_file}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No benchmark results available\")\n",
    "    df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Create performance comparison plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('SageMaker P4/P5 Instance Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Training Speed (Tokens per Second)\n",
    "    if 'avg_tokens_per_second' in df.columns:\n",
    "        ax1 = axes[0, 0]\n",
    "        bars1 = ax1.bar(df['instance_type'], df['avg_tokens_per_second'], \n",
    "                       color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "        ax1.set_title('Training Speed (Tokens/Second)', fontweight='bold')\n",
    "        ax1.set_ylabel('Tokens per Second')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars1, df['avg_tokens_per_second']):\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
    "                    f'{value:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Cost Comparison\n",
    "    if 'actual_cost' in df.columns:\n",
    "        ax2 = axes[0, 1]\n",
    "        bars2 = ax2.bar(df['instance_type'], df['actual_cost'], \n",
    "                       color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "        ax2.set_title('Training Cost (USD)', fontweight='bold')\n",
    "        ax2.set_ylabel('Cost (USD)')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars2, df['actual_cost']):\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                    f'${value:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. Cost Effectiveness\n",
    "    if 'cost_effectiveness' in df.columns:\n",
    "        ax3 = axes[1, 0]\n",
    "        bars3 = ax3.bar(df['instance_type'], df['cost_effectiveness'], \n",
    "                       color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "        ax3.set_title('Cost Effectiveness (Performance/Dollar)', fontweight='bold')\n",
    "        ax3.set_ylabel('Cost Effectiveness Score')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars3, df['cost_effectiveness']):\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. GPU Utilization\n",
    "    if 'avg_gpu_utilization' in df.columns:\n",
    "        ax4 = axes[1, 1]\n",
    "        bars4 = ax4.bar(df['instance_type'], df['avg_gpu_utilization'], \n",
    "                       color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "        ax4.set_title('Average GPU Utilization (%)', fontweight='bold')\n",
    "        ax4.set_ylabel('GPU Utilization (%)')\n",
    "        ax4.set_ylim(0, 100)\n",
    "        ax4.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars4, df['avg_gpu_utilization']):\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                    f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the plot\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    plot_file = f\"benchmark_comparison_{timestamp}.png\"\n",
    "    fig.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Performance comparison plot saved to: {plot_file}\")\n",
    "\n",
    "else:\n",
    "    print(\"No data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generate Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RECOMMENDATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Best overall performance\n",
    "    if 'overall_score' in df.columns:\n",
    "        best_overall = df.loc[df['overall_score'].idxmax()]\n",
    "        print(f\"🏆 BEST OVERALL PERFORMANCE: {best_overall['instance_type']}\")\n",
    "        print(f\"   • Overall Score: {best_overall['overall_score']:.3f}\")\n",
    "        if 'actual_cost' in best_overall:\n",
    "            print(f\"   • Cost: ${best_overall['actual_cost']:.2f}\")\n",
    "        if 'avg_tokens_per_second' in best_overall:\n",
    "            print(f\"   • Speed: {best_overall['avg_tokens_per_second']:.0f} tokens/sec\")\n",
    "        if 'avg_gpu_utilization' in best_overall:\n",
    "            print(f\"   • GPU Utilization: {best_overall['avg_gpu_utilization']:.1f}%\")\n",
    "    \n",
    "    # Most cost-effective\n",
    "    if 'cost_effectiveness' in df.columns:\n",
    "        best_cost = df.loc[df['cost_effectiveness'].idxmax()]\n",
    "        print(f\"\\n💰 MOST COST-EFFECTIVE: {best_cost['instance_type']}\")\n",
    "        print(f\"   • Cost Effectiveness: {best_cost['cost_effectiveness']:.3f}\")\n",
    "        if 'actual_cost' in best_cost:\n",
    "            print(f\"   • Cost: ${best_cost['actual_cost']:.2f}\")\n",
    "        if 'avg_tokens_per_second' in best_cost:\n",
    "            print(f\"   • Speed: {best_cost['avg_tokens_per_second']:.0f} tokens/sec\")\n",
    "    \n",
    "    # Fastest training\n",
    "    if 'avg_tokens_per_second' in df.columns:\n",
    "        fastest = df.loc[df['avg_tokens_per_second'].idxmax()]\n",
    "        print(f\"\\n⚡ FASTEST TRAINING: {fastest['instance_type']}\")\n",
    "        print(f\"   • Speed: {fastest['avg_tokens_per_second']:.0f} tokens/sec\")\n",
    "        if 'training_duration_hours' in fastest:\n",
    "            print(f\"   • Duration: {fastest['training_duration_hours']:.2f} hours\")\n",
    "        if 'actual_cost' in fastest:\n",
    "            print(f\"   • Cost: ${fastest['actual_cost']:.2f}\")\n",
    "    \n",
    "    # Usage recommendations\n",
    "    print(f\"\\n📋 USAGE RECOMMENDATIONS:\")\n",
    "    print(f\"   • For budget-conscious projects: Use the most cost-effective instance\")\n",
    "    print(f\"   • For time-critical projects: Use the fastest instance\")\n",
    "    print(f\"   • For balanced workloads: Use the best overall performance instance\")\n",
    "    print(f\"   • For production training: Consider the instance with best GPU utilization\")\n",
    "    \n",
    "    # Cost projections\n",
    "    print(f\"\\n💡 COST PROJECTIONS FOR FULL TRAINING (3 epochs):\")\n",
    "    for _, row in df.iterrows():\n",
    "        if 'actual_cost' in row and 'training_duration_hours' in row:\n",
    "            # Estimate cost for 3 epochs (assuming linear scaling)\n",
    "            full_cost = (row['actual_cost'] / BENCHMARK_EPOCHS) * 3\n",
    "            full_time = (row['training_duration_hours'] / BENCHMARK_EPOCHS) * 3\n",
    "            print(f\"   • {row['instance_type']}: ~${full_cost:.2f} (~{full_time:.1f} hours)\")\n",
    "\n",
    "else:\n",
    "    print(\"No data available for recommendations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Complete Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Create comprehensive report\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    report_file = f\"qwen_hebrew_benchmark_report_{timestamp}.md\"\n",
    "    \n",
    "    with open(report_file, 'w') as f:\n",
    "        f.write(\"# Qwen Hebrew Fine-tuning: SageMaker P4/P5 Instance Benchmark Report\\n\\n\")\n",
    "        f.write(f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Benchmark Configuration\\n\\n\")\n",
    "        f.write(f\"- **Instance Types:** {', '.join(INSTANCE_TYPES)}\\n\")\n",
    "        f.write(f\"- **Dataset:** {DATASET_S3_PATH}\\n\")\n",
    "        f.write(f\"- **Epochs:** {BENCHMARK_EPOCHS}\\n\")\n",
    "        f.write(f\"- **Max Steps:** {BENCHMARK_MAX_STEPS}\\n\")\n",
    "        f.write(f\"- **Region:** {REGION}\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Results Summary\\n\\n\")\n",
    "        f.write(df.to_markdown(index=False, floatfmt=\".3f\"))\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Recommendations\\n\\n\")\n",
    "        \n",
    "        if 'overall_score' in df.columns:\n",
    "            best_overall = df.loc[df['overall_score'].idxmax()]\n",
    "            f.write(f\"### Best Overall Performance: {best_overall['instance_type']}\\n\")\n",
    "            f.write(f\"- Overall Score: {best_overall['overall_score']:.3f}\\n\")\n",
    "            if 'actual_cost' in best_overall:\n",
    "                f.write(f\"- Cost: ${best_overall['actual_cost']:.2f}\\n\")\n",
    "            if 'avg_tokens_per_second' in best_overall:\n",
    "                f.write(f\"- Speed: {best_overall['avg_tokens_per_second']:.0f} tokens/sec\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        if 'cost_effectiveness' in df.columns:\n",
    "            best_cost = df.loc[df['cost_effectiveness'].idxmax()]\n",
    "            f.write(f\"### Most Cost-Effective: {best_cost['instance_type']}\\n\")\n",
    "            f.write(f\"- Cost Effectiveness: {best_cost['cost_effectiveness']:.3f}\\n\")\n",
    "            if 'actual_cost' in best_cost:\n",
    "                f.write(f\"- Cost: ${best_cost['actual_cost']:.2f}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"## Usage Guidelines\\n\\n\")\n",
    "        f.write(\"- **Budget-conscious projects:** Use the most cost-effective instance\\n\")\n",
    "        f.write(\"- **Time-critical projects:** Use the fastest instance\\n\")\n",
    "        f.write(\"- **Balanced workloads:** Use the best overall performance instance\\n\")\n",
    "        f.write(\"- **Production training:** Consider GPU utilization and stability\\n\")\n",
    "    \n",
    "    print(f\"\\nComprehensive report saved to: {report_file}\")\n",
    "    \n",
    "    # Upload report to S3\n",
    "    try:\n",
    "        s3_key = f\"benchmark_reports/{report_file}\"\n",
    "        s3_client.upload_file(report_file, BUCKET_NAME, s3_key)\n",
    "        print(f\"Report uploaded to: s3://{BUCKET_NAME}/{s3_key}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload report to S3: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"No data available for report generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Based on the benchmark results, you can now:\n",
    "\n",
    "1. **Choose the optimal instance type** for your production training\n",
    "2. **Submit a full training job** using the recommended instance\n",
    "3. **Scale your training** based on the performance characteristics\n",
    "4. **Optimize costs** by selecting the most cost-effective option\n",
    "\n",
    "### Submit Production Training Job\n",
    "\n",
    "Use the cell below to submit a production training job with your chosen instance type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit production training job (uncomment and modify as needed)\n",
    "\n",
    "# # Choose your preferred instance type based on benchmark results\n",
    "# PRODUCTION_INSTANCE_TYPE = 'ml.p4de.24xlarge'  # Update based on your benchmark results\n",
    "# PRODUCTION_EPOCHS = 3\n",
    "\n",
    "# # Initialize job manager\n",
    "# job_manager = SageMakerJobManager(\n",
    "#     role_arn=ROLE_ARN,\n",
    "#     bucket_name=BUCKET_NAME,\n",
    "#     region=REGION\n",
    "# )\n",
    "\n",
    "# # Submit production training job\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# production_job_name = f\"qwen-hebrew-production-{timestamp}\"\n",
    "\n",
    "# job_name = job_manager.submit_training_job(\n",
    "#     job_name=production_job_name,\n",
    "#     instance_type=PRODUCTION_INSTANCE_TYPE,\n",
    "#     dataset_path=DATASET_S3_PATH,\n",
    "#     epochs=PRODUCTION_EPOCHS,\n",
    "#     wandb_project='qwen-hebrew-production',\n",
    "#     checkpoint_s3_uri=f's3://{BUCKET_NAME}/checkpoints/{production_job_name}/'\n",
    "# )\n",
    "\n",
    "# print(f\"Production training job submitted: {job_name}\")\n",
    "# print(f\"Instance type: {PRODUCTION_INSTANCE_TYPE}\")\n",
    "# print(f\"Epochs: {PRODUCTION_EPOCHS}\")\n",
    "# print(f\"Monitor progress in SageMaker console or W&B dashboard\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
