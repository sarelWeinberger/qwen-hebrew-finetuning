{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen Hebrew Fine-tuning: SageMaker P4/P5 Instance Benchmark\n",
    "\n",
    "This notebook provides a complete workflow for:\n",
    "1. Setting up SageMaker infrastructure\n",
    "2. Running performance benchmarks across P4d, P4de, P5, P5e, and P5en instances\n",
    "3. Analyzing results and generating recommendations\n",
    "\n",
    "## Prerequisites\n",
    "- AWS credentials configured\n",
    "- SageMaker execution role with appropriate permissions\n",
    "- S3 bucket for data and model storage\n",
    "- Docker containers built and pushed to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install boto3 sagemaker pandas matplotlib seaborn wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the scripts directory to Python path\n",
    "sys.path.append('../scripts')\n",
    "sys.path.append('../infrastructure')\n",
    "\n",
    "from benchmark_runner import SageMakerBenchmarkRunner\n",
    "from sagemaker_jobs import SageMakerJobManager\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Update these variables with your AWS configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Configuration\n",
    "REGION = 'us-east-1'\n",
    "BUCKET_NAME = 'your-sagemaker-bucket'  # Update with your bucket name\n",
    "ROLE_ARN = 'arn:aws:iam::YOUR_ACCOUNT:role/SageMakerExecutionRole'  # Update with your role\n",
    "\n",
    "# Dataset Configuration\n",
    "DATASET_S3_PATH = f's3://{BUCKET_NAME}/processed-data/dataset/'  # Path to your processed Hebrew dataset\n",
    "\n",
    "# Benchmark Configuration\n",
    "INSTANCE_TYPES = ['ml.p4d.24xlarge', 'ml.p4de.24xlarge', 'ml.p5.48xlarge']\n",
    "BENCHMARK_EPOCHS = 1\n",
    "BENCHMARK_MAX_STEPS = 100  # For quick benchmarking\n",
    "\n",
    "# W&B Configuration (optional)\n",
    "WANDB_PROJECT = 'qwen-hebrew-sagemaker-benchmark'\n",
    "WANDB_ENTITY = 'your-wandb-entity'  # Update with your W&B entity\n",
    "\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"Bucket: {BUCKET_NAME}\")\n",
    "print(f\"Dataset: {DATASET_S3_PATH}\")\n",
    "print(f\"Instance types: {INSTANCE_TYPES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AWS clients\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=REGION)\n",
    "s3_client = boto3.client('s3', region_name=REGION)\n",
    "sts_client = boto3.client('sts')\n",
    "\n",
    "# Get account information\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "print(f\"Account ID: {account_id}\")\n",
    "\n",
    "# Verify S3 bucket access\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=BUCKET_NAME)\n",
    "    print(f\"‚úì S3 bucket {BUCKET_NAME} is accessible\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó S3 bucket {BUCKET_NAME} is not accessible: {e}\")\n",
    "\n",
    "# Check if dataset exists\n",
    "try:\n",
    "    dataset_key = DATASET_S3_PATH.replace(f's3://{BUCKET_NAME}/', '')\n",
    "    response = s3_client.list_objects_v2(Bucket=BUCKET_NAME, Prefix=dataset_key, MaxKeys=1)\n",
    "    if 'Contents' in response:\n",
    "        print(f\"‚úì Dataset found at {DATASET_S3_PATH}\")\n",
    "    else:\n",
    "        print(f\"‚úó Dataset not found at {DATASET_S3_PATH}\")\n",
    "        print(\"Please run data preparation first or update the dataset path\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error checking dataset: {e}\")\n",
    "\n",
    "# Verify ECR images\n",
    "ecr_client = boto3.client('ecr', region_name=REGION)\n",
    "training_image = f\"{account_id}.dkr.ecr.{REGION}.amazonaws.com/qwen-hebrew-training:latest\"\n",
    "\n",
    "try:\n",
    "    ecr_client.describe_images(\n",
    "        repositoryName='qwen-hebrew-training',\n",
    "        imageIds=[{'imageTag': 'latest'}]\n",
    "    )\n",
    "    print(f\"‚úì Training container image found: {training_image}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Training container image not found: {e}\")\n",
    "    print(\"Please build and push the Docker container first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Benchmark Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize benchmark runner\n",
    "benchmark_runner = SageMakerBenchmarkRunner(\n",
    "    role_arn=ROLE_ARN,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    region=REGION\n",
    ")\n",
    "\n",
    "print(\"Benchmark runner initialized successfully\")\n",
    "print(f\"Training image URI: {benchmark_runner.get_training_image_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run Performance Benchmark\n",
    "\n",
    "This will submit training jobs to all specified instance types and monitor their progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start benchmark\n",
    "print(f\"Starting benchmark across {len(INSTANCE_TYPES)} instance types...\")\n",
    "print(f\"This will take approximately 1-2 hours to complete\")\n",
    "print(f\"Instance types: {INSTANCE_TYPES}\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(f\"Benchmark started at: {start_time}\")\n",
    "\n",
    "# Run the benchmark\n",
    "results = benchmark_runner.run_benchmark(\n",
    "    instance_types=INSTANCE_TYPES,\n",
    "    dataset_path=DATASET_S3_PATH,\n",
    "    epochs=BENCHMARK_EPOCHS,\n",
    "    max_steps=BENCHMARK_MAX_STEPS\n",
    ")\n",
    "\n",
    "end_time = datetime.now()\n",
    "total_time = end_time - start_time\n",
    "print(f\"\\nBenchmark completed at: {end_time}\")\n",
    "print(f\"Total benchmark time: {total_time}\")\n",
    "print(f\"Collected results for {len(results)} jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comparison report\n",
    "if results:\n",
    "    df = benchmark_runner.generate_comparison_report(results)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"BENCHMARK RESULTS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Key metrics to display\n",
    "    display_columns = [\n",
    "        'instance_type', 'status', 'training_duration_hours', \n",
    "        'actual_cost', 'avg_tokens_per_second', 'avg_gpu_utilization',\n",
    "        'cost_effectiveness', 'overall_score'\n",
    "    ]\n",
    "    \n",
    "    available_columns = [col for col in display_columns if col in df.columns]\n",
    "    display_df = df[available_columns].round(3)\n",
    "    \n",
    "    print(display_df.to_string(index=False))\n",
    "    \n",
    "    # Save results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"benchmark_results_{timestamp}.csv\"\n",
    "    df.to_csv(results_file, index=False)\n",
    "    print(f\"\\nDetailed results saved to: {results_file}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No benchmark results available\")\n",
    "    df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Create performance comparison plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('SageMaker P4/P5 Instance Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Training Speed (Tokens per Second)\n",
    "    if 'avg_tokens_per_second' in df.columns:\n",
    "        ax1 = axes[0, 0]\n",
    "        bars1 = ax1.bar(df['instance_type'], df['avg_tokens_per_second'], \n",
    "                       color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "        ax1.set_title('Training Speed (Tokens/Second)', fontweight='bold')\n",
    "        ax1.set_ylabel('Tokens per Second')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars1, df['avg_tokens_per_second']):\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
    "                    f'{value:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Cost Comparison\n",
    "    if 'actual_cost' in df.columns:\n",
    "        ax2 = axes[0, 1]\n",
    "        bars2 = ax2.bar(df['instance_type'], df['actual_cost'], \n",
    "                       color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "        ax2.set_title('Training Cost (USD)', fontweight='bold')\n",
    "        ax2.set_ylabel('Cost (USD)')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars2, df['actual_cost']):\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                    f'${value:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. Cost Effectiveness\n",
    "    if 'cost_effectiveness' in df.columns:\n",
    "        ax3 = axes[1, 0]\n",
    "        bars3 = ax3.bar(df['instance_type'], df['cost_effectiveness'], \n",
    "                       color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "        ax3.set_title('Cost Effectiveness (Performance/Dollar)', fontweight='bold')\n",
    "        ax3.set_ylabel('Cost Effectiveness Score')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars3, df['cost_effectiveness']):\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. GPU Utilization\n",
    "    if 'avg_gpu_utilization' in df.columns:\n",
    "        ax4 = axes[1, 1]\n",
    "        bars4 = ax4.bar(df['instance_type'], df['avg_gpu_utilization'], \n",
    "                       color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "        ax4.set_title('Average GPU Utilization (%)', fontweight='bold')\n",
    "        ax4.set_ylabel('GPU Utilization (%)')\n",
    "        ax4.set_ylim(0, 100)\n",
    "        ax4.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars4, df['avg_gpu_utilization']):\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                    f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the plot\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    plot_file = f\"benchmark_comparison_{timestamp}.png\"\n",
    "    fig.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Performance comparison plot saved to: {plot_file}\")\n",
    "\n",
    "else:\n",
    "    print(\"No data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generate Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RECOMMENDATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Best overall performance\n",
    "    if 'overall_score' in df.columns:\n",
    "        best_overall = df.loc[df['overall_score'].idxmax()]\n",
    "        print(f\"üèÜ BEST OVERALL PERFORMANCE: {best_overall['instance_type']}\")\n",
    "        print(f\"   ‚Ä¢ Overall Score: {best_overall['overall_score']:.3f}\")\n",
    "        if 'actual_cost' in best_overall:\n",
    "            print(f\"   ‚Ä¢ Cost: ${best_overall['actual_cost']:.2f}\")\n",
    "        if 'avg_tokens_per_second' in best_overall:\n",
    "            print(f\"   ‚Ä¢ Speed: {best_overall['avg_tokens_per_second']:.0f} tokens/sec\")\n",
    "        if 'avg_gpu_utilization' in best_overall:\n",
    "            print(f\"   ‚Ä¢ GPU Utilization: {best_overall['avg_gpu_utilization']:.1f}%\")\n",
    "    \n",
    "    # Most cost-effective\n",
    "    if 'cost_effectiveness' in df.columns:\n",
    "        best_cost = df.loc[df['cost_effectiveness'].idxmax()]\n",
    "        print(f\"\\nüí∞ MOST COST-EFFECTIVE: {best_cost['instance_type']}\")\n",
    "        print(f\"   ‚Ä¢ Cost Effectiveness: {best_cost['cost_effectiveness']:.3f}\")\n",
    "        if 'actual_cost' in best_cost:\n",
    "            print(f\"   ‚Ä¢ Cost: ${best_cost['actual_cost']:.2f}\")\n",
    "        if 'avg_tokens_per_second' in best_cost:\n",
    "            print(f\"   ‚Ä¢ Speed: {best_cost['avg_tokens_per_second']:.0f} tokens/sec\")\n",
    "    \n",
    "    # Fastest training\n",
    "    if 'avg_tokens_per_second' in df.columns:\n",
    "        fastest = df.loc[df['avg_tokens_per_second'].idxmax()]\n",
    "        print(f\"\\n‚ö° FASTEST TRAINING: {fastest['instance_type']}\")\n",
    "        print(f\"   ‚Ä¢ Speed: {fastest['avg_tokens_per_second']:.0f} tokens/sec\")\n",
    "        if 'training_duration_hours' in fastest:\n",
    "            print(f\"   ‚Ä¢ Duration: {fastest['training_duration_hours']:.2f} hours\")\n",
    "        if 'actual_cost' in fastest:\n",
    "            print(f\"   ‚Ä¢ Cost: ${fastest['actual_cost']:.2f}\")\n",
    "    \n",
    "    # Usage recommendations\n",
    "    print(f\"\\nüìã USAGE RECOMMENDATIONS:\")\n",
    "    print(f\"   ‚Ä¢ For budget-conscious projects: Use the most cost-effective instance\")\n",
    "    print(f\"   ‚Ä¢ For time-critical projects: Use the fastest instance\")\n",
    "    print(f\"   ‚Ä¢ For balanced workloads: Use the best overall performance instance\")\n",
    "    print(f\"   ‚Ä¢ For production training: Consider the instance with best GPU utilization\")\n",
    "    \n",
    "    # Cost projections\n",
    "    print(f\"\\nüí° COST PROJECTIONS FOR FULL TRAINING (3 epochs):\")\n",
    "    for _, row in df.iterrows():\n",
    "        if 'actual_cost' in row and 'training_duration_hours' in row:\n",
    "            # Estimate cost for 3 epochs (assuming linear scaling)\n",
    "            full_cost = (row['actual_cost'] / BENCHMARK_EPOCHS) * 3\n",
    "            full_time = (row['training_duration_hours'] / BENCHMARK_EPOCHS) * 3\n",
    "            print(f\"   ‚Ä¢ {row['instance_type']}: ~${full_cost:.2f} (~{full_time:.1f} hours)\")\n",
    "\n",
    "else:\n",
    "    print(\"No data available for recommendations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Complete Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Create comprehensive report\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    report_file = f\"qwen_hebrew_benchmark_report_{timestamp}.md\"\n",
    "    \n",
    "    with open(report_file, 'w') as f:\n",
    "        f.write(\"# Qwen Hebrew Fine-tuning: SageMaker P4/P5 Instance Benchmark Report\\n\\n\")\n",
    "        f.write(f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Benchmark Configuration\\n\\n\")\n",
    "        f.write(f\"- **Instance Types:** {', '.join(INSTANCE_TYPES)}\\n\")\n",
    "        f.write(f\"- **Dataset:** {DATASET_S3_PATH}\\n\")\n",
    "        f.write(f\"- **Epochs:** {BENCHMARK_EPOCHS}\\n\")\n",
    "        f.write(f\"- **Max Steps:** {BENCHMARK_MAX_STEPS}\\n\")\n",
    "        f.write(f\"- **Region:** {REGION}\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Results Summary\\n\\n\")\n",
    "        f.write(df.to_markdown(index=False, floatfmt=\".3f\"))\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Recommendations\\n\\n\")\n",
    "        \n",
    "        if 'overall_score' in df.columns:\n",
    "            best_overall = df.loc[df['overall_score'].idxmax()]\n",
    "            f.write(f\"### Best Overall Performance: {best_overall['instance_type']}\\n\")\n",
    "            f.write(f\"- Overall Score: {best_overall['overall_score']:.3f}\\n\")\n",
    "            if 'actual_cost' in best_overall:\n",
    "                f.write(f\"- Cost: ${best_overall['actual_cost']:.2f}\\n\")\n",
    "            if 'avg_tokens_per_second' in best_overall:\n",
    "                f.write(f\"- Speed: {best_overall['avg_tokens_per_second']:.0f} tokens/sec\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        if 'cost_effectiveness' in df.columns:\n",
    "            best_cost = df.loc[df['cost_effectiveness'].idxmax()]\n",
    "            f.write(f\"### Most Cost-Effective: {best_cost['instance_type']}\\n\")\n",
    "            f.write(f\"- Cost Effectiveness: {best_cost['cost_effectiveness']:.3f}\\n\")\n",
    "            if 'actual_cost' in best_cost:\n",
    "                f.write(f\"- Cost: ${best_cost['actual_cost']:.2f}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"## Usage Guidelines\\n\\n\")\n",
    "        f.write(\"- **Budget-conscious projects:** Use the most cost-effective instance\\n\")\n",
    "        f.write(\"- **Time-critical projects:** Use the fastest instance\\n\")\n",
    "        f.write(\"- **Balanced workloads:** Use the best overall performance instance\\n\")\n",
    "        f.write(\"- **Production training:** Consider GPU utilization and stability\\n\")\n",
    "    \n",
    "    print(f\"\\nComprehensive report saved to: {report_file}\")\n",
    "    \n",
    "    # Upload report to S3\n",
    "    try:\n",
    "        s3_key = f\"benchmark_reports/{report_file}\"\n",
    "        s3_client.upload_file(report_file, BUCKET_NAME, s3_key)\n",
    "        print(f\"Report uploaded to: s3://{BUCKET_NAME}/{s3_key}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload report to S3: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"No data available for report generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Based on the benchmark results, you can now:\n",
    "\n",
    "1. **Choose the optimal instance type** for your production training\n",
    "2. **Submit a full training job** using the recommended instance\n",
    "3. **Scale your training** based on the performance characteristics\n",
    "4. **Optimize costs** by selecting the most cost-effective option\n",
    "\n",
    "### Submit Production Training Job\n",
    "\n",
    "Use the cell below to submit a production training job with your chosen instance type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit production training job (uncomment and modify as needed)\n",
    "\n",
    "# # Choose your preferred instance type based on benchmark results\n",
    "# PRODUCTION_INSTANCE_TYPE = 'ml.p4de.24xlarge'  # Update based on your benchmark results\n",
    "# PRODUCTION_EPOCHS = 3\n",
    "\n",
    "# # Initialize job manager\n",
    "# job_manager = SageMakerJobManager(\n",
    "#     role_arn=ROLE_ARN,\n",
    "#     bucket_name=BUCKET_NAME,\n",
    "#     region=REGION\n",
    "# )\n",
    "\n",
    "# # Submit production training job\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# production_job_name = f\"qwen-hebrew-production-{timestamp}\"\n",
    "\n",
    "# job_name = job_manager.submit_training_job(\n",
    "#     job_name=production_job_name,\n",
    "#     instance_type=PRODUCTION_INSTANCE_TYPE,\n",
    "#     dataset_path=DATASET_S3_PATH,\n",
    "#     epochs=PRODUCTION_EPOCHS,\n",
    "#     wandb_project='qwen-hebrew-production',\n",
    "#     checkpoint_s3_uri=f's3://{BUCKET_NAME}/checkpoints/{production_job_name}/'\n",
    "# )\n",
    "\n",
    "# print(f\"Production training job submitted: {job_name}\")\n",
    "# print(f\"Instance type: {PRODUCTION_INSTANCE_TYPE}\")\n",
    "# print(f\"Epochs: {PRODUCTION_EPOCHS}\")\n",
    "# print(f\"Monitor progress in SageMaker console or W&B dashboard\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}