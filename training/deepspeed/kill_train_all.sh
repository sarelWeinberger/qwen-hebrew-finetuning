#!/bin/bash

# Script to safely kill all training processes
# This script will terminate all Python training processes and DeepSpeed processes

echo "ðŸ›‘ Killing all training processes..."

# Function to safely kill processes
kill_processes() {
    local pattern=$1
    local description=$2
    
    pids=$(pgrep -f "$pattern" 2>/dev/null || true)
    if [ -n "$pids" ]; then
        echo "ðŸ“ Found $description processes: $pids"
        echo "   Sending SIGTERM..."
        kill -TERM $pids 2>/dev/null || true
        sleep 3
        
        # Check if processes are still running
        remaining_pids=$(pgrep -f "$pattern" 2>/dev/null || true)
        if [ -n "$remaining_pids" ]; then
            echo "   Some processes still running, sending SIGKILL..."
            kill -KILL $remaining_pids 2>/dev/null || true
            sleep 1
        fi
        echo "   âœ… $description processes terminated"
    else
        echo "   âœ… No $description processes found"
    fi
}

# Kill training processes in order of preference
echo ""
echo "ðŸ” Searching for training processes..."

# 1. Kill train.py processes
kill_processes "train\.py" "train.py"

# 2. Kill train_debug.py processes
kill_processes "train_debug\.py" "train_debug.py"

# 3. Kill accelerate launch processes
kill_processes "accelerate.*launch" "accelerate launch"

# 4. Kill any Python processes with DeepSpeed
kill_processes "python.*deepspeed" "Python DeepSpeed"

# 5. Kill any remaining DeepSpeed processes
kill_processes "deepspeed" "DeepSpeed"

# 6. Kill any Python processes with "qwen" in the command line
kill_processes "python.*qwen" "Qwen-related Python"

# 7. Kill any Python processes with "transformers" and "train" in command line
kill_processes "python.*transformers.*train" "Transformers training"

echo ""
echo "ðŸ§¹ Cleaning up GPU memory..."

# Clear GPU memory if nvidia-smi is available
if command -v nvidia-smi &> /dev/null; then
    echo "ðŸ“Š GPU memory before cleanup:"
    nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv,noheader,nounits
    
    # Reset GPU if nvidia-ml-py is available
    python3 -c "
try:
    import torch
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print('   âœ… PyTorch GPU cache cleared')
    else:
        print('   âš ï¸  CUDA not available in PyTorch')
except ImportError:
    print('   âš ï¸  PyTorch not available')
    
try:
    import gc
    gc.collect()
    print('   âœ… Python garbage collection completed')
except:
    print('   âš ï¸  Garbage collection failed')
" 2>/dev/null || echo "   âš ï¸  Could not run Python cleanup"

    echo ""
    echo "ðŸ“Š GPU memory after cleanup:"
    nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv,noheader,nounits
else
    echo "   âš ï¸  nvidia-smi not found, skipping GPU cleanup"
fi

echo ""
echo "ðŸ” Checking for remaining training processes..."
remaining=$(pgrep -f "train\.py|accelerate.*launch|deepspeed|python.*qwen" 2>/dev/null || true)
if [ -n "$remaining" ]; then
    echo "   âš ï¸  Warning: Some processes may still be running:"
    ps aux | grep -E "train\.py|accelerate.*launch|deepspeed|python.*qwen" | grep -v grep | head -5
    echo "   ðŸ’¡ You may need to kill them manually or restart the system"
else
    echo "   âœ… No remaining training processes found"
fi

echo ""
echo "ðŸŽ¯ Process cleanup summary:"
echo "   â€¢ Terminated all train.py processes"
echo "   â€¢ Terminated all accelerate launch processes"  
echo "   â€¢ Terminated all DeepSpeed processes"
echo "   â€¢ Cleared GPU memory cache"
echo "   â€¢ Performed garbage collection"

echo ""
echo "âœ… Training process cleanup completed!"
echo "ðŸ’¡ It's safe to start new training runs now."

# Show current GPU usage
if command -v nvidia-smi &> /dev/null; then
    echo ""
    echo "ðŸ“Š Current GPU status:"
    nvidia-smi --query-gpu=index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader
fi