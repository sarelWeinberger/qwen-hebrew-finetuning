#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import xml.etree.ElementTree as ET
import mwparserfromhell
import json
import re
import bz2
import csv
import ipaddress
import boto3
import shutil
from pathlib import Path
from tqdm import tqdm
import time
import os
from collections import defaultdict


class WikipediaProcessorRound2:
    def __init__(self, dump_path, s3_bucket, s3_prefix_main, s3_prefix_examples, max_articles=10000,
                 max_examples_per_category=100):
        self.dump_path = dump_path
        self.s3_bucket = s3_bucket
        self.s3_prefix_main = s3_prefix_main
        self.s3_prefix_examples = s3_prefix_examples
        self.max_articles = max_articles
        self.max_examples_per_category = max_examples_per_category

        # S3 client
        self.s3_client = boto3.client('s3')

        # ×ª×™×§×™×•×ª ×“×•×’×××•×ª (×§×™×™××•×ª ×‘-S3)
        self.example_categories = [
            "cr_and_3_newlines", "CSS", "empty_line_with_bullet", "equations",
            "html_escape_codes", "multiple_hyphens", "multiple_spaces", "PII",
            "start_end_white_space", "tables", "URL", "wiki_citations",
            "wiki_foreign_language_and_image_refs", "wiki_headers", "wiki_markup",
            "wiki_redirecting_articles", "wiki_tags_and_media_descriptions"
        ]

        # ××•× ×™× ×œ×“×•×’×××•×ª
        self.example_counts = defaultdict(int)

        # ××•× ×™× ×›×œ×œ×™×™×
        self.total_processed = 0
        self.total_scanned = 0

        # ×ª×™×§×™×” ××§×•××™×ª ×–×× ×™×ª ×•×¤×ª×™×—×ª ×§×•×‘×¥ ×”×¤×œ×˜ ×”×¨××©×™
        self.temp_dir = Path("temp_output")
        self.temp_dir.mkdir(exist_ok=True)
        self.output_file = open(self.temp_dir / "wikipedia_he_round2.jsonl", 'w', encoding='utf-8')

    def upload_example_file_to_s3(self, category, temp_file):
        """×”×¢×œ××ª ×§×•×‘×¥ ×“×•×’×××•×ª ×œ-S3"""
        try:
            s3_key = f"{self.s3_prefix_examples}{category}/{category}_examples.csv"

            self.s3_client.upload_file(
                str(temp_file),
                self.s3_bucket,
                s3_key
            )

            print(f"âœ… ×”×•×¢×œ×• ×“×•×’×××•×ª {category}: {self.example_counts[category]} ×“×•×’×××•×ª")
        except Exception as e:
            print(f"âŒ ×©×’×™××” ×‘×”×¢×œ××ª ×“×•×’×××•×ª {category}: {e}")

    def upload_main_output_to_s3(self):
        """×”×¢×œ××ª ×”×§×•×‘×¥ ×”×¨××©×™ ×œ-S3"""
        try:
            local_file = self.temp_dir / "wikipedia_he_round2.jsonl"
            s3_key = f"{self.s3_prefix_main}wikipedia_he_round2.jsonl"

            self.s3_client.upload_file(
                str(local_file),
                self.s3_bucket,
                s3_key
            )

            print(f"âœ… ×”×•×¢×œ×” ×§×•×‘×¥ ×¨××©×™: s3://{self.s3_bucket}/{s3_key}")
        except Exception as e:
            print(f"âŒ ×©×’×™××” ×‘×”×¢×œ××ª ×§×•×‘×¥ ×¨××©×™: {e}")

    def cleanup_temp_files(self):
        """× ×™×§×•×™ ×§×‘×¦×™× ×–×× ×™×™×"""
        try:
            shutil.rmtree(self.temp_dir)
            print("ğŸ—‘ï¸ × ×•×§×• ×§×‘×¦×™× ×–×× ×™×™×")
        except Exception as e:
            print(f"âš ï¸ ×©×’×™××” ×‘× ×™×§×•×™ ×§×‘×¦×™× ×–×× ×™×™×: {e}")

    def save_example(self, category, raw_text, clean_text):
        """×©××™×¨×ª ×“×•×’××” ×œ×§×˜×’×•×¨×™×” ××¡×•×™××ª ×‘-S3"""
        if self.example_counts[category] >= self.max_examples_per_category:
            return

        # ×™×¦×™×¨×ª ×§×•×‘×¥ CSV ×–×× ×™
        temp_file = self.temp_dir / f"{category}_temp.csv"

        # ×‘×“×™×§×” ×× ×”×§×•×‘×¥ ×§×™×™× ×•×™×¦×™×¨×ª headers
        file_exists = temp_file.exists()

        with open(temp_file, 'a', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)

            if not file_exists:
                writer.writerow(['raw_text', 'clean_text'])

            writer.writerow([raw_text, clean_text])

        self.example_counts[category] += 1

        # ×”×¢×œ××” ×œ-S3 ×›×œ 10 ×“×•×’×××•×ª ××• ×‘×¡×•×£
        if self.example_counts[category] % 10 == 0 or self.example_counts[category] == self.max_examples_per_category:
            self.upload_example_file_to_s3(category, temp_file)

    def clean_html_escape_codes(self, text):
        """×›×œ×œ 1: ×”×—×œ×¤×ª HTML escape codes"""
        original_text = text

        # ×”×—×œ×¤×ª escape codes ×©×•× ×™×
        text = text.replace('&quot;', '"')
        text = text.replace('&#34;', '"')
        text = text.replace('&#39;', "'")

        # ×©××™×¨×ª ×“×•×’××” ×× ×”×©×ª× ×” ××©×”×•
        if text != original_text:
            self.save_example("html_escape_codes", original_text, text)

        return text

    def clean_newlines_and_spaces(self, text):
        """×›×œ×œ 2: ×˜×™×¤×•×œ ×‘×©×•×¨×•×ª ×—×“×©×•×ª ×•×¨×•×•×—×™×"""
        original_text = text

        # ×”×¡×¨×ª carriage return
        text = text.replace('\r', '')

        # ×”×—×œ×¤×ª ×™×•×ª×¨ ×-3 ×©×•×¨×•×ª ×—×“×©×•×ª ×¨×¦×•×¤×•×ª ×‘××§×¡×™××•× 3
        text = re.sub(r'\n{4,}', '\n\n\n', text)

        # ×©××™×¨×ª ×“×•×’××” ×× ×”×©×ª× ×” ××©×”×•
        if text != original_text:
            self.save_example("cr_and_3_newlines", original_text, text)

        return text

    def clean_multiple_spaces(self, text):
        """×›×œ×œ 3: ×˜×™×¤×•×œ ×‘×¨×•×•×—×™× ××¨×•×‘×™×"""
        original_text = text

        def replace_spaces(match):
            space_count = len(match.group(0))

            # ×× ××ª×—×œ×§ ×‘-4, ×œ×”×©××™×¨ ×¢×“ ××§×¡×™××•× 16
            if space_count % 4 == 0:
                return ' ' * min(space_count, 16)
            else:
                # ×× ×œ× ××ª×—×œ×§ ×‘-4, ×œ×¦××¦× ×œ×¨×•×•×— ××—×“
                return ' '

        # ××¦×™××ª ×¨×¦×¤×™× ×©×œ 2+ ×¨×•×•×—×™×
        text = re.sub(r' {2,}', replace_spaces, text)

        # ×©××™×¨×ª ×“×•×’××” ×× ×”×©×ª× ×” ××©×”×•
        if text != original_text:
            self.save_example("multiple_spaces", original_text, text)

        return text

    def clean_whitespace_start_end(self, text):
        """×›×œ×œ 4: ×”×¡×¨×ª ×¨×•×•×—×™× ××ª×—×™×œ×ª ×•×¡×•×£ ×”×˜×§×¡×˜"""
        original_text = text
        text = text.strip()

        # ×©××™×¨×ª ×“×•×’××” ×× ×”×©×ª× ×” ××©×”×•
        if text != original_text:
            self.save_example("start_end_white_space", original_text, text)

        return text

    def is_localhost_ip(self, ip_str):
        """×‘×“×™×§×” ×× IP ×”×•× localhost (127.0.0.0/8)"""
        try:
            ip = ipaddress.ip_address(ip_str)
            localhost_network = ipaddress.ip_network('127.0.0.0/8')
            return ip in localhost_network
        except:
            return False

    def clean_pii(self, text):
        """×›×œ×œ 6: ××—×™×§×ª PII - IP ×•××™×™×œ (×©××™×¨×ª localhost)"""
        original_text = text

        # ××—×™×§×ª ×›×ª×•×‘×•×ª ××™×™×œ
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        text = re.sub(email_pattern, '[EMAIL_REMOVED]', text)

        # ××—×™×§×ª IP addresses (×—×•×¥ ×-localhost)
        ip_pattern = r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b'

        def replace_ip(match):
            ip = match.group(0)
            if self.is_localhost_ip(ip):
                return ip  # ×©××™×¨×ª localhost
            else:
                return '[IP_REMOVED]'

        text = re.sub(ip_pattern, replace_ip, text)

        # ×©××™×¨×ª ×“×•×’××” ×× ×”×©×ª× ×” ××©×”×•
        if text != original_text:
            self.save_example("PII", original_text, text)

        return text

    def clean_empty_bullet_lines(self, text):
        """×›×œ×œ 7: ×”×¡×¨×ª ×©×•×¨×•×ª ×¨×™×§×•×ª ×¢× bullet points"""
        original_text = text

        # ×”×¡×¨×ª ×©×•×¨×•×ª ×©××›×™×œ×•×ª ×¨×§ bullet points ×•×¨×•×•×—×™×
        bullet_pattern = r'^\s*[â€¢â—â– â—¦â–ªâ—†]+\s*$'
        lines = text.split('\n')
        cleaned_lines = []

        for line in lines:
            if not re.match(bullet_pattern, line):
                cleaned_lines.append(line)

        text = '\n'.join(cleaned_lines)

        # ×©××™×¨×ª ×“×•×’××” ×× ×”×©×ª× ×” ××©×”×•
        if text != original_text:
            self.save_example("empty_line_with_bullet", original_text, text)

        return text

    def clean_separator_lines(self, text):
        """×›×œ×œ 8: ×”×¡×¨×ª ×§×•×•×™ ×”×¤×¨×“×” ××¨×•×›×™×"""
        original_text = text

        # ×”×¡×¨×ª ×©×•×¨×•×ª ×¢× ×”×¤×¨×“×•×ª ××¨×•×›×•×ª - ×¨×§ ×ª×•×•×™ ×”×¤×¨×“×” ×‘××•×¨×š 4+
        separator_patterns = [
            r'^[-]{4,}$',  # ××§×¤×™× ×‘×œ×‘×“
            r'^[=]{4,}$',  # ×¡×™×× ×™ ×©×•×•×” ×‘×œ×‘×“
            r'^[_]{4,}$',  # ×§×• ×ª×—×ª×•×Ÿ ×‘×œ×‘×“
            r'^[~]{4,}$',  # ×˜×™×œ×“×” ×‘×œ×‘×“
            r'^[*]{4,}$',  # ×›×•×›×‘×™×•×ª ×‘×œ×‘×“
            r'^[#]{4,}$',  # ×¤××•× ×“ ×‘×œ×‘×“
        ]

        lines = text.split('\n')
        cleaned_lines = []

        for line in lines:
            line_stripped = line.strip()
            should_remove = False

            # ×‘×“×™×§×” ×× ×”×©×•×¨×” ××›×™×œ×” ×¨×§ ×ª×•×•×™ ×”×¤×¨×“×” (4 ××• ×™×•×ª×¨)
            for pattern in separator_patterns:
                if re.match(pattern, line_stripped):
                    should_remove = True
                    break

            # ×¨×§ ×× ×”×©×•×¨×” ×œ× ×¦×¨×™×›×” ×œ×”×™×•×ª ××•×¡×¨×ª, × ×›×œ×•×œ ××•×ª×”
            if not should_remove:
                cleaned_lines.append(line)

        text = '\n'.join(cleaned_lines)

        # ×©××™×¨×ª ×“×•×’××” ×× ×”×©×ª× ×” ××©×”×•
        if text != original_text:
            self.save_example("multiple_hyphens", original_text, text)

        return text

    def clean_css_from_tables(self, text):
        """×›×œ×œ 9: ×”×¡×¨×ª CSS ××˜×‘×œ××•×ª"""
        original_text = text

        # ×”×¡×¨×ª style attributes
        text = re.sub(r'style="[^"]*"', '', text)
        text = re.sub(r'cellspacing="[^"]*"', '', text)
        text = re.sub(r'cellpadding="[^"]*"', '', text)
        text = re.sub(r'class="[^"]*"', '', text)
        text = re.sub(r'width="[^"]*"', '', text)
        text = re.sub(r'height="[^"]*"', '', text)

        # ×©××™×¨×ª ×“×•×’××” ×× ×”×©×ª× ×” ××©×”×•
        if text != original_text:
            self.save_example("CSS", original_text, text)

        return text

    def convert_tables_to_markdown(self, text):
        """×›×œ×œ 10: ×”××¨×ª ×˜×‘×œ××•×ª ×•×™×§×™ ×œ××¨×§×“××•×Ÿ"""
        original_text = text

        def process_wiki_table(match):
            table_content = match.group(0)
            table_inner = table_content[2:-2]  # ×”×¡×¨ {| ×• |}

            rows = re.split(r'\|-', table_inner)
            markdown_rows = []

            for i, row in enumerate(rows):
                row = row.strip()
                if not row:
                    continue

                # ×¤×™×¦×•×œ ×ª××™×
                cells = re.split(r'\|\||\|', row)
                clean_cells = []

                for cell in cells:
                    cell = cell.strip()
                    if cell and not re.match(r'^\d+px$', cell):
                        # × ×™×§×•×™ ×ª×
                        cell = re.sub(r'^[\|\s]+', '', cell)
                        cell = re.sub(r'[\|\s]+$', '', cell)
                        if cell and len(cell) > 1:
                            clean_cells.append(cell)

                if clean_cells:
                    # ×™×¦×™×¨×ª ×©×•×¨×ª ××¨×§×“××•×Ÿ
                    markdown_row = "| " + " | ".join(clean_cells) + " |"
                    markdown_rows.append(markdown_row)

                    # ×”×•×¡×¤×ª ×©×•×¨×ª ×”×¤×¨×“×” ××—×¨×™ ×”×©×•×¨×” ×”×¨××©×•× ×”
                    if i == 0 and len(clean_cells) > 0:
                        separator = "|" + "---|" * len(clean_cells)
                        markdown_rows.append(separator)

            return "\n".join(markdown_rows) if markdown_rows else ""

        # ×”××¨×ª ×˜×‘×œ××•×ª ×•×™×§×™
        table_pattern = r'\{\|.*?\|\}'
        text = re.sub(table_pattern, process_wiki_table, text, flags=re.DOTALL)

        # ×©××™×¨×ª ×“×•×’××” ×× ×”×©×ª× ×” ××©×”×•
        if text != original_text:
            self.save_example("tables", original_text, text)

        return text

    def clean_wiki_templates_and_markup(self, text):
        """×›×œ×œ 11a,b: ×”×¡×¨×ª ×ª×‘× ×™×•×ª ×•-markup ×©×œ ×•×™×§×™"""
        original_text = text

        # ×”×¡×¨×ª ×ª×‘× ×™×•×ª ××•×¨×›×‘×•×ª ×¢× ×ª×‘× ×™×•×ª ××§×•× × ×•×ª {{}}
        # ×¢×•×©×” ×–××ª ××¡×¤×¨ ×¤×¢××™× ×›×“×™ ×œ×”×ª××•×“×“ ×¢× ×§×™× ×•×Ÿ ×¢××•×§
        for _ in range(5):  # ××§×¡×™××•× 5 ××™×˜×¨×¦×™×•×ª
            old_text = text
            text = re.sub(r'\{\{[^{}]*\}\}', '', text)
            if text == old_text:  # ×× ×œ× ×”×©×ª× ×”, ××¤×¡×™×§
                break

        # ×”×¡×¨×ª ×§×™×©×•×¨×™ ×§×‘×¦×™×/×ª××•× ×•×ª ×œ×¤× ×™ ×§×™×©×•×¨×™× ×¨×’×™×œ×™×
        media_link_patterns = [
            r'\[\[×§×•×‘×¥:.*?\]\]',
            r'\[\[File:.*?\]\]',
            r'\[\[Image:.*?\]\]',
            r'\[\[×ª××•× ×”:.*?\]\]'
        ]

        for pattern in media_link_patterns:
            text = re.sub(pattern, '', text, flags=re.DOTALL | re.IGNORECASE)

        # ×”×¡×¨×ª ×§×™×©×•×¨×™× ×¤× ×™××™×™× [[]] ×¢× ×ª×™××•×¨×™×
        # ××˜×¤×œ ×‘×§×™×©×•×¨×™× ××”×¦×•×¨×” [[×§×™×©×•×¨|×ª×™××•×¨]] â†’ ×ª×™××•×¨
        text = re.sub(r'\[\[([^|\]]+)\|([^\]]+)\]\]', r'\2', text)

        # ×”×¡×¨×ª ×§×™×©×•×¨×™× ×¤×©×•×˜×™× [[×§×™×©×•×¨]] â†’ ×§×™×©×•×¨
        text = re.sub(r'\[\[([^\]]+)\]\]', r'\1', text)

        # ×”×¡×¨×ª ×ª×’×™×•×ª HTML (×›×•×œ×œ ×ª×’×™×•×ª ×¢× ×ª×›×•× ×•×ª)
        text = re.sub(r'<[^>]*>', '', text)

        # × ×™×§×•×™ ×¨×•×•×—×™× ××™×•×ª×¨×™× ×•×¤×¡×™×§×™× ×©× ×•×ª×¨×•
        # ×”×¡×¨×ª ×¤×¡×™×§×™× ××™×•×ª×¨×™× ×©× ×•×ª×¨×• ××—×¨×™ ×”×¡×¨×ª ×§×™×©×•×¨×™×
        text = re.sub(r',\s*,', ',', text)  # ×¤×¡×™×§×™× ×›×¤×•×œ×™×
        text = re.sub(r',\s*\]', ']', text)  # ×¤×¡×™×§ ×œ×¤× ×™ ×¡×’×™×¨×ª ×¡×•×’×¨×™×™×
        text = re.sub(r'\[\s*,', '[', text)  # ×¤×¡×™×§ ××—×¨×™ ×¤×ª×™×—×ª ×¡×•×’×¨×™×™×

        # ×”×¡×¨×ª ×¨×•×•×—×™× ××¨×•×‘×™× (×¨×§ ×¨×•×•×—×™× ×•×˜××‘×™×, ×œ× ×©×•×¨×•×ª ×—×“×©×•×ª)
        text = re.sub(r'[ \t]+', ' ', text)

        # ×”×¡×¨×ª ×¤×¡×™×§×™× ×‘×ª×—×™×œ×ª ××• ×¡×•×£ ××©×¤×˜×™×
        text = re.sub(r'^\s*,\s*', '', text, flags=re.MULTILINE)
        text = re.sub(r'\s*,\s*$', '', text, flags=re.MULTILINE)

        # ×©××™×¨×ª ×“×•×’××” ×× ×”×©×ª× ×” ××©×”×•
        if text != original_text:
            self.save_example("wiki_markup", original_text, text)

        return text

    def clean_wiki_media_descriptions(self, text):
        """×›×œ×œ 11b: ×”×¡×¨×ª ×ª×™××•×¨×™ ××“×™×” ×•×ª××•× ×•×ª"""
        original_text = text

        # ×”×¡×¨×ª ×ª×™××•×¨×™ ××™×§×•× ×ª××•× ×•×ª - ×‘××•×¤×Ÿ ×¡×¤×¦×™×¤×™
        location_keywords = ['×©×××œ', '×™××™×Ÿ', '××¨×›×–', '×××•×–×¢×¨', 'thumb', 'thumbnail', 'frame', 'framed', 'left', 'right',
                             'center']

        for keyword in location_keywords:
            # ×”×¡×¨×” ×¡×¤×¦×™×¤×™×ª ×©×œ ×ª×™××•×¨×™ ××™×§×•× (×›××• ×××•×–×¢×¨|300px|)
            pattern = rf'\b{keyword}\|[^|]*?\|'
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)

        # ×”×¡×¨×ª ×”×¤× ×™×•×ª "×¨××•" - ×ª××™×›×” ×‘×¢×‘×¨×™×ª ×•×× ×’×œ×™×ª
        see_patterns = [
            r'×¨××• [A-Za-z\s,×-×ª\.]+',
            r'×¨××” [A-Za-z\s,×-×ª\.]+',
            r'see [A-Za-z\s,\.]+',
            r'See [A-Za-z\s,\.]+',
            r'×¨××• ×’× [A-Za-z\s,×-×ª\.]+',
            r'×¨××” ×’× [A-Za-z\s,×-×ª\.]+',
            r'see also [A-Za-z\s,\.]+',
            r'See also [A-Za-z\s,\.]+',
        ]

        for pattern in see_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)

        # ×”×¡×¨×ª ×ª×™××•×¨×™× ×‘×¡×•×’×¨×™×™× ×©××›×™×œ×™× ××™×“×¢ ×¢×œ ××“×™×”
        text = re.sub(r'\([^)]*(?:px|×××•×–×¢×¨|thumb|frame)[^)]*\)', '', text, flags=re.IGNORECASE)

        # ×”×¡×¨×ª ××™×“×¢ ×¢×œ ×’×•×“×œ ×ª××•× ×•×ª
        text = re.sub(r'\b\d+px\b', '', text)

        # × ×™×§×•×™ ×©×•×¨×•×ª ×¨×™×§×•×ª ×¢×•×“×¤×•×ª ×©× ×•×¦×¨×•
        text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)

        # × ×™×§×•×™ ×¨×•×•×—×™× ××™×•×ª×¨×™× (×¨×§ ×¨×•×•×—×™× ×•×˜××‘×™×, ×œ× ×©×•×¨×•×ª ×—×“×©×•×ª)
        text = re.sub(r'[ \t]+', ' ', text)
        text = re.sub(r'^[ \t]+', '', text, flags=re.MULTILINE)
        text = re.sub(r'[ \t]+$', '', text, flags=re.MULTILINE)

        # ×©××™×¨×ª ×“×•×’××” ×× ×”×©×ª× ×” ××©×”×•
        if text != original_text:
            self.save_example("wiki_tags_and_media_descriptions", original_text, text)

        return text

    def clean_wiki_headers(self, text):
        """×›×œ×œ 11d: ×”××¨×ª ×›×•×ª×¨×•×ª ×•×™×§×™ ×œ×¤×•×¨××˜ ××¨×§×“××•×Ÿ"""
        original_text = text

        # ×”××¨×ª ×›×•×ª×¨×•×ª ×•×™×§×™ (== ×›×•×ª×¨×ª ==) ×œ××¨×§×“××•×Ÿ (## ×›×•×ª×¨×ª)
        def convert_header(match):
            # ××¡×¤×¨ ×”-= ×§×•×‘×¢ ××ª ×¨××ª ×”×›×•×ª×¨×ª
            equals_prefix = match.group(1)
            header_text = match.group(2).strip()
            equals_suffix = match.group(3)

            # ×¡×¤×™×¨×ª ××¡×¤×¨ ×”-= (×œ×•×§×— ××ª ×”××™× ×™××•× ×‘×™×Ÿ ×”×ª×—×œ×” ×•×¡×•×£)
            level = min(len(equals_prefix), len(equals_suffix))

            # ×”××¨×” ×œ-markdown headers (××§×¡×™××•× 6 ×¨××•×ª)
            markdown_level = min(level, 6)
            return '#' * markdown_level + ' ' + header_text

        # ×“×¤×•×¡ ×¨×’×•×œ×¨×™ ××©×•×¤×¨ ×œ×–×™×”×•×™ ×›×•×ª×¨×•×ª ×•×™×§×™
        # ××—×¤×©: (=+) + ×¨×•×•×—×™× ××•×¤×¦×™×•× ×œ×™× + ×ª×•×›×Ÿ + ×¨×•×•×—×™× ××•×¤×¦×™×•× ×œ×™× + (=+)
        header_pattern = r'^(={2,6})\s*([^=\r\n]+?)\s*(={2,6})\s*$'

        text = re.sub(header_pattern, convert_header, text, flags=re.MULTILINE)

        # ×©××™×¨×ª ×“×•×’××” ×× ×”×©×ª× ×” ××©×”×•
        if text != original_text:
            self.save_example("wiki_headers", original_text, text)

        return text

    def clean_wiki_citations(self, text):
        """×›×œ×œ 11e: ×”×¡×¨×ª citations"""
        original_text = text

        # ×”×¡×¨×ª citations ×©×•× ×™×
        citation_patterns = [
            r'<ref[^>]*>.*?</ref>',  # ref tags
            r'<ref[^>]*/>',  # self-closing ref tags
            r'\{\{cite[^}]*\}\}',  # cite templates
            r'\{\{×¦-[^}]*\}\}',  # Hebrew citations
            r'\{\{×”×¢×¨×”[^}]*\}\}',  # Hebrew notes
            r'\{\{××§×•×¨[^}]*\}\}',  # Hebrew sources
        ]

        for pattern in citation_patterns:
            text = re.sub(pattern, '', text, flags=re.DOTALL | re.IGNORECASE)

        # ×©××™×¨×ª ×“×•×’××” ×× ×”×©×ª× ×” ××©×”×•
        if text != original_text:
            self.save_example("wiki_citations", original_text, text)

        return text

    def check_redirecting_article(self, raw_wikitext):
        """×›×œ×œ 11f: ×‘×“×™×§×” ×× ×”××××¨ ×”×•× ×”×¤× ×™×”"""
        is_redirect = raw_wikitext.strip().startswith('#REDIRECT') or raw_wikitext.strip().startswith('#×”×¤× ×™×”')

        if is_redirect:
            self.save_example("wiki_redirecting_articles", raw_wikitext, "[REDIRECTING ARTICLE - EXCLUDED]")

        return is_redirect

    def apply_all_cleaning_rules(self, title, raw_wikitext):
        """×”×¤×¢×œ×ª ×›×œ ×›×œ×œ×™ ×”× ×™×§×™×•×Ÿ ×¢×œ ×˜×§×¡×˜"""
        # ×‘×“×™×§×” ×× ×–×” ××××¨ ××¤× ×” (×›×œ×œ 11f)
        if self.check_redirecting_article(raw_wikitext):
            return None

        # ×”××¨×” ×œ××•×‘×™×™×§×˜ wikicode
        try:
            wikicode = mwparserfromhell.parse(raw_wikitext)
            text = str(wikicode)
        except:
            text = raw_wikitext

        # ×”×¤×¢×œ×ª ×›×œ ×”×›×œ×œ×™×
        text = self.clean_html_escape_codes(text)  # ×›×œ×œ 1
        text = self.clean_newlines_and_spaces(text)  # ×›×œ×œ 2
        text = self.clean_multiple_spaces(text)  # ×›×œ×œ 3
        text = self.clean_whitespace_start_end(text)  # ×›×œ×œ 4
        text = self.clean_pii(text)  # ×›×œ×œ 6
        text = self.clean_empty_bullet_lines(text)  # ×›×œ×œ 7
        text = self.clean_separator_lines(text)  # ×›×œ×œ 8
        text = self.clean_css_from_tables(text)  # ×›×œ×œ 9
        text = self.convert_tables_to_markdown(text)  # ×›×œ×œ 10
        text = self.clean_wiki_templates_and_markup(text)  # ×›×œ×œ 11a,b
        text = self.clean_wiki_media_descriptions(text)  # ×›×œ×œ 11b
        text = self.clean_wiki_headers(text)  # ×›×œ×œ 11d
        text = self.clean_wiki_citations(text)  # ×›×œ×œ 11e

        # ×‘×“×™×§×” ×¡×•×¤×™×ª ×©×œ ××™×›×•×ª
        if not text or len(text) < 100:
            return None

        return text

    def count_words(self, text):
        """×¡×¤×™×¨×ª ××™×œ×™×"""
        if not text:
            return 0
        return len(text.split())

    def count_bytes(self, text):
        """×¡×¤×™×¨×ª ×‘×™×™×˜×™× ×‘-UTF-8"""
        if not text:
            return 0
        return len(text.encode('utf-8'))

    def is_valid_article(self, page_elem):
        """×‘×“×™×§×” ×× ×”×“×£ ×ª×§×™×Ÿ ×œ×¢×™×‘×•×“"""
        namespace_elem = page_elem.find('.//{*}ns')
        if namespace_elem is None or namespace_elem.text != '0':
            return False

        revision = page_elem.find('.//{*}revision')
        if revision is None:
            return False

        text_elem = revision.find('.//{*}text')
        if text_elem is None or not text_elem.text:
            return False

        return True

    def process_dump(self):
        """×¢×™×‘×•×“ ×”×“×××¤ ×¢× ×”×’×‘×œ×”"""
        print("ğŸš€ ××ª×—×™×œ ×¢×™×‘×•×“ ×•×™×§×™×¤×“×™×” - ×¡×™×‘×•×‘ 2")
        print(f"ğŸ“‚ ×§×•×‘×¥ ×§×œ×˜: {self.dump_path}")
        print(f"â˜ï¸ S3 ×¨××©×™: s3://{self.s3_bucket}/{self.s3_prefix_main}")
        print(f"ğŸ“ S3 ×“×•×’×××•×ª: s3://{self.s3_bucket}/{self.s3_prefix_examples}")
        print(f"ğŸ”¢ ××’×‘×œ×”: {self.max_articles} ×¢×¨×›×™×")
        print(f"ğŸ“Š ×“×•×’×××•×ª: ×¢×“ {self.max_examples_per_category} ×œ×›×œ ×§×˜×’×•×¨×™×”")
        print("=" * 60)

        start_time = time.time()

        try:
            with bz2.open(self.dump_path, 'rt', encoding='utf-8') as dump_file:
                with tqdm(total=self.max_articles, desc="ğŸ”„ ×¢×™×‘×•×“ ×¢×¨×›×™×", unit="articles") as pbar:

                    for event, elem in ET.iterparse(dump_file, events=('start', 'end')):
                        if event == 'end' and elem.tag.endswith('page'):
                            self.total_scanned += 1

                            # ×‘×“×™×§×” ×× ×”×“×£ ×ª×§×™×Ÿ
                            if self.is_valid_article(elem):
                                # ×—×™×œ×•×¥ ××™×“×¢
                                title_elem = elem.find('.//{*}title')
                                revision = elem.find('.//{*}revision')
                                text_elem = revision.find('.//{*}text')

                                title = title_elem.text if title_elem is not None else ""
                                raw_wikitext = text_elem.text if text_elem is not None else ""

                                # ×¢×™×‘×•×“ ×”×¢×¨×š
                                cleaned_text = self.apply_all_cleaning_rules(title, raw_wikitext)

                                if cleaned_text:
                                    # ×™×¦×™×¨×ª ×¤×¨×™×˜ JSONL
                                    article_item = {
                                        "text": cleaned_text,
                                        "word_count": self.count_words(cleaned_text),
                                        "byte_count": self.count_bytes(cleaned_text)
                                    }

                                    # ×›×ª×™×‘×” ×œ×§×•×‘×¥ ×”×¤×œ×˜
                                    json.dump(article_item, self.output_file, ensure_ascii=False)
                                    self.output_file.write('\n')

                                    self.total_processed += 1
                                    pbar.update(1)
                                    pbar.set_description(f"ğŸ”„ ×¢×™×‘×•×“: {title[:25]}...")

                                    # ×‘×“×™×§×” ×× ×”×’×¢× ×• ×œ××’×‘×œ×”
                                    if self.total_processed >= self.max_articles:
                                        break

                            elem.clear()

                        # ×‘×“×™×§×” ×× ×”×’×¢× ×• ×œ××’×‘×œ×”
                        if self.total_processed >= self.max_articles:
                            break

        except KeyboardInterrupt:
            print("\nâš ï¸ ×”×¢×™×‘×•×“ ×”×•×¤×¡×§ ×¢×œ ×™×“×™ ×”××©×ª××©")
        except Exception as e:
            print(f"\nâŒ ×©×’×™××” ×‘×¢×™×‘×•×“: {e}")

        finally:
            self.output_file.close()

            # ×”×¢×œ××” ×œ-S3 ×©×œ ×”×§×•×‘×¥ ×”×¨××©×™
            self.upload_main_output_to_s3()

            # ×”×¢×œ××” ×¡×•×¤×™×ª ×©×œ ×›×œ ×§×‘×¦×™ ×”×“×•×’×××•×ª ×©× ×•×ª×¨×•
            for category in self.example_categories:
                temp_file = self.temp_dir / f"{category}_temp.csv"
                if temp_file.exists() and self.example_counts[category] > 0:
                    self.upload_example_file_to_s3(category, temp_file)

            # × ×™×§×•×™ ×§×‘×¦×™× ×–×× ×™×™×
            self.cleanup_temp_files()

        # ×¡×™×›×•× ×¡×•×¤×™
        self.print_final_summary(start_time)

    def print_final_summary(self, start_time):
        """×”×“×¤×¡×ª ×¡×™×›×•× ×¡×•×¤×™"""
        total_time = time.time() - start_time

        print("\n" + "=" * 60)
        print("ğŸ‰ ×¢×™×‘×•×“ ×”×•×©×œ×!")
        print(f"ğŸ“Š ×¡×˜×˜×™×¡×˜×™×§×•×ª ×¡×•×¤×™×•×ª:")
        print(f"   â±ï¸ ×–××Ÿ ×›×•×œ×œ: {total_time / 60:.1f} ×“×§×•×ª")
        print(f"   ğŸ“– ×“×¤×™× ×©× ×¡×¨×§×•: {self.total_scanned:,}")
        print(f"   âœ… ×¢×¨×›×™× ×©×¢×•×‘×“×•: {self.total_processed:,}")
        print(f"   ğŸ“ˆ ×©×™×¢×•×¨ ×”×¦×œ×—×”: {(self.total_processed / self.total_scanned) * 100:.2f}%")

        print(f"\nğŸ“Š ×“×•×’×××•×ª ×©× ×©××¨×•:")
        for category, count in self.example_counts.items():
            if count > 0:
                print(f"   ğŸ“ {category}: {count} ×“×•×’×××•×ª")

        print(f"\nâ˜ï¸ ×”×¤×œ×˜ ×–××™×Ÿ ×‘-S3:")
        print(f"   ğŸ“„ ×§×•×‘×¥ ×¨××©×™: s3://{self.s3_bucket}/{self.s3_prefix_main}")
        print(f"   ğŸ“ ×“×•×’×××•×ª: s3://{self.s3_bucket}/{self.s3_prefix_examples}")


def main():
    """×”×¤×¢×œ×” ×¨××©×™×ª"""
    print("ğŸ¯ ×¢×™×‘×•×“ ×•×™×§×™×¤×“×™×” ×¢×‘×¨×™×ª - ×¡×™×‘×•×‘ 2 ×¢× ×›×œ×œ×™ × ×™×§×™×•×Ÿ ××¢×•×“×›× ×™×")
    print("=" * 60)

    # ×”×’×“×¨×•×ª
    dump_path = r'C:\Users\Dan Revital\OneDrive\Documents\gepeta\data\wikipedia\hewiki-20250520-pages-articles.xml.bz2'
    s3_bucket = 'gepeta-datasets'
    s3_prefix_main = 'partly-processed/wikipedia_round_2/'
    s3_prefix_examples = 'partly-processed/round_2_test_examples/'
    max_articles = 1000  # ×”×’×‘×œ×” ×œ×‘×“×™×§×”
    max_examples_per_category = 100

    # ×™×¦×™×¨×ª ××¢×‘×“
    processor = WikipediaProcessorRound2(
        dump_path=dump_path,
        s3_bucket=s3_bucket,
        s3_prefix_main=s3_prefix_main,
        s3_prefix_examples=s3_prefix_examples,
        max_articles=max_articles,
        max_examples_per_category=max_examples_per_category
    )

    # ×‘×“×™×§×ª ×§×™×©×•×¨×™×ª S3
    try:
        processor.s3_client.head_bucket(Bucket=s3_bucket)
        print(f"âœ… ×§×™×©×•×¨×™×ª S3 ×ª×§×™× ×”: s3://{s3_bucket}")
    except Exception as e:
        print(f"âŒ ×‘×¢×™×” ×‘×§×™×©×•×¨×™×ª S3: {e}")
        return

    # ×”×¤×¢×œ×ª ×”×¢×™×‘×•×“
    print(f"\nğŸš€ ××ª×—×™×œ ×¢×™×‘×•×“ ×¢× ×›×œ×œ×™ × ×™×§×™×•×Ÿ ××¢×•×“×›× ×™×...")
    processor.process_dump()


if __name__ == "__main__":
    main()