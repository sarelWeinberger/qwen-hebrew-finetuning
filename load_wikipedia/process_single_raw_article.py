#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import xml.etree.ElementTree as ET
import mwparserfromhell
import re
import bz2
from pathlib import Path


def find_article_in_dump(dump_path, article_title):
    """
    ××•×¦× ×¢×¨×š ×¡×¤×¦×™×¤×™ ×‘×“×××¤ ×•×™×§×™×¤×“×™×
    """
    print(f"ğŸ” ××—×¤×© ×¢×¨×š: '{article_title}' ×‘×“×××¤...")
    print(f"ğŸ“‚ ×“×××¤: {dump_path}")
    print("â³ ×–×” ×¢×œ×•×œ ×œ×§×—×ª ×–××Ÿ...")

    scanned_pages = 0

    with bz2.open(dump_path, 'rt', encoding='utf-8') as file:
        for event, elem in ET.iterparse(file, events=('start', 'end')):
            if event == 'end' and elem.tag.endswith('page'):
                scanned_pages += 1

                # ×”×“×¤×¡×ª ×”×ª×§×“××•×ª ×›×œ 10,000 ×“×¤×™×
                if scanned_pages % 10000 == 0:
                    print(f"   ×¡×¨×§×ª×™ {scanned_pages:,} ×“×¤×™×...")

                # ×—×™×œ×•×¥ ×”×›×•×ª×¨×ª
                title_elem = elem.find('.//{*}title')
                if title_elem is not None and title_elem.text == article_title:

                    print(f"âœ… × ××¦× ×¢×¨×š: '{article_title}' ××—×¨×™ {scanned_pages:,} ×“×¤×™×!")

                    # ×—×™×œ×•×¥ ×”×˜×§×¡×˜
                    revision = elem.find('.//{*}revision')
                    text_elem = revision.find('.//{*}text') if revision is not None else None

                    if text_elem is not None and text_elem.text:
                        raw_wikitext = text_elem.text
                        print(f"ğŸ“ ××•×¨×š ×•×™×§×™-×˜×§×¡×˜ ×’×•×œ××™: {len(raw_wikitext):,} ×ª×•×•×™×")

                        elem.clear()
                        return raw_wikitext
                    else:
                        print(f"âŒ ×œ× × ××¦× ×ª×•×›×Ÿ ×‘×¢×¨×š")
                        elem.clear()
                        return None

                elem.clear()

    print(f"âŒ ×œ× × ××¦× ×¢×¨×š: '{article_title}' (×¡×¨×§×ª×™ {scanned_pages:,} ×“×¤×™×)")
    return None


def normalize_text_for_training(text):
    """
    × ×¨××•×œ ×˜×§×¡×˜ (×‘×“×™×•×§ ×›××• ×‘×ª×•×›× ×™×ª ×”×¨××©×™×ª)
    """
    if not text or not isinstance(text, str):
        return text

    # ×ª×™×§×•×Ÿ ×ª×•×•×™ ×‘×¨×™×—×” ×œ×¤× ×™ ×”×›×œ
    text = text.replace('\\"', '"')
    text = text.replace("\\'", "'")
    text = text.replace('\\\\', '\\')

    # ×ª×™×§×•×Ÿ ×’× ×•×¨×™××¦×™×•×ª ×©×œ ×ª×•×•×™ ×‘×¨×™×—×”
    text = text.replace('&quot;', '"')
    text = text.replace('&#34;', '"')
    text = text.replace('&#39;', "'")

    # ××—×§ ×©×•×¨×•×ª ×—×“×©×•×ª ×•-\r
    text = text.replace('\n', ' ')
    text = text.replace('\r', ' ')
    text = text.replace('\t', ' ')

    # ××—×§ ×ª×•×•×™ ×‘×¨×™×—×” ×©× ×©××¨×•
    text = text.replace('\\n', ' ')
    text = text.replace('\\t', ' ')
    text = text.replace('\\r', ' ')

    # ×¨×•×•×—×™× ××¨×•×‘×™× â†’ ×¨×•×•×— ×™×—×™×“
    text = re.sub(r'\s+', ' ', text)

    # × ×§×” ×¨×•×•×—×™× ×‘×”×ª×—×œ×”/×¡×•×£
    text = text.strip()

    return text


def process_with_our_method(raw_wikitext):
    """
    ×¢×™×‘×•×“ ×¢× ×”×©×™×˜×” ×©×œ× ×• (×‘×“×™×•×§ ×›××• ×‘×ª×•×›× ×™×ª ×”×¨××©×™×ª)
    """
    print(f"âš™ï¸ ××¢×‘×“ ××ª ×”×•×™×§×™-×˜×§×¡×˜...")

    try:
        wikicode = mwparserfromhell.parse(raw_wikitext)
        print(f"âœ… ×•×™×§×™-×˜×§×¡×˜ × ×•×ª×— ×‘×”×¦×œ×—×”")
    except Exception as e:
        print(f"âŒ ×©×’×™××” ×‘× ×™×ª×•×— ×•×™×§×™-×˜×§×¡×˜: {e}")
        # fallback ×œ×˜×§×¡×˜ ×’×•×œ××™
        return normalize_text_for_training(raw_wikitext[:1000])

    # ×”×¡×¨×ª ×ª×‘× ×™×•×ª
    print(f"ğŸ§¹ ××¡×™×¨ ×ª×‘× ×™×•×ª...")
    templates_to_remove = []
    for template in wikicode.filter_templates():
        template_name = str(template.name).strip().lower()

        # ××œ ×ª×¡×™×¨ ×ª×‘× ×™×•×ª ××ª××˜×™×•×ª ×—×©×•×‘×•×ª
        if template_name in ['math', '××ª××˜×™×§×”', '× ×•×¡×—×”']:
            continue

        # ×ª×‘× ×™×•×ª ×œ×”×¡×¨×” ××œ××”
        remove_patterns = [
            'cite', '×¦-', '×”×¢×¨×”', '××§×•×¨', 'reflist', '××§×•×¨×•×ª',
            '×¦×™×•×Ÿ', 'ref', 'citation', 'web', 'news', 'book', 'journal'
        ]
        if any(pattern in template_name for pattern in remove_patterns):
            templates_to_remove.append(template)

    # ×”×¡×¨×ª ×”×ª×‘× ×™×•×ª
    for template in templates_to_remove:
        try:
            wikicode.remove(template)
        except:
            pass

    print(f"âœ… ×”×•×¡×¨×• {len(templates_to_remove)} ×ª×‘× ×™×•×ª")

    # ×”××¨×ª ×§×™×©×•×¨×™× ×¤× ×™××™×™× ×œ×˜×§×¡×˜
    print(f"ğŸ”— ××¢×‘×“ ×§×™×©×•×¨×™×...")
    links_processed = 0
    for link in wikicode.filter_wikilinks():
        try:
            if link.text:
                wikicode.replace(link, str(link.text))
            else:
                title = str(link.title)
                if '|' in title:
                    title = title.split('|')[0]
                wikicode.replace(link, title)
            links_processed += 1
        except:
            pass

    print(f"âœ… ×¢×•×‘×“×• {links_processed} ×§×™×©×•×¨×™×")

    # ×˜×™×¤×•×œ ×‘×ª×’×™×•×ª ××™×•×—×“×•×ª
    print(f"ğŸ·ï¸ ××¢×‘×“ ×ª×’×™×•×ª...")
    math_tags = 0
    for tag in wikicode.filter_tags():
        try:
            if tag.tag.lower() in ['math', 'chem']:
                # ×©××™×¨×ª ×”× ×•×¡×—×” ×¢× ×¡×™××•×Ÿ ××™×•×—×“ - ×‘×“×™×•×§ ×›××• ×‘××“×’×
                wikicode.replace(tag, f"[× ×•×¡×—×”: {tag.contents}]")
                math_tags += 1
            elif tag.tag.lower() in ['ref', 'references']:
                wikicode.remove(tag)
        except:
            pass

    print(f"âœ… × ××¦××• {math_tags} × ×•×¡×—××•×ª ××ª××˜×™×•×ª")

    # ×”××¨×ª ×”×›×œ ×œ×˜×§×¡×˜
    content = str(wikicode.strip_code())
    print(f"ğŸ“„ ××•×¨×š ××—×¨×™ ×”××¨×” ×œ×˜×§×¡×˜: {len(content):,} ×ª×•×•×™×")

    # × ×™×§×•×™ regex
    print(f"ğŸ§½ ××‘×¦×¢ × ×™×§×•×™ regex...")
    # ×”×¡×¨×ª ×ª×‘× ×™×•×ª ×©× ×©××¨×•
    content = re.sub(r'\{\{[^}]*\}\}', '', content)
    # ×”×¡×¨×ª ×§×™×©×•×¨×™× ×©× ×©××¨×•
    content = re.sub(r'\[\[[^]]*\]\]', '', content)
    # ×”×¡×¨×ª ×ª×’×™×•×ª HTML
    content = re.sub(r'<[^>]*>', '', content)

    # ×”×¡×¨×ª ×ª×™××•×¨×™ ×ª××•× ×•×ª ×•××“×™×”
    content = re.sub(r'^(×©×××œ|×™××™×Ÿ|××¨×›×–|×××•×–×¢×¨)\|.*$', '', content, flags=re.MULTILINE)
    content = re.sub(r'^\s*(×©×××œ|×™××™×Ÿ|××¨×›×–|×××•×–×¢×¨)\|.*$', '', content, flags=re.MULTILINE)

    # ×”×¡×¨×ª ×”×¡×‘×¨×™ ×©×¤×•×ª ×–×¨×•×ª ×‘×¡×•×’×¨×™×™×
    foreign_languages = ['×‘×’×¨×× ×™×ª', '×‘×”×•× ×’×¨×™×ª', '×‘×¢×¨×‘×™×ª', '×‘×›×•×¨×“×™×ª', '×‘×× ×’×œ×™×ª',
                         '×‘×¦×¨×¤×ª×™×ª', '×‘××™×˜×œ×§×™×ª', '×‘×¨×•×¡×™×ª', '×‘×™×•×•× ×™×ª', '×‘×œ×˜×™× ×™×ª']
    for lang in foreign_languages:
        pattern = r'\(' + lang + r':.*?\)'
        content = re.sub(pattern, '', content)

    # ×”×¡×¨×ª ×”×¤× ×™×•×ª ×œ×ª××•× ×•×ª
    content = re.sub(r'×¨××• [A-Za-z\s,]+\.', '', content)

    # × ×™×§×•×™ ×›×œ×œ×™
    content = re.sub(r'\n{3,}', '\n\n', content)  # ×©×•×¨×•×ª ×¨×™×§×•×ª ××¨×•×‘×•×ª
    content = re.sub(r' {2,}', ' ', content)  # ×¨×•×•×—×™× ××¨×•×‘×™×
    content = re.sub(r'^\s*=+.*?=+\s*$', '', content, flags=re.MULTILINE)  # ×›×•×ª×¨×•×ª ×•×™×§×™

    print(f"ğŸ“„ ××•×¨×š ××—×¨×™ × ×™×§×•×™ regex: {len(content):,} ×ª×•×•×™×")

    # ×–×™×”×•×™ ×›×•×ª×¨×•×ª (×¤×©×•×˜)
    print(f"ğŸ“‹ ××–×”×” ×›×•×ª×¨×•×ª...")
    content = identify_headers(content)

    # × ×¨××•×œ ×”×˜×§×¡×˜ ×œ××™××•×Ÿ
    print(f"ğŸ”„ ×× ×¨××œ ×˜×§×¡×˜...")
    content = normalize_text_for_training(content)

    print(f"âœ… ××•×¨×š ×¡×•×¤×™: {len(content):,} ×ª×•×•×™×")

    return content


def identify_headers(content):
    """
    ×–×™×”×•×™ ×•×¡×™××•×Ÿ ×›×•×ª×¨×•×ª (×‘×“×™×•×§ ×›××• ×‘×ª×•×›× ×™×ª ×”×¨××©×™×ª)
    """
    lines = content.split('\n')
    processed_lines = []
    headers_found = 0

    for i, line in enumerate(lines):
        line = line.strip()

        # ×–×™×”×•×™ ×›×•×ª×¨×ª ×¤×©×•×˜
        if (line and len(line) < 100 and
                i < len(lines) - 1 and
                len(lines[i + 1].strip()) > 50 and
                line.count('.') <= 1 and
                line.count(',') <= 2):

            # ×‘×“×™×§×•×ª × ×•×¡×¤×•×ª ×œ×•×•×“× ×©×–×• ×›×•×ª×¨×ª
            header_keywords = ['×”×™×¡×˜×•×¨×™×”', '×‘×™×•×’×¨×¤×™×”', '×¨×§×¢', '×ª×•×œ×“×•×ª', '××•×¦×', '×ª×¨×‘×•×ª', '××©×¤×—×ª×•', '×™×œ×“×•×ª×•', '× ×¢×•×¨×™×•',
                               '×”×ª×¤×ª×—×•×ª', '××©×™××•×ª']
            if (not line.endswith('.') or
                    any(keyword in line for keyword in header_keywords)):
                processed_lines.append(f"## {line}")
                headers_found += 1
            else:
                processed_lines.append(line)
        else:
            processed_lines.append(line)

    print(f"âœ… ×–×•×”×• {headers_found} ×›×•×ª×¨×•×ª")
    return '\n'.join(processed_lines)


def main():
    """
    ×”×¤×¢×œ×” ×¨××©×™×ª
    """
    print("ğŸ¯ ××•×¦× ×•××¢×‘×“ ×¢×¨×š ××”×“×××¤ ×©×œ× ×•")
    print("=" * 60)

    # ×”×’×“×¨×•×ª
    dump_path = r'C:\Users\Dan Revital\OneDrive\Documents\gepeta\data\wikipedia\hewiki-latest-pages-articles.xml.bz2'
    article_title = '×¨××ª ×’×Ÿ'

    print(f"ğŸ” ××—×¤×© ×¢×¨×š: '{article_title}'")
    print(f"ğŸ“‚ ×‘×“×××¤: {dump_path}")
    print()

    # ×©×œ×‘ 1: ××¦×™××ª ×”×¢×¨×š
    raw_wikitext = find_article_in_dump(dump_path, article_title)

    if not raw_wikitext:
        print(f"âŒ ×œ× × ××¦× ×”×¢×¨×š '{article_title}'")
        return

    print()

    # ×©×œ×‘ 2: ×¢×™×‘×•×“ ×¢× ×”×©×™×˜×” ×©×œ× ×•
    print("=" * 60)
    processed_content = process_with_our_method(raw_wikitext)

    # ×©×œ×‘ 3: ×©××™×¨×ª ×”×ª×•×¦××•×ª
    print("=" * 60)
    print("ğŸ’¾ ×©×•××¨ ×ª×•×¦××•×ª...")

    # ×©××™×¨×ª ×”×˜×§×¡×˜ ×”×’×•×œ××™
    raw_filename = f"{article_title.replace(' ', '_')}_raw_wikitext.txt"
    with open(raw_filename, 'w', encoding='utf-8') as f:
        f.write(raw_wikitext)
    print(f"âœ… ×•×™×§×™-×˜×§×¡×˜ ×’×•×œ××™ × ×©××¨: {raw_filename}")

    # ×©××™×¨×ª ×”×˜×§×¡×˜ ×”××¢×•×‘×“
    processed_filename = f"{article_title.replace(' ', '_')}_our_processed.txt"
    with open(processed_filename, 'w', encoding='utf-8') as f:
        f.write(processed_content)
    print(f"âœ… ×˜×§×¡×˜ ××¢×•×‘×“ × ×©××¨: {processed_filename}")

    # ×¡×™×›×•×
    print("\n" + "=" * 60)
    print("ğŸ‰ ×”×•×©×œ× ×‘×”×¦×œ×—×”!")
    print(f"ğŸ“Š ×¡×™×›×•×:")
    print(f"   ×•×™×§×™-×˜×§×¡×˜ ×’×•×œ××™: {len(raw_wikitext):,} ×ª×•×•×™×")
    print(f"   ×˜×§×¡×˜ ××¢×•×‘×“: {len(processed_content):,} ×ª×•×•×™×")
    print(f"   ×“×—×™×¡×”: {(1 - len(processed_content) / len(raw_wikitext)) * 100:.1f}%")

    print(f"\nğŸ“„ ×“×•×’××” ××”×˜×§×¡×˜ ×”××¢×•×‘×“ (200 ×ª×•×•×™× ×¨××©×•× ×™×):")
    print(f"   {processed_content[:200]}...")

    print(f"\nğŸ’¡ ×¢×›×©×™×• ×ª×•×›×œ ×œ×”×©×•×•×ª ×¢× ×”×§×•×‘×¥:")
    print(f"   {article_title.replace(' ', '_')}_dikta.txt")


if __name__ == "__main__":
    main()