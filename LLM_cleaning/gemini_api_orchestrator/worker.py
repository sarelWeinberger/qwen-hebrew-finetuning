#!/usr/bin/env python3
"""
Gepeta EC2 Worker - ×¢×•×‘×“ ×™×—×™×“ ×œ×§×•×‘×¥ ××—×“
Usage: python worker.py --prefix Geektime --part 0 --dataset geektime

NOTE: This script expects to run in a virtual environment at /opt/venv
"""

import google.generativeai as genai
import boto3
import pandas as pd
from io import StringIO
import time
import os
import argparse
import json
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from dotenv import load_dotenv

load_dotenv()

# ×”×’×“×¨×•×ª S3
SOURCE_BUCKET = "gepeta-datasets"
SOURCE_PREFIX = "partly-processed/regex-and-dedup"
TARGET_BUCKET = "gepeta-datasets"
TARGET_PREFIX = "processed/"
STATUS_BUCKET = "gepeta-datasets"
STATUS_PREFIX = "worker-status/"

# ×”×’×“×¨×•×ª ×¢×™×‘×•×“
MAX_WORKERS = 10
BATCH_SIZE = 50


class SingleFileProcessor:
    def __init__(self, api_key, prefix, part_number, dataset_name):
        if not api_key:
            raise ValueError("âŒ ×—×¡×¨ GOOGLE_API_KEY!")

        self.prefix = prefix
        self.part_number = part_number
        self.dataset_name = dataset_name.lower()

        genai.configure(api_key=api_key)
        self.model_name = 'gemini-2.0-flash'
        self.s3_client = boto3.client('s3')
        self.worker_id = f"{prefix}_part-{part_number}"

        self.stats = {
            'worker_id': self.worker_id,
            'prefix': prefix,
            'part_number': part_number,
            'dataset': dataset_name,
            'status': 'starting',
            'progress_percent': 0,
            'start_time': datetime.now().isoformat(),
            'total_rows': 0,
            'rows_already_clean': 0,
            'rows_processed_now': 0,
            'rate_limit_errors_found': 0,
            'rows_skipped_so_far': 0,  # ×›××” ×›×‘×¨ ×“×™×œ×’× ×•
            'rows_processed_so_far': 0,  # ×›××” ×›×‘×¨ ×¢×™×‘×“× ×•
            'new_rate_limit_errors': 0  # ×©×’×™××•×ª rate limit ×—×“×©×•×ª ×©×™×¦×¨× ×•
        }

    def update_status(self, status, **kwargs):
        """×¢×“×›×•×Ÿ ×¡×˜×˜×•×¡ ×•×“×™×•×•×—"""
        self.stats['status'] = status
        self.stats['last_update'] = datetime.now().isoformat()

        for key, value in kwargs.items():
            self.stats[key] = value

        try:
            status_key = f"{STATUS_PREFIX}{self.worker_id}.json"

            # ×”××¨×” ×©×œ numpy types ×œ-Python types ×œ×¤× ×™ JSON
            safe_stats = {}
            for key, value in self.stats.items():
                if hasattr(value, 'item'):  # numpy scalar
                    safe_stats[key] = value.item()
                elif isinstance(value, (int, float)):
                    safe_stats[key] = int(value) if isinstance(value, int) else float(value)
                else:
                    safe_stats[key] = value

            self.s3_client.put_object(
                Bucket=STATUS_BUCKET,
                Key=status_key,
                Body=json.dumps(safe_stats, ensure_ascii=False, indent=2),
                ContentType='application/json'
            )
        except Exception as e:
            print(f"âš ï¸ ×©×’×™××” ×‘×¢×“×›×•×Ÿ ×¡×˜×˜×•×¡: {e}")

        percent = self.stats.get('progress_percent', 0)
        print(f"ğŸ“Š {self.worker_id}: {status} - {percent:.1f}%")

    def find_target_file(self):
        """××¦×™××ª ×”×§×•×‘×¥ ×”××¢×•×‘×“ ×‘-processed/"""
        self.update_status("searching_file")

        # ×—×™×¤×•×© ×™×©×¨ ×‘×ª×™×§×™×™×ª processed
        processed_prefix = f"{TARGET_PREFIX}{self.dataset_name}/"

        print(f"ğŸ” ××—×¤×© ×§×•×‘×¥ ××¢×•×‘×“ ×‘-{processed_prefix}")

        paginator = self.s3_client.get_paginator('list_objects_v2')

        for page in paginator.paginate(Bucket=TARGET_BUCKET, Prefix=processed_prefix):
            if 'Contents' not in page:
                continue

            for obj in page['Contents']:
                key = obj['Key']
                filename = os.path.basename(key)

                if (filename.startswith(self.prefix) and
                        f"part-{self.part_number}" in filename and
                        filename.endswith('.csv') and
                        obj['Size'] > 0):
                    print(f"âœ… × ××¦× ×§×•×‘×¥ ××¢×•×‘×“: {filename}")
                    return TARGET_BUCKET, key

        # ×× ×œ× × ××¦× ×‘processed, ×—×¤×© ×‘××§×•× ×”××§×•×¨×™
        print(f"âš ï¸ ×œ× × ××¦× ×§×•×‘×¥ ××¢×•×‘×“, ××—×¤×© ×‘××§×•×¨...")

        for page in paginator.paginate(Bucket=SOURCE_BUCKET, Prefix=SOURCE_PREFIX):
            if 'Contents' not in page:
                continue

            for obj in page['Contents']:
                key = obj['Key']
                filename = os.path.basename(key)

                if (filename.startswith(self.prefix) and
                        f"part-{self.part_number}" in filename and
                        filename.endswith('.csv') and
                        obj['Size'] > 0):
                    print(f"âœ… × ××¦× ×§×•×‘×¥ ××§×•×¨×™: {filename}")
                    return SOURCE_BUCKET, key

        raise FileNotFoundError(f"×œ× × ××¦× ×§×•×‘×¥ ×¢×‘×•×¨ {self.prefix} part-{self.part_number}")

    def count_words(self, text):
        """×¡×¤×™×¨×ª ××™×œ×™× ×‘×˜×§×¡×˜"""
        if pd.isna(text) or not isinstance(text, str):
            return 0
        return len(str(text).split())

    def is_valid_clean_text(self, text):
        """×‘×“×™×§×” ×× cleaned_text ×ª×§×™×Ÿ (×œ× rate limit error)"""
        if pd.isna(text) or not isinstance(text, str):
            return False

        # Debug print
        if text.startswith("[API_ERROR]"):
            print(f"ğŸ” DEBUG: × ××¦× API_ERROR: {text[:100]}...")
            return False

        # ×× ×–×” ×¨×™×§ ××• ×§×¦×¨ ××“×™, ×œ× ×ª×§×™×Ÿ
        if len(text.strip()) < 3:
            print(f"ğŸ” DEBUG: ×˜×§×¡×˜ ×§×¦×¨ ××“×™: '{text}'")
            return False

        return True

    def is_rate_limit_error(self, text):
        """×‘×“×™×§×” ×× ×”×˜×§×¡×˜ ×”×•× ×©×’×™××ª rate limit"""
        if not isinstance(text, str):
            return False

        is_error = (text.startswith("[API_ERROR] 429") or
                    "RATE_LIMIT_EXCEEDED" in text or
                    "Quota exceeded" in text)

        if is_error:
            print(f"ğŸ” DEBUG: × ××¦× Rate Limit Error: {text[:100]}...")

        return is_error
        """×‘×“×™×§×” ×× cleaned_text ×ª×§×™×Ÿ (×œ× rate limit error)"""
        if pd.isna(text) or not isinstance(text, str):
            return False

        # ×× ×–×” ×©×’×™××”, ×œ× ×ª×§×™×Ÿ
        if text.startswith("[API_ERROR]") or text.startswith("[RATE_LIMIT_ERROR]"):
            return False

        # ×× ×–×” ×¨×™×§ ××• ×§×¦×¨ ××“×™, ×œ× ×ª×§×™×Ÿ
        if len(text.strip()) < 3:
            return False

        return True

    def clean_text_with_api(self, text):
        """× ×™×§×•×™ ×˜×§×¡×˜ ×¢× Google API"""
        model = genai.GenerativeModel(self.model_name)

        prompt = f"""× ×§×” ××ª ×”×˜×§×¡×˜ ×”×¢×‘×¨×™ ×”×‘× ××¤×’××™ ×§×™×“×•×“, ×ª×’×™×•×ª HTML, ×¤×¨×¡×•××•×ª ×•×ª×‘× ×™×•×ª. ×”×—×–×¨ ×¨×§ ×˜×§×¡×˜ × ×§×™ ×‘×¢×‘×¨×™×ª:

{text}

×˜×§×¡×˜ × ×§×™:"""

        try:
            response = model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            # ×× ×–×” ×©×’×™××ª rate limit - ×¢×“×›×Ÿ ××•× ×”
            error_msg = f"[API_ERROR] {str(e)}"
            if "429" in str(e) or "RATE_LIMIT_EXCEEDED" in str(e) or "Quota exceeded" in str(e):
                self.stats['new_rate_limit_errors'] += 1
            return error_msg

    def process_texts_parallel(self, texts):
        """×¢×™×‘×•×“ ××§×‘×™×œ×™"""
        results = [''] * len(texts)

        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_index = {
                executor.submit(self.clean_text_with_api, text): i
                for i, text in enumerate(texts)
            }

            for future in as_completed(future_to_index):
                index = future_to_index[future]
                try:
                    result = future.result()
                    results[index] = result
                except Exception as e:
                    results[index] = f"[ERROR] {str(e)}"

        return results

    def process_file(self):
        """×¢×™×‘×•×“ ×”×§×•×‘×¥ ×”×¨××©×™ - ×¢×•×‘×“ ×¢×œ ×§×‘×¦×™× ××¢×•×‘×“×™×"""
        try:
            # ×—×™×¤×•×© ×•×˜×¢×™× ×ª ×§×•×‘×¥ (××¢×•×‘×“ ××• ××§×•×¨×™)
            source_bucket, file_key = self.find_target_file()
            filename = os.path.basename(file_key)
            is_processed_file = source_bucket == TARGET_BUCKET

            self.update_status("loading_file")

            response = self.s3_client.get_object(Bucket=source_bucket, Key=file_key)
            content = response['Body'].read().decode('utf-8')
            df = pd.read_csv(StringIO(content))

            print(f"ğŸ“Š × ×˜×¢×Ÿ: {len(df)} ×©×•×¨×•×ª ×-{'processed' if is_processed_file else 'source'}")

            if 'text' not in df.columns or 'n_count' not in df.columns:
                raise ValueError(f"×—×¡×¨×•×ª ×¢××•×“×•×ª × ×“×¨×©×•×ª ×‘-{filename}")

            # ×‘×“×™×§×” ×× ×™×© ×¢××•×“×ª cleaned_text
            has_cleaned_text = 'cleaned_text' in df.columns
            print(f"ğŸ“‹ ×™×© ×¢××•×“×ª cleaned_text: {'âœ…' if has_cleaned_text else 'âŒ'}")

            if has_cleaned_text:
                # ×–×” ×§×•×‘×¥ ××¢×•×‘×“ - × ×ª×§×Ÿ ×¨×§ rate limit errors
                total_rows = len(df)
                valid_clean = df['cleaned_text'].apply(self.is_valid_clean_text).sum()
                rate_limit_errors = df['cleaned_text'].apply(self.is_rate_limit_error).sum()

                # ×¢×“×›×•×Ÿ stats
                self.stats['total_rows'] = total_rows
                self.stats['rows_already_clean'] = valid_clean
                self.stats['rows_processed_now'] = 0  # ×™×¢×•×“×›×Ÿ ×‘××”×œ×š ×”×¢×™×‘×•×“
                self.stats['rate_limit_errors_found'] = rate_limit_errors

                print(f"ğŸ“ˆ × ×™×ª×•×— ×§×•×‘×¥ ××¢×•×‘×“:")
                print(f"   â€¢ ×¡×”\"×› ×©×•×¨×•×ª: {total_rows:,}")
                print(f"   â€¢ ×›×‘×¨ × ×§×™×•×ª: {valid_clean:,}")
                print(f"   â€¢ ×©×’×™××•×ª rate limit: {rate_limit_errors:,}")

                if rate_limit_errors > 0:
                    # ×¢×™×‘×•×“ ×¨×§ ×©×•×¨×•×ª ×¢× rate limit errors
                    texts_to_process = []
                    indices_to_process = []

                    for idx, row in df.iterrows():
                        if self.is_rate_limit_error(row.get('cleaned_text')):
                            text = row['text']
                            if pd.notna(text) and len(str(text).strip()) > 0:
                                texts_to_process.append(str(text))
                                indices_to_process.append(idx)

                    print(f"ğŸ”„ ××ª×§×Ÿ {len(texts_to_process)} ×©×’×™××•×ª rate limit...")

                    # ×¢×“×›×•×Ÿ ××¡×¤×¨ ×”×©×•×¨×•×ª ×©××ª×¢×‘×“×•×ª
                    self.stats['rows_processed_now'] = len(texts_to_process)

                    # ×©×œ×— ×¢×“×›×•×Ÿ ×¡×˜×˜×•×¡ ×¢× ×›×œ ×”× ×ª×•× ×™×
                    self.update_status("processing")

                    if texts_to_process:
                        total_batches = (len(texts_to_process) + BATCH_SIZE - 1) // BATCH_SIZE
                        self.update_status("processing")

                        processed_results = []
                        for batch_idx in range(0, len(texts_to_process), BATCH_SIZE):
                            batch_num = (batch_idx // BATCH_SIZE) + 1
                            batch = texts_to_process[batch_idx:batch_idx + BATCH_SIZE]

                            progress_percent = (batch_num / total_batches) * 100
                            self.update_status("processing",
                                               current_batch=batch_num,
                                               progress_percent=progress_percent)

                            cleaned_batch = self.process_texts_parallel(batch)
                            processed_results.extend(cleaned_batch)

                            # ×¢×“×›×•×Ÿ ××•× ×” ×”×©×•×¨×•×ª ×©×¢×•×‘×“×•
                            self.stats['rows_processed_so_far'] = min(len(processed_results), len(texts_to_process))
                            self.update_status("processing", progress_percent=progress_percent)

                            if batch_num < total_batches:
                                time.sleep(0.5)

                        # ×”×—×œ×¤×ª ×©×’×™××•×ª rate limit ×‘×˜×§×¡×˜ × ×§×™
                        for i, idx in enumerate(indices_to_process):
                            if i < len(processed_results):
                                df.loc[idx, 'cleaned_text'] = processed_results[i]

                    # ×¢×›×©×™×• ×¢×‘×•×¨ ×¢×œ ×›×œ ×”×©×•×¨×•×ª ×œ×¢×“×›×•×Ÿ ××•× ×™× ×¡×•×¤×™
                    print("ğŸ“Š ××¢×“×›×Ÿ ××•× ×™×...")
                    for idx, row in df.iterrows():
                        if self.is_valid_clean_text(row.get('cleaned_text')):
                            # ×©×•×¨×” × ×§×™×™×” - × ×—×©×‘×ª ×›"×“×•×œ×’×”"
                            if idx < valid_clean:  # ×¨×§ ×× ×‘×××ª ×”×™×ª×” × ×§×™×™×” ××œ×›×ª×—×™×œ×”
                                self.stats['rows_skipped_so_far'] = min(self.stats['rows_skipped_so_far'] + 1,
                                                                        valid_clean)

                        # ×¢×“×›×Ÿ ×¡×˜×˜×•×¡ ×›×œ 500 ×©×•×¨×•×ª
                        if (idx + 1) % 500 == 0:
                            self.update_status("processing")

                    # ×¡×™×™×× ×• - ×¢×“×›×Ÿ ×œ×¡×˜×˜×•×¡ ×¡×•×¤×™
                    self.stats['rows_skipped_so_far'] = valid_clean

                else:
                    print("âœ… ×›×œ ×”×˜×§×¡×˜×™× ×›×‘×¨ × ×§×™×™× - ×¨×§ ×¡×•×¤×¨ ××™×œ×™×")
                    # ×›×œ ×”×©×•×¨×•×ª × ×§×™×•×ª - ×¢×“×›×Ÿ ××•× ×” ×”×“×™×œ×•×’×™×
                    self.stats['rows_skipped_so_far'] = total_rows

                df_result = df.copy()
            else:
                # ×§×•×‘×¥ ××§×•×¨×™ - ×¢×™×‘×•×“ ××œ× ×›××• ×”×§×•×“ ×”××§×•×¨×™
                print("ğŸ”„ ×§×•×‘×¥ ××§×•×¨×™ - ××¢×‘×“ ×”×›×œ")

                texts = df['text'].dropna().tolist()
                if not texts:
                    raise ValueError(f"××™×Ÿ ×˜×§×¡×˜×™× ×‘-{filename}")

                # ×¢×“×›×•×Ÿ stats ×œ×¢×™×‘×•×“ ××œ×
                self.stats['total_rows'] = len(texts)
                self.stats['rows_already_clean'] = 0
                self.stats['rows_processed_now'] = len(texts)
                self.stats['rate_limit_errors_found'] = 0

                total_batches = (len(texts) + BATCH_SIZE - 1) // BATCH_SIZE
                self.update_status("processing")

                all_cleaned_texts = []

                for batch_idx in range(0, len(texts), BATCH_SIZE):
                    batch_num = (batch_idx // BATCH_SIZE) + 1
                    batch = texts[batch_idx:batch_idx + BATCH_SIZE]

                    progress_percent = (batch_num / total_batches) * 100

                    # ×¢×“×›×Ÿ ××•× ×” ×”×©×•×¨×•×ª ×©×¢×•×‘×“×• ×¢×“ ×›×”
                    self.stats['rows_processed_so_far'] = min(batch_idx + len(batch), len(texts))

                    self.update_status("processing",
                                       current_batch=batch_num,
                                       progress_percent=progress_percent)

                    cleaned_batch = self.process_texts_parallel(batch)
                    all_cleaned_texts.extend(cleaned_batch)

                    if batch_num < total_batches:
                        time.sleep(0.5)

                df_result = df.copy()
                df_result['cleaned_text'] = all_cleaned_texts[:len(df)]

            # ×”×•×¡×¤×ª/×¢×“×›×•×Ÿ ×¡×¤×™×¨×ª ××™×œ×™×
            print("ğŸ“Š ××—×©×‘ ××¡×¤×¨ ××™×œ×™×...")
            self.update_status("calculating_words")
            df_result['cleaned_text_words'] = df_result['cleaned_text'].apply(self.count_words)

            # ×—×™×©×•×‘ ×¡×˜×˜×™×¡×˜×™×§×•×ª
            original_words = (df_result['n_count'].sum() - 1)
            cleaned_words = df_result['cleaned_text_words'].sum()

            self.update_status("saving")

            target_key = f"{TARGET_PREFIX}{self.dataset_name}/{filename}"

            csv_buffer = StringIO()
            df_result.to_csv(csv_buffer, index=False, encoding='utf-8')

            self.s3_client.put_object(
                Bucket=TARGET_BUCKET,
                Key=target_key,
                Body=csv_buffer.getvalue(),
                ContentType='text/csv'
            )

            self.update_status("completed",
                               progress_percent=100,
                               total_original_words=int(original_words),
                               total_cleaned_words=int(cleaned_words),
                               target_path=f"s3://{TARGET_BUCKET}/{target_key}")

            print(f"âœ… ×”×•×©×œ×: {self.worker_id}")
            print(f"ğŸ’¾ × ×©××¨ ×‘: s3://{TARGET_BUCKET}/{target_key}")
            print(f"ğŸ“Š ××™×œ×™×: {original_words:,} â†’ {cleaned_words:,}")
            return True

        except Exception as e:
            self.update_status("error", error_message=str(e))
            print(f"âŒ ×©×’×™××” ×‘-{self.worker_id}: {e}")
            return False


def get_api_key_for_worker(part_number):
    """×‘×—×™×¨×ª API Key ×œ×¤×™ ××¡×¤×¨ ×”-part"""
    # ×—×œ×•×§×” ×©×œ 143 ××›×•× ×•×ª ×‘×™×Ÿ 2 API Keys
    # ××›×•× ×•×ª 0-70: SANDBOX_1 (71 ××›×•× ×•×ª)
    # ××›×•× ×•×ª 71-142: SANDBOX_2 (72 ××›×•× ×•×ª)
    if part_number <= 70:
        api_key = os.getenv("GOOGLE_API_KEY_SANDBOX_1")
        key_name = "SANDBOX_1"
    else:
        api_key = os.getenv("GOOGLE_API_KEY_SANDBOX_2")
        key_name = "SANDBOX_2"

    if not api_key:
        # fallback ×œSANDBOX_2 ×× ×”××¤×ª×— ×œ× × ××¦×
        api_key = os.getenv("GOOGLE_API_KEY_SANDBOX_2")
        key_name = "SANDBOX_2_FALLBACK"

    print(f"ğŸ”‘ ××©×ª××© ×‘-API Key: {key_name} (part-{part_number})")
    return api_key


def main():
    parser = argparse.ArgumentParser(description='Gepeta Single File Worker')
    parser.add_argument('--prefix', required=True, help='Dataset prefix')
    parser.add_argument('--part', type=int, required=True, help='Part number')
    parser.add_argument('--dataset', required=True, help='Dataset name')

    args = parser.parse_args()

    # ×‘×—×™×¨×ª API Key ×œ×¤×™ part number
    api_key = get_api_key_for_worker(args.part)
    if not api_key:
        print("âŒ ×œ× × ××¦× API Key ××ª××™×")
        return False

    try:
        processor = SingleFileProcessor(
            api_key=api_key,
            prefix=args.prefix,
            part_number=args.part,
            dataset_name=args.dataset
        )

        success = processor.process_file()
        return success

    except Exception as e:
        print(f"âŒ ×©×’×™××” ×§×¨×™×˜×™×ª: {e}")
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)