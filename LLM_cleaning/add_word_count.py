#!/usr/bin/env python3
"""
Dataset Summary Tool - ××—×©×‘ ×¡×™×›×•× ××™×œ×™× ×¢×‘×•×¨ ×“×˜××¡×˜×™× ××¢×•×‘×“×™×

×§×•×¨× ×§×‘×¦×™× ××¢×•×‘×“×™× ×-S3 ×•××—×©×‘:
- text_words (×¡×™×›×•× n_count ××™× ×•×¡ 1)
- clean_text_words (×¡×¤×™×¨×ª ××™×œ×™× ×‘-cleaned_text)

×™×•×¦×¨ ×§×•×‘×¥ CSV ×¢× ×¡×™×›×•× ×œ×›×œ ×“×˜××¡×˜
"""

import boto3
import pandas as pd
from io import StringIO
import os
from datetime import datetime

# =============================================================================
# ×”×’×“×¨×•×ª ×“×˜××¡×˜×™×
# =============================================================================

DATASETS_CONFIG = [
    {
        'name': 'Geektime',
        'type': 'folder',  # ×ª×™×§×™×” ×©×œ××”
        'bucket': 'gepeta-datasets',
        'path': 'processed/geektime/'
    },
    {
        'name': 'YisraelHayom',
        'type': 'file',  # ×§×•×‘×¥ ×™×—×™×“
        'bucket': 'gepeta-datasets',
        'path': 'processed/yisraelhayom/YisraelHayomData-Combined-Deduped.forgpt_part-0_cleaned.csv'
    }
]

# ×”×’×“×¨×•×ª ×¤×œ×˜
OUTPUT_BUCKET = 'gepeta-datasets'
OUTPUT_PREFIX = 'summaries/'


class DatasetSummaryTool:
    """×›×œ×™ ×œ×¡×™×›×•× ×¡×˜×˜×™×¡×˜×™×§×•×ª ×“×˜××¡×˜×™× ××¢×•×‘×“×™×"""

    def __init__(self):
        self.s3_client = boto3.client('s3')
        self.summary_data = []

    def count_words(self, text):
        """×¡×¤×™×¨×ª ××™×œ×™× ×‘×˜×§×¡×˜"""
        if pd.isna(text) or text == '':
            return 0
        words = str(text).split()
        return len(words)

    def read_csv_from_s3(self, bucket, key):
        """×§×¨×™××ª CSV ×-S3"""
        try:
            print(f"  ğŸ“– ×§×•×¨×: {os.path.basename(key)}")
            response = self.s3_client.get_object(Bucket=bucket, Key=key)
            content = response['Body'].read().decode('utf-8')
            df = pd.read_csv(StringIO(content))
            return df
        except Exception as e:
            print(f"  âŒ ×©×’×™××” ×‘×§×¨×™××ª {key}: {e}")
            return None

    def list_files_in_folder(self, bucket, prefix):
        """×¨×©×™××ª ×§×‘×¦×™× ×‘×ª×™×§×™×”"""
        files = []
        try:
            paginator = self.s3_client.get_paginator('list_objects_v2')

            for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
                if 'Contents' not in page:
                    continue

                for obj in page['Contents']:
                    key = obj['Key']
                    if key.endswith('.csv') and obj['Size'] > 0:
                        files.append(key)

        except Exception as e:
            print(f"âŒ ×©×’×™××” ×‘×¨×™×©×•× ×§×‘×¦×™× ×-{prefix}: {e}")

        return files

    def process_dataset(self, dataset_config):
        """×¢×™×‘×•×“ ×“×˜××¡×˜ ×™×—×™×“"""
        dataset_name = dataset_config['name']
        dataset_type = dataset_config['type']
        bucket = dataset_config['bucket']
        path = dataset_config['path']

        print(f"\nğŸ” ××¢×‘×“ ×“×˜××¡×˜: {dataset_name}")
        print(f"ğŸ“ ××™×§×•×: s3://{bucket}/{path}")

        if dataset_type == 'folder':
            # ×ª×™×§×™×” ×©×œ××”
            files = self.list_files_in_folder(bucket, path)
            if not files:
                print(f"âš ï¸ ×œ× × ××¦××• ×§×‘×¦×™× ×‘-{path}")
                return

            print(f"âœ… × ××¦××• {len(files)} ×§×‘×¦×™×")

            # ×¢×™×‘×•×“ ×›×œ ×”×§×‘×¦×™× ×‘×ª×™×§×™×”
            total_text_words = 0
            total_clean_text_words = 0
            processed_files = 0

            for file_key in files:
                df = self.read_csv_from_s3(bucket, file_key)
                if df is None:
                    continue

                # ×‘×“×™×§×ª ×¢××•×“×•×ª × ×“×¨×©×•×ª
                if 'n_count' not in df.columns or 'cleaned_text' not in df.columns:
                    print(f"  âš ï¸ ×—×¡×¨×•×ª ×¢××•×“×•×ª ×‘-{os.path.basename(file_key)}")
                    continue

                # ×—×™×©×•×‘ ××™×œ×™× ×‘××§×•×¨ (××™× ×•×¡ 1 ××”×¡×š ×”×›×œ - ×ª×™×§×•×Ÿ ×”×›×•×ª×¨×ª)
                file_text_words = df['n_count'].sum() - 1

                # ×—×™×©×•×‘ ××™×œ×™× ×‘×˜×§×¡×˜ × ×§×™
                file_clean_words = df['cleaned_text'].apply(self.count_words).sum()

                total_text_words += file_text_words
                total_clean_text_words += file_clean_words
                processed_files += 1

                print(f"  âœ… {os.path.basename(file_key)}: {file_text_words:,} â†’ {file_clean_words:,} ××™×œ×™×")

        elif dataset_type == 'file':
            # ×§×•×‘×¥ ×™×—×™×“
            df = self.read_csv_from_s3(bucket, path)
            if df is None:
                return

            # ×‘×“×™×§×ª ×¢××•×“×•×ª × ×“×¨×©×•×ª
            if 'n_count' not in df.columns or 'cleaned_text' not in df.columns:
                print(f"âŒ ×—×¡×¨×•×ª ×¢××•×“×•×ª ×‘×§×•×‘×¥")
                return

            # ×—×™×©×•×‘ ××™×œ×™× ×‘××§×•×¨ (××™× ×•×¡ 1 ××”×¡×š ×”×›×œ - ×ª×™×§×•×Ÿ ×”×›×•×ª×¨×ª)
            total_text_words = df['n_count'].sum() - 1

            # ×—×™×©×•×‘ ××™×œ×™× ×‘×˜×§×¡×˜ × ×§×™
            total_clean_text_words = df['cleaned_text'].apply(self.count_words).sum()
            processed_files = 1

            print(f"  âœ… {os.path.basename(path)}: {total_text_words:,} â†’ {total_clean_text_words:,} ××™×œ×™×")

        else:
            print(f"âŒ ×¡×•×’ ×“×˜××¡×˜ ×œ× ××•×›×¨: {dataset_type}")
            return

        # ×”×“×¤×¡×ª ×¡×™×›×•× ×”×“×˜××¡×˜
        print(f"\nğŸ“Š ×¡×™×›×•× {dataset_name}:")
        print(f"  ğŸ“„ ×§×‘×¦×™× ××¢×•×‘×“×™×: {processed_files}")
        print(f"  ğŸ“ ××™×œ×™× ××§×•×¨×™×•×ª: {total_text_words:,}")
        print(f"  âœ¨ ××™×œ×™× × ×§×™×•×ª: {total_clean_text_words:,}")
        if total_text_words > 0:
            reduction = ((total_text_words - total_clean_text_words) / total_text_words * 100)
            print(f"  ğŸ“‰ ×”×¤×—×ª×”: {reduction:.1f}%")

        # ×”×•×¡×¤×” ×œ×¡×™×›×•× ×”×›×œ×œ×™
        self.summary_data.append({
            'Dataset': dataset_name,
            'text_words': total_text_words,
            'clean_text_words': total_clean_text_words
        })

    def save_summary_to_s3(self):
        """×©××™×¨×ª ×”×¡×™×›×•× ×œ-S3"""
        try:
            # ×™×¦×™×¨×ª DataFrame
            summary_df = pd.DataFrame(self.summary_data)

            # ×”×•×¡×¤×ª ×©×•×¨×ª ×¡×™×›×•× ×›×œ×œ×™
            if len(summary_df) > 1:
                total_row = {
                    'Dataset': 'TOTAL',
                    'text_words': summary_df['text_words'].sum(),
                    'clean_text_words': summary_df['clean_text_words'].sum()
                }
                summary_df = pd.concat([summary_df, pd.DataFrame([total_row])], ignore_index=True)

            # ×©××™×¨×” ×œ-S3
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_key = f"{OUTPUT_PREFIX}dataset_word_summary_{timestamp}.csv"

            csv_buffer = StringIO()
            summary_df.to_csv(csv_buffer, index=False, encoding='utf-8')

            self.s3_client.put_object(
                Bucket=OUTPUT_BUCKET,
                Key=output_key,
                Body=csv_buffer.getvalue(),
                ContentType='text/csv'
            )

            print(f"\nâœ… ×¡×™×›×•× × ×©××¨ ×‘: s3://{OUTPUT_BUCKET}/{output_key}")

            # ×”×“×¤×¡×ª ×”×¡×™×›×•×
            print(f"\nğŸ“‹ ×¡×™×›×•× ×¡×•×¤×™:")
            print(summary_df.to_string(index=False))

            return True

        except Exception as e:
            print(f"âŒ ×©×’×™××” ×‘×©××™×¨×ª ×”×¡×™×›×•×: {e}")
            return False

    def run_summary(self):
        """×”×¨×¦×ª ×¡×™×›×•× ××œ×"""
        print("ğŸš€ ××ª×—×™×œ ×¡×™×›×•× ×“×˜××¡×˜×™× ××¢×•×‘×“×™×")
        print("=" * 60)

        # ×¢×™×‘×•×“ ×›×œ ×”×“×˜××¡×˜×™×
        for dataset_config in DATASETS_CONFIG:
            self.process_dataset(dataset_config)

        # ×©××™×¨×ª ×”×¡×™×›×•×
        success = self.save_summary_to_s3()

        if success:
            print(f"\nğŸ‰ ×¡×™×›×•× ×”×•×©×œ× ×‘×”×¦×œ×—×”!")
        else:
            print(f"\nâŒ ×©×’×™××” ×‘×©××™×¨×ª ×”×¡×™×›×•×")


def main():
    """×¤×•× ×§×¦×™×” ×¨××©×™×ª"""
    print("ğŸ” Dataset Summary Tool")
    print("=" * 60)
    print("ğŸ“Š ××—×©×‘ ×¡×™×›×•× ××™×œ×™× ×¢×‘×•×¨ ×“×˜××¡×˜×™× ××¢×•×‘×“×™×")
    print()

    print("ğŸ“‹ ×“×˜××¡×˜×™× ×©×™×¢×•×‘×“×•:")
    for i, dataset in enumerate(DATASETS_CONFIG, 1):
        print(f"  {i}. {dataset['name']}: s3://{dataset['bucket']}/{dataset['path']}")

    print()

    try:
        tool = DatasetSummaryTool()
        tool.run_summary()

    except Exception as e:
        print(f"âŒ ×©×’×™××” ×›×œ×œ×™×ª: {e}")

    print(f"\nâ° ×¡×™×•×: {datetime.now().strftime('%H:%M:%S')}")


if __name__ == "__main__":
    main()