#!/usr/bin/env python3
"""
Gepeta Project - Geektime Files Processor
×¢×™×‘×•×“ ×§×‘×¦×™ Geektime ×¢× Google Gemma API

Usage:
1. pip install google-generativeai boto3 pandas python-dotenv
2. ×”×’×“×¨ GOOGLE_API_KEY ×•-AWS credentials
3. python geektime_processor.py
"""

import google.generativeai as genai
import boto3
import pandas as pd
from io import StringIO
import time
import os
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from dotenv import load_dotenv
import json
import random

# =============================================================================
# ×”×’×“×¨×•×ª
# =============================================================================

load_dotenv()

# API Keys
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY_SANDBOX_2", "YOUR_API_KEY_HERE")

# S3 Settings
SOURCE_BUCKET = "gepeta-datasets"
SOURCE_PREFIX = "partly-processed/regex-and-dedup"
TARGET_BUCKET = "gepeta-datasets"
TARGET_PREFIX = "processed/"

# Processing Settings
PREFIX_FILTER = "Geektime"  # ×”×§×™×“×•××ª ×©×× ×—× ×• ××¢×‘×“×™× (× ×™×ª×Ÿ ×œ×©×™× ×•×™)
MAX_WORKERS = 10  # ×”×¤×—×ª×ª×™ ×-10 ×œ-5 ×œ×× ×™×¢×ª quota issues
BATCH_SIZE = 25  # ×”×¤×—×ª×ª×™ ×-50 ×œ-25 ×œ×× ×™×¢×ª quota issues
TEST_LIMIT = 100  # ××¡×¤×¨ ×§×‘×¦×™× ×œ×‘×“×™×§×” (None ×œ×›×œ ×”×§×‘×¦×™×)

# Retry Settings for Quota Management
MAX_RETRIES = 5  # ××¡×¤×¨ ××§×¡×™××œ×™ ×©×œ × ×™×¡×™×•× ×•×ª
BASE_RETRY_DELAY = 10  # ×”×©×”×™×” ×‘×¡×™×¡×™×ª ×‘×©× ×™×•×ª
MAX_RETRY_DELAY = 300  # ×”×©×”×™×” ××§×¡×™××œ×™×ª ×‘×©× ×™×•×ª
QUOTA_COOLDOWN = 60  # ×”×©×”×™×” ××—×¨×™ quota error ×‘×©× ×™×•×ª

class GeektimeProcessor:
    """××¢×‘×“ ×§×‘×¦×™ Geektime ×¢× Google API"""

    def __init__(self, api_key):
        """××ª×—×•×œ ×”××¢×‘×“"""
        if api_key == "YOUR_API_KEY_HERE":
            raise ValueError("âŒ ×¢×“×›×Ÿ ××ª GOOGLE_API_KEY!")

        # Google AI Setup
        genai.configure(api_key=api_key)
        #self.model_name = 'gemma-3-27b-it'
        self.model_name = 'gemini-1.5-flash'

        # S3 Setup
        self.s3_client = boto3.client('s3')

        # Statistics
        self.stats = {
            'files_processed': 0,
            'texts_processed': 0,
            'total_time': 0,
            'errors': [],
            'quota_errors': 0,
            'retries': 0,
            'start_time': datetime.now()
        }

        print("ğŸš€ Gepeta Generic Prefix Processor ××•×›×Ÿ")
        print(f"ğŸ“ ××§×•×¨: s3://{SOURCE_BUCKET}/{SOURCE_PREFIX}")
        print(f"ğŸ¯ ×™×¢×“: s3://{TARGET_BUCKET}/{TARGET_PREFIX}{PREFIX_FILTER.lower()}/")
        print(f"ğŸ” ×§×™×“×•××ª: {PREFIX_FILTER}")
        print(f"ğŸ‘¥ Workers: {MAX_WORKERS}")
        print(f"â° ×”×ª×—×œ×”: {self.stats['start_time'].strftime('%H:%M:%S')}")

    def list_geektime_files(self, limit=None):
        """××¦×™××ª ×›×œ ×§×‘×¦×™ Geektime"""
        print(f"ğŸ” ××—×¤×© ×§×‘×¦×™ {PREFIX_FILTER}...")

        files = []
        paginator = self.s3_client.get_paginator('list_objects_v2')

        page_count = 0
        for page in paginator.paginate(Bucket=SOURCE_BUCKET, Prefix=SOURCE_PREFIX):
            page_count += 1
            print(f"  ×¡×•×¨×§ ×“×£ {page_count}...")

            if 'Contents' not in page:
                continue

            page_files = [
                obj['Key'] for obj in page['Contents']
                if obj['Key'].endswith('.csv')
                   and PREFIX_FILTER in os.path.basename(obj['Key'])
                   and obj['Size'] > 0
            ]

            files.extend(page_files)
            print(f"  ×“×£ {page_count}: × ××¦××• {len(page_files)} ×§×‘×¦×™ {PREFIX_FILTER}")

            # ×”×’×‘×œ×” ×œ×‘×“×™×§×”
            if limit and len(files) >= limit:
                files = files[:limit]
                break

        print(f"âœ… × ××¦××• {len(files)} ×§×‘×¦×™ {PREFIX_FILTER} ×›×•×œ×œ")
        return files

    def read_csv_from_s3(self, bucket, key):
        """×§×¨×™××ª CSV ×-S3"""
        try:
            response = self.s3_client.get_object(Bucket=bucket, Key=key)
            content = response['Body'].read().decode('utf-8')
            df = pd.read_csv(StringIO(content))
            return df
        except Exception as e:
            print(f"âŒ ×©×’×™××” ×‘×§×¨×™××ª {key}: {e}")
            return None

    def save_csv_to_s3(self, df, bucket, key):
        """×©××™×¨×ª CSV ×œ-S3"""
        try:
            csv_buffer = StringIO()
            df.to_csv(csv_buffer, index=False, encoding='utf-8')

            self.s3_client.put_object(
                Bucket=bucket,
                Key=key,
                Body=csv_buffer.getvalue(),
                ContentType='text/csv'
            )
            return True
        except Exception as e:
            print(f"âŒ ×©×’×™××” ×‘×©××™×¨×ª {key}: {e}")
            return False

    def is_quota_error(self, error):
        """×‘×“×™×§×” ×× ×”×©×’×™××” ×§×©×•×¨×” ×œ-quota"""
        error_str = str(error).lower()
        quota_indicators = [
            'quota',
            'rate limit',
            'too many requests',
            'resource_exhausted',
            'quota_value',
            'retry_delay'
        ]
        return any(indicator in error_str for indicator in quota_indicators)

    def extract_retry_delay(self, error):
        """×—×™×œ×•×¥ ×–××Ÿ ×”××ª× ×” ××”×©×’×™××” ×× ×§×™×™×"""
        try:
            error_str = str(error)
            # ×—×™×¤×•×© ××—×¨ retry_delay
            if 'retry_delay' in error_str and 'seconds:' in error_str:
                import re
                match = re.search(r'seconds:\s*(\d+)', error_str)
                if match:
                    return int(match.group(1))
        except:
            pass
        return None

    def clean_text_with_api(self, text, retry_count=0):
        """× ×™×§×•×™ ×˜×§×¡×˜ ×™×—×™×“ ×¢× Google API ×•×˜×™×¤×•×œ ×‘×©×’×™××•×ª quota"""
        model = genai.GenerativeModel(self.model_name)

        prompt = f"""× ×§×” ××ª ×”×˜×§×¡×˜ ×”×¢×‘×¨×™ ×”×‘× ××¤×’××™ ×§×™×“×•×“, ×ª×’×™×•×ª HTML, ×¤×¨×¡×•××•×ª ×•×ª×‘× ×™×•×ª. ×”×—×–×¨ ×¨×§ ×˜×§×¡×˜ × ×§×™ ×‘×¢×‘×¨×™×ª:

{text[:800]}

×˜×§×¡×˜ × ×§×™:"""

        for attempt in range(MAX_RETRIES):
            try:
                response = model.generate_content(prompt)
                return response.text.strip()

            except Exception as e:
                if self.is_quota_error(e):
                    self.stats['quota_errors'] += 1
                    self.stats['retries'] += 1

                    # ×—×™×œ×•×¥ ×–××Ÿ ×”××ª× ×” ××”×©×’×™××”
                    suggested_delay = self.extract_retry_delay(e)

                    if suggested_delay:
                        wait_time = suggested_delay + random.randint(1, 5)  # ×”×•×¡×¤×ª jitter
                        print(f"â³ Quota error - ×××ª×™×Ÿ {wait_time} ×©× ×™×•×ª (×”×•×¨××” ××”×©×¨×ª)")
                    else:
                        # ×—×™×©×•×‘ exponential backoff
                        wait_time = min(BASE_RETRY_DELAY * (2 ** attempt) + random.randint(1, 10), MAX_RETRY_DELAY)
                        print(f"â³ Quota error - ×××ª×™×Ÿ {wait_time} ×©× ×™×•×ª (× ×™×¡×™×•×Ÿ {attempt + 1}/{MAX_RETRIES})")

                    time.sleep(wait_time)
                    continue
                else:
                    # ×©×’×™××” ××—×¨×ª - ×œ× quota
                    print(f"âŒ API Error (×œ× quota): {str(e)[:100]}...")
                    return f"[API_ERROR] {str(e)}"

        # ×× ×”×’×¢× ×• ×œ×›××Ÿ, ×›×œ ×”× ×™×¡×™×•× ×•×ª × ×›×©×œ×•
        print(f"âŒ ×›×©×œ ×‘×›×œ {MAX_RETRIES} ×”× ×™×¡×™×•× ×•×ª - ××•×•×ª×¨ ×¢×œ ×”×˜×§×¡×˜")
        return "[MAX_RETRIES_EXCEEDED] ×œ× ×”×¦×œ×™×— ×œ×¢×‘×“ ××—×¨×™ ××¡×¤×¨ × ×™×¡×™×•× ×•×ª"

    def process_texts_parallel(self, texts):
        """×¢×™×‘×•×“ ××§×‘×™×œ×™ ×©×œ ×¨×©×™××ª ×˜×§×¡×˜×™× ×¢× ×˜×™×¤×•×œ ××©×•×¤×¨ ×‘×©×’×™××•×ª"""
        results = [''] * len(texts)  # ×©××™×¨×ª ×¡×“×¨

        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # ×©×œ×™×—×ª ×‘×§×©×•×ª ×¢× index
            future_to_index = {
                executor.submit(self.clean_text_with_api, text): i
                for i, text in enumerate(texts)
            }

            # ××™×¡×•×£ ×ª×•×¦××•×ª
            completed_count = 0
            for future in as_completed(future_to_index):
                index = future_to_index[future]
                try:
                    result = future.result()
                    results[index] = result
                    completed_count += 1

                    # ×”×“×¤×¡×ª ×”×ª×§×“××•×ª ×›×œ 10 ×˜×§×¡×˜×™×
                    if completed_count % 10 == 0:
                        print(f"    âœ… ×”×•×©×œ××• {completed_count}/{len(texts)} ×˜×§×¡×˜×™×")

                except Exception as e:
                    results[index] = f"[THREAD_ERROR] {str(e)}"
                    print(f"âŒ ×©×’×™××” ×‘×˜×§×¡×˜ {index}: {str(e)[:50]}...")

        return results

    def process_single_file(self, file_key):
        """×¢×™×‘×•×“ ×§×•×‘×¥ ×™×—×™×“"""
        file_name = os.path.basename(file_key)
        print(f"\nğŸ“ ××¢×‘×“: {file_name}")

        file_start_time = time.time()

        # ×§×¨×™××ª ×§×•×‘×¥
        df = self.read_csv_from_s3(SOURCE_BUCKET, file_key)
        if df is None:
            self.stats['errors'].append(f"×œ× ×”×¦×œ×™×— ×œ×§×¨×•×: {file_name}")
            return False

        print(f"ğŸ“Š × ×˜×¢×Ÿ: {len(df)} ×©×•×¨×•×ª")

        # ×‘×“×™×§×ª ×¢××•×“×ª text
        if 'text' not in df.columns:
            print(f"âŒ ××™×Ÿ ×¢××•×“×ª 'text' ×‘-{file_name}")
            self.stats['errors'].append(f"××™×Ÿ ×¢××•×“×ª text: {file_name}")
            return False

        # ×”×›× ×ª ×˜×§×¡×˜×™× ×œ×¢×™×‘×•×“
        texts = df['text'].dropna().tolist()
        if not texts:
            print(f"âš ï¸ ××™×Ÿ ×˜×§×¡×˜×™× ×‘-{file_name}")
            return False

        print(f"ğŸ”„ ××¢×‘×“ {len(texts)} ×˜×§×¡×˜×™× ×‘×‘××¦'×™×...")

        # ×¢×™×‘×•×“ ×‘×‘××¦'×™×
        all_cleaned_texts = []
        total_batches = (len(texts) + BATCH_SIZE - 1) // BATCH_SIZE

        for batch_idx in range(0, len(texts), BATCH_SIZE):
            batch_num = (batch_idx // BATCH_SIZE) + 1
            batch = texts[batch_idx:batch_idx + BATCH_SIZE]

            print(f"  ğŸ“¦ ×‘××¦' {batch_num}/{total_batches} ({len(batch)} ×˜×§×¡×˜×™×)")

            batch_start = time.time()
            cleaned_batch = self.process_texts_parallel(batch)
            batch_time = time.time() - batch_start

            all_cleaned_texts.extend(cleaned_batch)

            print(f"  âœ… ×”×•×©×œ×: {batch_time:.1f}s ({batch_time / len(batch):.2f}s/text)")

            # ×”×©×”×™×” ×‘×™×Ÿ ×‘××¦'×™× ×œ×× ×™×¢×ª rate limiting
            if batch_num < total_batches:
                sleep_time = 2 + random.uniform(0.5, 2.0)  # 2-4 ×©× ×™×•×ª ×¢× jitter
                print(f"  â³ ×”××ª× ×” {sleep_time:.1f}s ×œ×¤× ×™ ×”×‘××¦' ×”×‘×...")
                time.sleep(sleep_time)

        # ×™×¦×™×¨×ª DataFrame ×¢× ×ª×•×¦××•×ª
        df_result = df.copy()
        df_result['cleaned_text'] = all_cleaned_texts[:len(df)]

        # ×©××™×¨×” ×œ-S3
        target_key = f"{TARGET_PREFIX}{PREFIX_FILTER.lower()}/{file_name}"
        success = self.save_csv_to_s3(df_result, TARGET_BUCKET, target_key)

        file_time = time.time() - file_start_time

        if success:
            print(f"âœ… × ×©××¨: s3://{TARGET_BUCKET}/{target_key}")
            print(f"â° ×–××Ÿ ×§×•×‘×¥: {file_time / 60:.1f} ×“×§×•×ª")

            # ×¢×“×›×•×Ÿ ×¡×˜×˜×™×¡×˜×™×§×•×ª
            self.stats['files_processed'] += 1
            self.stats['texts_processed'] += len(texts)
            self.stats['total_time'] += file_time

            return True
        else:
            self.stats['errors'].append(f"×©×’×™××” ×‘×©××™×¨×ª: {file_name}")
            return False

    def print_progress(self, current, total, start_time):
        """×”×“×¤×¡×ª ×”×ª×§×“××•×ª"""
        elapsed = time.time() - start_time
        if current > 0:
            avg_time_per_file = elapsed / current
            estimated_remaining = (total - current) * avg_time_per_file

            print(f"ğŸ“Š ×”×ª×§×“××•×ª: {current}/{total} ({current / total * 100:.1f}%)")
            print(f"â° ×–××Ÿ ×©×—×œ×£: {elapsed / 60:.1f} ×“×§×•×ª")
            print(f"ğŸ”® ×–××Ÿ ××©×•×¢×¨ ×œ×¡×™×•×: {estimated_remaining / 60:.1f} ×“×§×•×ª")
            print(f"ğŸ“ˆ ×§×¦×‘: {self.stats['texts_processed'] / elapsed:.1f} ×˜×§×¡×˜×™×/×©× ×™×™×”")

            # ×”×•×¡×¤×ª ××™×“×¢ ×¢×œ quota errors
            if self.stats['quota_errors'] > 0:
                print(f"âš ï¸ ×©×’×™××•×ª Quota: {self.stats['quota_errors']} (× ×™×¡×™×•× ×•×ª ×—×•×–×¨×™×: {self.stats['retries']})")

    def print_final_stats(self):
        """×”×“×¤×¡×ª ×¡×˜×˜×™×¡×˜×™×§×•×ª ×¡×•×¤×™×•×ª"""
        total_time = time.time() - self.stats['start_time'].timestamp()

        print(f"\n{'=' * 60}")
        print("ğŸ“Š ×¡×™×›×•× ×¢×™×‘×•×“ Geektime")
        print("=" * 60)
        print(f"âœ… ×§×‘×¦×™× ××¢×•×‘×“×™×: {self.stats['files_processed']}")
        print(f"ğŸ“ ×˜×§×¡×˜×™× ××¢×•×‘×“×™×: {self.stats['texts_processed']:,}")
        print(f"â° ×–××Ÿ ×›×•×œ×œ: {total_time / 60:.1f} ×“×§×•×ª")

        if self.stats['texts_processed'] > 0:
            avg_time = total_time / self.stats['texts_processed']
            print(f"âš¡ ×–××Ÿ ×××•×¦×¢ ×œ×˜×§×¡×˜: {avg_time:.2f} ×©× ×™×•×ª")
            print(f"ğŸš€ ×§×¦×‘ ×¢×™×‘×•×“: {self.stats['texts_processed'] / total_time:.1f} ×˜×§×¡×˜×™×/×©× ×™×™×”")

        # ×”×¦×’×ª ××™×“×¢ ×¢×œ quota management
        if self.stats['quota_errors'] > 0:
            print(f"\nğŸ“Š × ×™×”×•×œ Quota:")
            print(f"âš ï¸  ×©×’×™××•×ª Quota: {self.stats['quota_errors']}")
            print(f"ğŸ”„ × ×™×¡×™×•× ×•×ª ×—×•×–×¨×™×: {self.stats['retries']}")
            success_rate = ((self.stats['texts_processed']) / (
                        self.stats['texts_processed'] + self.stats['quota_errors'])) * 100
            print(f"âœ… ××—×•×– ×”×¦×œ×—×”: {success_rate:.1f}%")

        if self.stats['errors']:
            print(f"\nâŒ ×©×’×™××•×ª ({len(self.stats['errors'])}):")
            for error in self.stats['errors'][:5]:  # ×¨×§ 5 ×¨××©×•× ×•×ª
                print(f"  â€¢ {error}")
            if len(self.stats['errors']) > 5:
                print(f"  ... ×•×¢×•×“ {len(self.stats['errors']) - 5} ×©×’×™××•×ª")

        print(f"\nğŸ’¾ ×§×‘×¦×™× × ×©××¨×• ×‘: s3://{TARGET_BUCKET}/{TARGET_PREFIX}{PREFIX_FILTER.lower()}/")

    def run_processing(self, test_mode=True):
        """×”×¨×¦×ª ×¢×™×‘×•×“ ××œ×"""
        print(f"ğŸš€ ××ª×—×™×œ ×¢×™×‘×•×“ ×§×‘×¦×™ {PREFIX_FILTER}")

        # ××¦×™××ª ×§×‘×¦×™×
        limit = TEST_LIMIT if test_mode else None
        files = self.list_geektime_files(limit)

        if not files:
            print(f"âŒ ×œ× × ××¦××• ×§×‘×¦×™ {PREFIX_FILTER}")
            return

        print(f"\nğŸ¯ ××¦×‘: {'×‘×“×™×§×”' if test_mode else '×™×™×¦×•×¨'}")
        print(f"ğŸ“ ××¡×¤×¨ ×§×‘×¦×™× ×œ×¢×™×‘×•×“: {len(files)}")
        print(f"âš™ï¸  ×”×’×“×¨×•×ª Quota Management:")
        print(f"   â€¢ Max Workers: {MAX_WORKERS}")
        print(f"   â€¢ Batch Size: {BATCH_SIZE}")
        print(f"   â€¢ Max Retries: {MAX_RETRIES}")
        print(f"   â€¢ Base Retry Delay: {BASE_RETRY_DELAY}s")

        # ××™×©×•×¨ ××©×ª××©
        if not test_mode:
            response = input(f"\nâ“ ×œ×”×ª×—×™×œ ×¢×™×‘×•×“ {len(files)} ×§×‘×¦×™×? (y/N): ")
            if response.lower() != 'y':
                print("âŒ ×¢×™×‘×•×“ ×‘×•×˜×œ")
                return

        # ×¢×™×‘×•×“ ×§×‘×¦×™×
        start_time = time.time()
        successful_files = 0

        for i, file_key in enumerate(files, 1):
            print(f"\n{'=' * 60}")
            print(f"ğŸ“ ×§×•×‘×¥ {i}/{len(files)}: {os.path.basename(file_key)}")

            success = self.process_single_file(file_key)
            if success:
                successful_files += 1

            # ×”×ª×§×“××•×ª ×›×œ 5 ×§×‘×¦×™×
            if i % 5 == 0 or i == len(files):
                self.print_progress(i, len(files), start_time)

        # ×¡×™×›×•×
        self.print_final_stats()

        print(f"\nğŸ‰ ×¢×™×‘×•×“ ×”×•×©×œ×!")
        print(f"âœ… {successful_files}/{len(files)} ×§×‘×¦×™× ×¢×•×‘×“×• ×‘×”×¦×œ×—×”")


def main():
    """×¤×•× ×§×¦×™×” ×¨××©×™×ª"""
    print("ğŸš€ Gepeta Geektime Processor")
    print("=" * 60)

    try:
        # ×™×¦×™×¨×ª ××¢×‘×“
        processor = GeektimeProcessor(GOOGLE_API_KEY)

        # ×‘×—×™×¨×ª ××¦×‘
        print(f"\nğŸ¯ ××¤×©×¨×•×™×•×ª:")
        print(f"1. ×‘×“×™×§×” ({TEST_LIMIT} ×§×‘×¦×™×)")
        print(f"2. ×™×™×¦×•×¨ (×›×œ ×”×§×‘×¦×™×)")

        choice = input("×‘×—×¨ (1/2): ").strip()

        if choice == "1":
            print(f"\nğŸ§ª ××ª×—×™×œ ×‘×“×™×§×” ×¢× {TEST_LIMIT} ×§×‘×¦×™×...")
            processor.run_processing(test_mode=True)
        elif choice == "2":
            print(f"\nğŸ­ ××ª×—×™×œ ×¢×™×‘×•×“ ×™×™×¦×•×¨...")
            processor.run_processing(test_mode=False)
        else:
            print("âŒ ×‘×—×™×¨×” ×œ× ×ª×§×™× ×”")
            return

    except ValueError as e:
        print(f"âŒ {e}")
        print("ğŸ’¡ ×¢×“×›×Ÿ ××ª GOOGLE_API_KEY ×‘×ª×—×™×œ×ª ×”×§×•×‘×¥ ××• ×‘×§×•×‘×¥ .env")
    except Exception as e:
        print(f"âŒ ×©×’×™××”: {e}")

    print(f"\nâ° ×¡×™×•×: {datetime.now().strftime('%H:%M:%S')}")


if __name__ == "__main__":
    main()