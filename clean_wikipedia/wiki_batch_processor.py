#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Wikipedia Batch Processor
==========================

×¢×™×‘×•×“ ××œ× ×©×œ ×“×××¤ ×•×™×§×™×¤×“×™×” ×¢×‘×¨×™×ª ×¢× ×©××™×¨×ª ×“×•×’×××•×ª ×œ-S3.
××©×ª××© ×‘××•×“×•×œ ×”× ×™×§×•×™ ×”××¨×›×–×™.
"""

import xml.etree.ElementTree as ET
import json
import bz2
import csv
import boto3
import shutil
import random
from pathlib import Path
from tqdm import tqdm
import time
from collections import defaultdict

from wiki_text_cleaner import WikipediaTextCleaner, count_words, count_bytes

WIKI_ARTICLES = 500000 # Estimated total no. of articles, for the progress bar, in case we prcoess them all

class WikipediaBatchProcessor:
    """××—×œ×§×” ×œ×¢×™×‘×•×“ ××œ× ×©×œ ×“×××¤ ×•×™×§×™×¤×“×™×” ×¢× ×©××™×¨×ª ×“×•×’×××•×ª"""

    def __init__(self, dump_path, s3_bucket, s3_prefix_main, s3_prefix_examples,
                 max_articles=10000, max_examples_per_category=100, max_random_samples=100):
        self.dump_path = dump_path
        self.s3_bucket = s3_bucket
        self.s3_prefix_main = s3_prefix_main
        self.s3_prefix_examples = s3_prefix_examples
        self.max_articles = max_articles
        self.max_examples_per_category = max_examples_per_category
        self.max_random_samples = max_random_samples

        # S3 client
        self.s3_client = boto3.client('s3')

        # ×ª×™×§×™×•×ª ×“×•×’×××•×ª (×§×™×™××•×ª ×‘-S3)
        self.example_categories = [
            "cr_and_3_newlines", "CSS", "empty_line_with_bullet", "equations",
            "html_escape_codes", "multiple_hyphens", "multiple_spaces", "PII",
            "start_end_white_space", "tables", "URL", "wiki_citations",
            "wiki_foreign_language_and_image_refs", "wiki_headers", "wiki_markup",
            "wiki_redirecting_articles", "wiki_tags_and_media_descriptions"
        ]

        # ××•× ×™× ×œ×“×•×’×××•×ª
        self.example_counts = defaultdict(int)

        # ××•× ×™× ×›×œ×œ×™×™×
        self.total_processed = 0
        self.total_scanned = 0

        # ×“×’×™××” ××§×¨××™×ª ×›×œ×œ×™×ª
        self.random_sample_indices = set()
        self.random_samples_collected = 0
        self.random_samples_data = []

        # ×™×¦×™×¨×ª ×¨×©×™××” ×©×œ ××™× ×“×§×¡×™× ××§×¨××™×™× ×œ×“×’×™××”
        if self.max_random_samples > 0:
            # ×‘×—×™×¨×ª ××™× ×“×§×¡×™× ××§×¨××™×™× ××ª×•×š ×”×¨×™×¦×”
            self.random_sample_indices = set(random.sample(
                range(min(self.max_articles, 50000)),
                min(self.max_random_samples, self.max_articles)
            ))

        # ×ª×™×§×™×” ××§×•××™×ª ×–×× ×™×ª ×•×¤×ª×™×—×ª ×§×•×‘×¥ ×”×¤×œ×˜ ×”×¨××©×™
        self.temp_dir = Path("temp_output")
        self.temp_dir.mkdir(exist_ok=True)
        self.output_file = open(self.temp_dir / "wikipedia_he_processed.jsonl", 'w', encoding='utf-8')

        # ×™×¦×™×¨×ª ×× ×§×” ×¢× callback ×œ×©××™×¨×ª ×“×•×’×××•×ª
        self.cleaner = WikipediaTextCleaner(example_callback=self._save_example)

    def _save_example(self, category, raw_text, clean_text):
        """callback ×œ×©××™×¨×ª ×“×•×’×××•×ª ×œ×§×˜×’×•×¨×™×” ××¡×•×™××ª"""
        if self.example_counts[category] >= self.max_examples_per_category:
            return

        # ×™×¦×™×¨×ª ×§×•×‘×¥ CSV ×–×× ×™
        temp_file = self.temp_dir / f"{category}_temp.csv"

        # ×‘×“×™×§×” ×× ×”×§×•×‘×¥ ×§×™×™× ×•×™×¦×™×¨×ª headers
        file_exists = temp_file.exists()

        with open(temp_file, 'a', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)

            if not file_exists:
                writer.writerow(['raw_text', 'clean_text'])

            writer.writerow([raw_text, clean_text])

        self.example_counts[category] += 1

        # ×”×¢×œ××” ×œ-S3 ×›×œ 10 ×“×•×’×××•×ª ××• ×‘×¡×•×£
        if (self.example_counts[category] % 10 == 0 or
                self.example_counts[category] == self.max_examples_per_category):
            self._upload_example_file_to_s3(category, temp_file)

    def _save_random_sample(self, raw_wikitext, clean_text, title):
        """×©××™×¨×ª ×“×’×™××” ××§×¨××™×ª ×›×œ×œ×™×ª"""
        if self.random_samples_collected >= self.max_random_samples:
            return

        # ×”×•×¡×¤×ª ×”×“×’×™××” ×œ×¨×©×™××”
        self.random_samples_data.append({
            'raw_text': raw_wikitext,
            'clean_text': clean_text,
            'title': title
        })

        self.random_samples_collected += 1

        # ×©××™×¨×” ×œ×§×•×‘×¥ ×–×× ×™ ×›×œ 10 ×“×’×™××•×ª ××• ×‘×¡×•×£
        if (self.random_samples_collected % 10 == 0 or
                self.random_samples_collected == self.max_random_samples):
            self._write_random_samples_to_file()

    def _write_random_samples_to_file(self):
        """×›×ª×™×‘×ª ×”×“×’×™××•×ª ×”××§×¨××™×•×ª ×œ×§×•×‘×¥ ×–×× ×™"""
        temp_file = self.temp_dir / "random_samples_temp.csv"

        # ×‘×“×™×§×” ×× ×”×§×•×‘×¥ ×§×™×™× ×•×™×¦×™×¨×ª headers
        file_exists = temp_file.exists()

        with open(temp_file, 'a', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)

            if not file_exists:
                writer.writerow(['raw_text', 'clean_text', 'title'])

            # ×›×ª×™×‘×ª ×›×œ ×”×“×’×™××•×ª ×©× ××¡×¤×•
            for sample in self.random_samples_data:
                writer.writerow([sample['raw_text'], sample['clean_text'], sample['title']])

        # × ×™×§×•×™ ×”×¨×©×™××” ××—×¨×™ ×›×ª×™×‘×”
        self.random_samples_data.clear()

    def _upload_example_file_to_s3(self, category, temp_file):
        """×”×¢×œ××ª ×§×•×‘×¥ ×“×•×’×××•×ª ×œ-S3"""
        try:
            s3_key = f"{self.s3_prefix_examples}{category}/{category}_examples.csv"

            self.s3_client.upload_file(
                str(temp_file),
                self.s3_bucket,
                s3_key
            )

            print(f"âœ… ×”×•×¢×œ×• ×“×•×’×××•×ª {category}: {self.example_counts[category]} ×“×•×’×××•×ª")
        except Exception as e:
            print(f"âŒ ×©×’×™××” ×‘×”×¢×œ××ª ×“×•×’×××•×ª {category}: {e}")

    def _upload_random_samples_to_s3(self):
        """×”×¢×œ××ª ×”×“×’×™××•×ª ×”××§×¨××™×•×ª ×œ-S3"""
        try:
            temp_file = self.temp_dir / "random_samples_temp.csv"
            if not temp_file.exists():
                return

            s3_key = f"{self.s3_prefix_examples}random_samples/random_samples.csv"

            self.s3_client.upload_file(
                str(temp_file),
                self.s3_bucket,
                s3_key
            )

            print(f"âœ… ×”×•×¢×œ×• ×“×’×™××•×ª ××§×¨××™×•×ª ×›×œ×œ×™×•×ª: {self.random_samples_collected} ×“×’×™××•×ª")
        except Exception as e:
            print(f"âŒ ×©×’×™××” ×‘×”×¢×œ××ª ×“×’×™××•×ª ××§×¨××™×•×ª: {e}")

    def _upload_main_output_to_s3(self):
        """×”×¢×œ××ª ×”×§×•×‘×¥ ×”×¨××©×™ ×œ-S3"""
        try:
            local_file = self.temp_dir / "wikipedia_he_processed.jsonl"
            s3_key = f"{self.s3_prefix_main}wikipedia_he_processed.jsonl"

            self.s3_client.upload_file(
                str(local_file),
                self.s3_bucket,
                s3_key
            )

            print(f"âœ… ×”×•×¢×œ×” ×§×•×‘×¥ ×¨××©×™: s3://{self.s3_bucket}/{s3_key}")
        except Exception as e:
            print(f"âŒ ×©×’×™××” ×‘×”×¢×œ××ª ×§×•×‘×¥ ×¨××©×™: {e}")

    def _cleanup_temp_files(self):
        """× ×™×§×•×™ ×§×‘×¦×™× ×–×× ×™×™×"""
        try:
            shutil.rmtree(self.temp_dir)
            print("ğŸ—‘ï¸ × ×•×§×• ×§×‘×¦×™× ×–×× ×™×™×")
        except Exception as e:
            print(f"âš ï¸ ×©×’×™××” ×‘× ×™×§×•×™ ×§×‘×¦×™× ×–×× ×™×™×: {e}")

    def _is_valid_article(self, page_elem):
        """×‘×“×™×§×” ×× ×”×“×£ ×ª×§×™×Ÿ ×œ×¢×™×‘×•×“"""
        namespace_elem = page_elem.find('.//{*}ns')
        if namespace_elem is None or namespace_elem.text != '0':
            return False

        revision = page_elem.find('.//{*}revision')
        if revision is None:
            return False

        text_elem = revision.find('.//{*}text')
        if text_elem is None or not text_elem.text:
            return False

        return True

    def process_dump(self):
        """×¢×™×‘×•×“ ×”×“×××¤ ×¢× ×”×’×‘×œ×”"""
        print("ğŸš€ ××ª×—×™×œ ×¢×™×‘×•×“ ×•×™×§×™×¤×“×™×” - ×¢× ×× ×§×” ×××•×—×“")
        print(f"ğŸ“‚ ×§×•×‘×¥ ×§×œ×˜: {self.dump_path}")
        print(f"â˜ï¸ S3 ×¨××©×™: s3://{self.s3_bucket}/{self.s3_prefix_main}")
        print(f"ğŸ“ S3 ×“×•×’×××•×ª: s3://{self.s3_bucket}/{self.s3_prefix_examples}")
        print(f"ğŸ”¢ ××’×‘×œ×”: {self.max_articles} ×¢×¨×›×™×")
        print(f"ğŸ“Š ×“×•×’×××•×ª: ×¢×“ {self.max_examples_per_category} ×œ×›×œ ×§×˜×’×•×¨×™×”")
        print(f"ğŸ² ×“×’×™××•×ª ××§×¨××™×•×ª ×›×œ×œ×™×•×ª: {self.max_random_samples}")
        print("=" * 60)

        start_time = time.time()

        try:
            with bz2.open(self.dump_path, 'rt', encoding='utf-8') as dump_file:
                with tqdm(total=self.max_articles, desc="ğŸ”„ ×¢×™×‘×•×“ ×¢×¨×›×™×", unit="articles") as pbar:

                    for event, elem in ET.iterparse(dump_file, events=('start', 'end')):
                        if event == 'end' and elem.tag.endswith('page'):
                            self.total_scanned += 1

                            # ×‘×“×™×§×” ×× ×”×“×£ ×ª×§×™×Ÿ
                            if self._is_valid_article(elem):
                                # ×—×™×œ×•×¥ ××™×“×¢
                                title_elem = elem.find('.//{*}title')
                                revision = elem.find('.//{*}revision')
                                text_elem = revision.find('.//{*}text')

                                title = title_elem.text if title_elem is not None else ""
                                raw_wikitext = text_elem.text if text_elem is not None else ""

                                # ×¢×™×‘×•×“ ×”×¢×¨×š ×‘×××¦×¢×•×ª ×”×× ×§×” ×”××¨×›×–×™
                                cleaned_text = self.cleaner.clean_article(title, raw_wikitext)

                                if cleaned_text:
                                    # ×‘×“×™×§×” ×× ×¦×¨×™×š ×œ×©××•×¨ ×“×’×™××” ××§×¨××™×ª
                                    if (self.total_processed in self.random_sample_indices and
                                            self.random_samples_collected < self.max_random_samples):
                                        self._save_random_sample(raw_wikitext, cleaned_text, title)

                                    # ×™×¦×™×¨×ª ×¤×¨×™×˜ JSONL
                                    article_item = {
                                        "text": cleaned_text,
                                        "word_count": count_words(cleaned_text),
                                        "byte_count": count_bytes(cleaned_text),
                                        "title": title
                                    }

                                    # ×›×ª×™×‘×” ×œ×§×•×‘×¥ ×”×¤×œ×˜
                                    json.dump(article_item, self.output_file, ensure_ascii=False)
                                    self.output_file.write('\n')

                                    self.total_processed += 1
                                    pbar.update(1)
                                    pbar.set_description(f"ğŸ”„ ×¢×™×‘×•×“: {title[:25]}...")

                                    # ×‘×“×™×§×” ×× ×”×’×¢× ×• ×œ××’×‘×œ×”
                                    if self.total_processed >= self.max_articles:
                                        break

                            elem.clear()

                        # ×‘×“×™×§×” ×× ×”×’×¢× ×• ×œ××’×‘×œ×”
                        if self.total_processed >= self.max_articles:
                            break

        except KeyboardInterrupt:
            print("\nâš ï¸ ×”×¢×™×‘×•×“ ×”×•×¤×¡×§ ×¢×œ ×™×“×™ ×”××©×ª××©")
        except Exception as e:
            print(f"\nâŒ ×©×’×™××” ×‘×¢×™×‘×•×“: {e}")

        finally:
            self.output_file.close()

            # ×•×™×“×•× ×©××™×¨×ª ×›×œ ×”×“×’×™××•×ª ×”××§×¨××™×•×ª ×©× ×•×ª×¨×•
            if self.random_samples_data:
                self._write_random_samples_to_file()

            # ×”×¢×œ××” ×œ-S3 ×©×œ ×”×§×•×‘×¥ ×”×¨××©×™
            self._upload_main_output_to_s3()

            # ×”×¢×œ××” ×©×œ ×”×“×’×™××•×ª ×”××§×¨××™×•×ª
            self._upload_random_samples_to_s3()

            # ×”×¢×œ××” ×¡×•×¤×™×ª ×©×œ ×›×œ ×§×‘×¦×™ ×”×“×•×’×××•×ª ×©× ×•×ª×¨×•
            for category in self.example_categories:
                temp_file = self.temp_dir / f"{category}_temp.csv"
                if temp_file.exists() and self.example_counts[category] > 0:
                    self._upload_example_file_to_s3(category, temp_file)

            # × ×™×§×•×™ ×§×‘×¦×™× ×–×× ×™×™×
            self._cleanup_temp_files()

        # ×¡×™×›×•× ×¡×•×¤×™
        self._print_final_summary(start_time)

    def _print_final_summary(self, start_time):
        """×”×“×¤×¡×ª ×¡×™×›×•× ×¡×•×¤×™"""
        total_time = time.time() - start_time
        cleaning_stats = self.cleaner.get_stats()

        print("\n" + "=" * 60)
        print("ğŸ‰ ×¢×™×‘×•×“ ×”×•×©×œ×!")
        print(f"ğŸ“Š ×¡×˜×˜×™×¡×˜×™×§×•×ª ×¡×•×¤×™×•×ª:")
        print(f"   â±ï¸ ×–××Ÿ ×›×•×œ×œ: {total_time / 60:.1f} ×“×§×•×ª")
        print(f"   ğŸ“– ×“×¤×™× ×©× ×¡×¨×§×•: {self.total_scanned:,}")
        print(f"   âœ… ×¢×¨×›×™× ×©×¢×•×‘×“×•: {self.total_processed:,}")
        print(f"   ğŸ“ˆ ×©×™×¢×•×¨ ×”×¦×œ×—×”: {(self.total_processed / self.total_scanned) * 100:.2f}%")

        print(f"\nğŸ”§ ×¡×˜×˜×™×¡×˜×™×§×•×ª × ×™×§×•×™:")
        for stat_name, count in cleaning_stats.items():
            if count > 0:
                print(f"   ğŸ“Š {stat_name}: {count:,}")

        print(f"\nğŸ“Š ×“×•×’×××•×ª ×©× ×©××¨×•:")
        for category, count in self.example_counts.items():
            if count > 0:
                print(f"   ğŸ“ {category}: {count} ×“×•×’×××•×ª")

        # ×”×•×¡×¤×ª ×¡×˜×˜×™×¡×˜×™×§×” ×¢×œ ×”×“×’×™××•×ª ×”××§×¨××™×•×ª
        print(f"\nğŸ² ×“×’×™××•×ª ××§×¨××™×•×ª ×›×œ×œ×™×•×ª:")
        print(f"   ğŸ“Š × ××¡×¤×•: {self.random_samples_collected} ××ª×•×š {self.max_random_samples}")

        print(f"\nâ˜ï¸ ×”×¤×œ×˜ ×–××™×Ÿ ×‘-S3:")
        print(f"   ğŸ“„ ×§×•×‘×¥ ×¨××©×™: s3://{self.s3_bucket}/{self.s3_prefix_main}")
        print(f"   ğŸ“ ×“×•×’×××•×ª: s3://{self.s3_bucket}/{self.s3_prefix_examples}")
        print(f"   ğŸ² ×“×’×™××•×ª ××§×¨××™×•×ª: s3://{self.s3_bucket}/{self.s3_prefix_examples}random_samples/")


def main():
    """×”×¤×¢×œ×” ×¨××©×™×ª"""
    print("ğŸ¯ ×¢×™×‘×•×“ ×•×™×§×™×¤×“×™×” ×¢×‘×¨×™×ª - ××¢×¨×›×ª ×××•×—×“×ª")
    print("=" * 60)

    # ×”×’×“×¨×•×ª
    dump_path = r'C:\Users\Dan Revital\OneDrive\Documents\gepeta\data\wikipedia\hewiki-latest-pages-articles.xml.bz2'
    s3_bucket = 'gepeta-datasets'
    s3_prefix_main = 'processed_and_cleaned/wikipedia/'
    s3_prefix_examples = 'processed/unified_examples/'
    max_articles = WIKI_ARTICLES  # ×”×’×‘×œ×” ××¤×©×¨×™×ª ×œ×‘×“×™×§×”
    max_examples_per_category = 0
    max_random_samples = 0  # ×“×’×™××•×ª ××§×¨××™×•×ª ×›×œ×œ×™×•×ª

    # ×™×¦×™×¨×ª ××¢×‘×“
    processor = WikipediaBatchProcessor(
        dump_path=dump_path,
        s3_bucket=s3_bucket,
        s3_prefix_main=s3_prefix_main,
        s3_prefix_examples=s3_prefix_examples,
        max_articles=max_articles,
        max_examples_per_category=max_examples_per_category,
        max_random_samples=max_random_samples
    )

    # ×‘×“×™×§×ª ×§×™×©×•×¨×™×ª S3
    try:
        processor.s3_client.head_bucket(Bucket=s3_bucket)
        print(f"âœ… ×§×™×©×•×¨×™×ª S3 ×ª×§×™× ×”: s3://{s3_bucket}")
    except Exception as e:
        print(f"âŒ ×‘×¢×™×” ×‘×§×™×©×•×¨×™×ª S3: {e}")
        return

    # ×”×¤×¢×œ×ª ×”×¢×™×‘×•×“
    print(f"\nğŸš€ ××ª×—×™×œ ×¢×™×‘×•×“ ×¢× ××¢×¨×›×ª × ×™×§×•×™ ×××•×—×“×ª...")
    processor.process_dump()


if __name__ == "__main__":
    main()