#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Wikipedia Article Finder
=========================

×—×™×¤×•×© ×¢×¨×š ×¡×¤×¦×™×¤×™ ×‘×“×××¤ ×•×™×§×™×¤×“×™×”, × ×™×§×•×™, ×•×©××™×¨×” ×œ×§×•×‘×¥.
××©×ª××© ×‘××•×“×•×œ ×”× ×™×§×•×™ ×”××¨×›×–×™.
"""

import xml.etree.ElementTree as ET
import bz2
from pathlib import Path
from typing import Optional

from wiki_text_cleaner import WikipediaTextCleaner, count_words, count_bytes


class WikipediaArticleFinder:
    """××—×œ×§×” ×œ×—×™×¤×•×© ×•× ×™×§×•×™ ×¢×¨×š ×¡×¤×¦×™×¤×™"""

    def __init__(self, dump_path: str, save_examples: bool = False):
        """
        ××ª×—×•×œ ×”×—×™×¤×•×©

        Args:
            dump_path: × ×ª×™×‘ ×œ×§×•×‘×¥ ×”×“×××¤
            save_examples: ×”×× ×œ×©××•×¨ ×“×•×’×××•×ª × ×™×§×•×™ (×‘×¨×™×¨×ª ××—×“×œ: False)
        """
        self.dump_path = dump_path
        self.save_examples = save_examples

        # ×™×¦×™×¨×ª ×× ×§×”
        if save_examples:
            self.cleaner = WikipediaTextCleaner(example_callback=self._save_example_to_file)
            self.examples_saved = {}
        else:
            self.cleaner = WikipediaTextCleaner()

    def _save_example_to_file(self, category: str, raw_text: str, clean_text: str):
        """×©××™×¨×ª ×“×•×’×××•×ª ×œ×§×‘×¦×™× ××§×•××™×™×"""
        if category not in self.examples_saved:
            self.examples_saved[category] = 0

        # ×”×’×‘×œ×” ×œ-10 ×“×•×’×××•×ª ×œ×›×œ ×§×˜×’×•×¨×™×”
        if self.examples_saved[category] >= 10:
            return

        examples_dir = Path("examples")
        examples_dir.mkdir(exist_ok=True)

        example_file = examples_dir / f"{category}_examples.txt"

        with open(example_file, 'a', encoding='utf-8') as f:
            f.write(f"=== ×“×•×’××” {self.examples_saved[category] + 1} ===\n")
            f.write("BEFORE:\n")
            f.write(raw_text[:500] + "...\n" if len(raw_text) > 500 else raw_text + "\n")
            f.write("\nAFTER:\n")
            f.write(clean_text[:500] + "...\n" if len(clean_text) > 500 else clean_text + "\n")
            f.write("\n" + "=" * 50 + "\n\n")

        self.examples_saved[category] += 1
        print(f"ğŸ“ × ×©××¨×” ×“×•×’××” {category} ({self.examples_saved[category]}/10)")

    def find_article_in_dump(self, article_title: str) -> Optional[str]:
        """
        ××•×¦× ×¢×¨×š ×¡×¤×¦×™×¤×™ ×‘×“×××¤ ×•×™×§×™×¤×“×™×

        Args:
            article_title: ×©× ×”×¢×¨×š ×œ×—×™×¤×•×©

        Returns:
            ×•×™×§×™-×˜×§×¡×˜ ×’×•×œ××™ ××• None ×× ×œ× × ××¦×
        """
        print(f"ğŸ” ××—×¤×© ×¢×¨×š: '{article_title}' ×‘×“×××¤...")
        print(f"ğŸ“‚ ×“×××¤: {self.dump_path}")
        print("â³ ×–×” ×¢×œ×•×œ ×œ×§×—×ª ×–××Ÿ...")

        scanned_pages = 0

        try:
            with bz2.open(self.dump_path, 'rt', encoding='utf-8') as file:
                for event, elem in ET.iterparse(file, events=('start', 'end')):
                    if event == 'end' and elem.tag.endswith('page'):
                        scanned_pages += 1

                        # ×”×“×¤×¡×ª ×”×ª×§×“××•×ª ×›×œ 10,000 ×“×¤×™×
                        if scanned_pages % 10000 == 0:
                            print(f"   ×¡×¨×§×ª×™ {scanned_pages:,} ×“×¤×™×...")

                        # ×—×™×œ×•×¥ ×”×›×•×ª×¨×ª
                        title_elem = elem.find('.//{*}title')
                        if title_elem is not None and title_elem.text == article_title:

                            print(f"âœ… × ××¦× ×¢×¨×š: '{article_title}' ××—×¨×™ {scanned_pages:,} ×“×¤×™×!")

                            # ×—×™×œ×•×¥ ×”×˜×§×¡×˜
                            revision = elem.find('.//{*}revision')
                            text_elem = revision.find('.//{*}text') if revision is not None else None

                            if text_elem is not None and text_elem.text:
                                raw_wikitext = text_elem.text
                                print(f"ğŸ“ ××•×¨×š ×•×™×§×™-×˜×§×¡×˜ ×’×•×œ××™: {len(raw_wikitext):,} ×ª×•×•×™×")

                                elem.clear()
                                return raw_wikitext
                            else:
                                print(f"âŒ ×œ× × ××¦× ×ª×•×›×Ÿ ×‘×¢×¨×š")
                                elem.clear()
                                return None

                        elem.clear()

        except Exception as e:
            print(f"âŒ ×©×’×™××” ×‘×—×™×¤×•×©: {e}")
            return None

        print(f"âŒ ×œ× × ××¦× ×¢×¨×š: '{article_title}' (×¡×¨×§×ª×™ {scanned_pages:,} ×“×¤×™×)")
        return None

    def process_article(self, article_title: str, output_dir: str = ".") -> bool:
        """
        ××—×¤×©, ×× ×§×” ×•×©×•××¨ ×¢×¨×š

        Args:
            article_title: ×©× ×”×¢×¨×š
            output_dir: ×ª×™×§×™×™×ª ×¤×œ×˜

        Returns:
            True ×× ×”×¦×œ×™×—, False ××—×¨×ª
        """
        print(f"ğŸ¯ ××¢×‘×“ ×¢×¨×š: '{article_title}'")
        print("=" * 60)

        # ×©×œ×‘ 1: ×—×™×¤×•×© ×”×¢×¨×š
        raw_wikitext = self.find_article_in_dump(article_title)

        if not raw_wikitext:
            print(f"âŒ ×œ× × ××¦× ×”×¢×¨×š '{article_title}'")
            return False

        print()

        # ×©×œ×‘ 2: × ×™×§×•×™ ×”×¢×¨×š
        print("âš™ï¸ ×× ×§×” ××ª ×”×•×™×§×™-×˜×§×¡×˜...")

        cleaned_text = self.cleaner.clean_article(article_title, raw_wikitext)

        if not cleaned_text:
            print("âŒ ×”×¢×¨×š × ×¤×¡×œ ×‘×¢×™×‘×•×“ (×™×™×ª×›×Ÿ ×©×”×•× ×”×¤× ×™×” ××• ×§×¦×¨ ××“×™)")
            return False

        print(f"âœ… × ×™×§×•×™ ×”×•×©×œ× ×‘×”×¦×œ×—×”")
        print(f"ğŸ“„ ××•×¨×š ××—×¨×™ ×¢×™×‘×•×“: {len(cleaned_text):,} ×ª×•×•×™×")

        # ×©×œ×‘ 3: ×©××™×¨×ª ×”×ª×•×¦××•×ª
        print("=" * 60)
        print("ğŸ’¾ ×©×•××¨ ×ª×•×¦××•×ª...")

        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        # ×©××™×¨×ª ×”×˜×§×¡×˜ ×”×’×•×œ××™
        safe_title = article_title.replace(' ', '_').replace('/', '_')
        raw_filename = output_path / f"{safe_title}_raw_wikitext.txt"
        with open(raw_filename, 'w', encoding='utf-8') as f:
            f.write(raw_wikitext)
        print(f"âœ… ×•×™×§×™-×˜×§×¡×˜ ×’×•×œ××™ × ×©××¨: {raw_filename}")

        # ×©××™×¨×ª ×”×˜×§×¡×˜ ×”××¢×•×‘×“
        processed_filename = output_path / f"{safe_title}_cleaned.txt"
        with open(processed_filename, 'w', encoding='utf-8') as f:
            f.write(cleaned_text)
        print(f"âœ… ×˜×§×¡×˜ ××¢×•×‘×“ × ×©××¨: {processed_filename}")

        # ×©××™×¨×ª ××˜××“×˜×”
        metadata_filename = output_path / f"{safe_title}_metadata.txt"
        with open(metadata_filename, 'w', encoding='utf-8') as f:
            f.write(f"×›×•×ª×¨×ª: {article_title}\n")
            f.write(f"××•×¨×š ×’×•×œ××™: {len(raw_wikitext):,} ×ª×•×•×™×\n")
            f.write(f"××•×¨×š × ×§×™: {len(cleaned_text):,} ×ª×•×•×™×\n")
            f.write(f"××™×œ×™×: {count_words(cleaned_text):,}\n")
            f.write(f"×‘×™×™×˜×™×: {count_bytes(cleaned_text):,}\n")
            f.write(f"×“×—×™×¡×”: {(1 - len(cleaned_text) / len(raw_wikitext)) * 100:.1f}%\n")

            # ×¡×˜×˜×™×¡×˜×™×§×•×ª × ×™×§×•×™
            cleaning_stats = self.cleaner.get_stats()
            if cleaning_stats:
                f.write(f"\n×¡×˜×˜×™×¡×˜×™×§×•×ª × ×™×§×•×™:\n")
                for stat_name, count in cleaning_stats.items():
                    if count > 0:
                        f.write(f"  {stat_name}: {count}\n")

        print(f"âœ… ××˜××“×˜×” × ×©××¨×”: {metadata_filename}")

        # ×¡×™×›×•×
        print("\n" + "=" * 60)
        print("ğŸ‰ ×”×•×©×œ× ×‘×”×¦×œ×—×”!")
        print(f"ğŸ“Š ×¡×™×›×•×:")
        print(f"   ×•×™×§×™-×˜×§×¡×˜ ×’×•×œ××™: {len(raw_wikitext):,} ×ª×•×•×™×")
        print(f"   ×˜×§×¡×˜ ××¢×•×‘×“: {len(cleaned_text):,} ×ª×•×•×™×")
        print(f"   ××™×œ×™×: {count_words(cleaned_text):,}")
        print(f"   ×“×—×™×¡×”: {(1 - len(cleaned_text) / len(raw_wikitext)) * 100:.1f}%")

        if self.save_examples and hasattr(self, 'examples_saved'):
            total_examples = sum(self.examples_saved.values())
            if total_examples > 0:
                print(f"   ×“×•×’×××•×ª ×©× ×©××¨×•: {total_examples}")

        print(f"\nğŸ“„ ×“×•×’××” ××”×˜×§×¡×˜ ×”××¢×•×‘×“ (200 ×ª×•×•×™× ×¨××©×•× ×™×):")
        print(f"   {cleaned_text[:200]}...")

        return True


def main():
    """×”×¤×¢×œ×” ×¨××©×™×ª"""
    print("ğŸ¯ ×—×™×¤×•×© ×•× ×™×§×•×™ ×¢×¨×š ×•×™×§×™×¤×“×™×” - ××¢×¨×›×ª ×××•×—×“×ª")
    print("=" * 60)

    # ×”×’×“×¨×•×ª
    dump_path = r'C:\Users\Dan Revital\OneDrive\Documents\gepeta\data\wikipedia\hewiki-latest-pages-articles.xml.bz2'
    article_title = '×§×•×“ ×¤×ª×•×—'
    output_dir = "article_output"
    save_examples = True  # ×©××™×¨×ª ×“×•×’×××•×ª × ×™×§×•×™

    print(f"ğŸ” ××—×¤×© ×¢×¨×š: '{article_title}'")
    print(f"ğŸ“‚ ×‘×“×××¤: {dump_path}")
    print(f"ğŸ“ ×¤×œ×˜: {output_dir}")
    print(f"ğŸ“ ×©××™×¨×ª ×“×•×’×××•×ª: {'×›×Ÿ' if save_examples else '×œ×'}")
    print()

    # ×™×¦×™×¨×ª ×—×™×¤×•×©
    finder = WikipediaArticleFinder(dump_path, save_examples=save_examples)

    # ×¢×™×‘×•×“ ×”×¢×¨×š
    success = finder.process_article(article_title, output_dir)

    if not success:
        print("âŒ ×”×¢×™×‘×•×“ × ×›×©×œ")
        return

    print("\nğŸ‰ ×”×ª×”×œ×™×š ×”×•×©×œ× ×‘×”×¦×œ×—×”!")


if __name__ == "__main__":
    main()