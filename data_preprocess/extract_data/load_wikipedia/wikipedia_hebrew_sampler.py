import xml.etree.ElementTree as ET
import mwparserfromhell
import json
import re
import random
from pathlib import Path
import bz2
from tqdm import tqdm


class WikipediaSampleProcessor:
    def __init__(self, dump_path, output_file, sample_size=100):
        self.dump_path = dump_path
        self.output_file = output_file
        self.sample_size = sample_size
        self.articles = []
        self.valid_pages = []

    def collect_valid_pages(self, max_scan=50000):
        """××•×¡×£ ×“×¤×™× ×ª×§×™× ×™× ×œ×“×’×™××” ×¢× ×¤×¨×•×’×¨×¡ ×‘××¨"""
        print(f"ğŸ” Scanning dump for valid pages (max {max_scan})...")

        with tqdm(
                total=max_scan,
                desc="ğŸ“– Scanning pages",
                unit="pages",
                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
        ) as pbar:

            with bz2.open(self.dump_path, 'rt', encoding='utf-8') as file:
                scanned = 0
                for event, elem in ET.iterparse(file, events=('start', 'end')):
                    if event == 'end' and elem.tag.endswith('page'):
                        if self.is_valid_page(elem):
                            page_data = self.extract_basic_info(elem)
                            if page_data:
                                self.valid_pages.append(page_data)

                        elem.clear()
                        scanned += 1

                        # ×¢×“×›×•×Ÿ ×¤×¨×•×’×¨×¡ ×‘××¨
                        pbar.update(1)
                        valid_ratio = len(self.valid_pages) / scanned * 100 if scanned > 0 else 0
                        pbar.set_postfix_str(f"Valid: {len(self.valid_pages)} ({valid_ratio:.1f}%)")

                        # ×”×¤×¡×§ ××—×¨×™ ×¡×¨×™×§×” ××¡×¤×§×ª ××• ×× ××¦×× ×• ××¡×¤×™×§
                        if scanned >= max_scan or len(self.valid_pages) >= self.sample_size * 5:
                            break

        print(f"âœ… Found {len(self.valid_pages)} valid pages out of {scanned} scanned")

    def is_valid_page(self, page_elem):
        """×‘×“×™×§×” ××”×™×¨×” ×× ×”×“×£ ×ª×§×™×Ÿ"""
        # ×¨×§ namespace 0 (×¢×¨×›×™× ×¨××©×™×™×)
        namespace = self.get_text(page_elem, 'ns')
        if namespace != '0':
            return False

        # ×‘×“×™×§×ª ×ª×•×›×Ÿ ×‘×¡×™×¡×™×ª
        revision = page_elem.find('.//{*}revision')
        if revision is None:
            return False

        text = self.get_text(revision, 'text')
        if not text or len(text) < 500:
            return False

        if text.strip().startswith('#REDIRECT'):
            return False

        return True

    def extract_basic_info(self, page_elem):
        """×—×™×œ×•×¥ ××™×“×¢ ×‘×¡×™×¡×™ ×œ×“×’×™××”"""
        title = self.get_text(page_elem, 'title')
        page_id = self.get_text(page_elem, 'id')

        revision = page_elem.find('.//{*}revision')
        text = self.get_text(revision, 'text')

        return {
            'id': page_id,
            'title': title,
            'text': text,
            'length': len(text)
        }

    def get_text(self, element, tag_name):
        """×—×™×œ×•×¥ ×˜×§×¡×˜ ×××œ×× ×˜ XML"""
        elem = element.find(f'.//{{{element.tag.split("}")[0][1:]}}}{tag_name}')
        return elem.text if elem is not None and elem.text else ""

    def create_sample(self):
        """×™×¦×™×¨×ª ×“×’×™××” ××§×¨××™×ª ×¢× ×¤×¨×•×’×¨×¡ ×‘××¨"""
        if len(self.valid_pages) < self.sample_size:
            print(f"âš ï¸ Warning: Only {len(self.valid_pages)} valid pages found, using all of them")
            sample = self.valid_pages
        else:
            sample = random.sample(self.valid_pages, self.sample_size)

        print(f"âš™ï¸ Processing {len(sample)} articles...")

        # ×¤×¨×•×’×¨×¡ ×‘××¨ ×¢× ××™×“×¢ ××¤×•×¨×˜
        successful = 0
        failed = 0

        for page_data in tqdm(sample, desc="ğŸ“„ Processing articles", unit="articles"):
            try:
                processed = self.process_article(page_data)
                if processed:
                    self.articles.append(processed)
                    successful += 1
                else:
                    failed += 1

            except Exception as e:
                failed += 1
                tqdm.write(f"âŒ Error processing '{page_data['title'][:30]}...': {str(e)[:50]}")
                continue

        print(f"âœ… Successfully processed: {successful}")
        print(f"âŒ Failed: {failed}")
        print(f"ğŸ“Š Success rate: {successful / (successful + failed) * 100:.1f}%")

    def process_article(self, page_data):
        """×¢×™×‘×•×“ ×¢×¨×š ×™×—×™×“"""
        try:
            # ×¢×™×‘×•×“ ×ª×•×›×Ÿ ×•×™×§×™
            processed = self.process_wikitext(page_data['text'])

            if not processed['content'] or len(processed['content']) < 200:
                return None

            return {
                'id': page_data['id'],
                'title': page_data['title'],
                'content': processed['content'],
                'summary': processed['summary'],
                'categories': processed['categories'],
                'infobox': processed['infobox'],
                'tables': processed['tables'],
                'original_length': page_data['length'],
                'processed_length': len(processed['content']),
                'quality_score': self.calculate_quality(processed)
            }

        except Exception as e:
            print(f"Error in process_article: {e}")
            return None

    def process_wikitext(self, wikitext):
        """×¢×™×‘×•×“ ×˜×§×¡×˜ ×•×™×§×™ ×œ××‘× ×” × ×§×™"""
        try:
            wikicode = mwparserfromhell.parse(wikitext)
        except Exception as e:
            print(f"Error parsing wikitext: {e}")
            return {
                'content': self.normalize_text_for_training(wikitext[:1000]),
                'summary': '',
                'categories': [],
                'infobox': '',
                'tables': []
            }

        # ×—×™×œ×•×¥ ×¨×›×™×‘×™× ×©×•× ×™×
        categories = self.extract_categories(wikicode)
        infobox = self.extract_infobox(wikicode)
        tables = self.extract_tables(wikicode)

        # × ×™×§×•×™ ×”×˜×§×¡×˜ ×”×¨××©×™
        content = self.clean_content(wikicode)
        summary = self.extract_summary(content)

        return {
            'content': content,
            'summary': summary,
            'categories': categories,
            'infobox': infobox,
            'tables': tables
        }

    def normalize_text_for_training(self, text):
        """
        ×× ×¨××œ ×˜×§×¡×˜ ×œ××™××•×Ÿ ××•×“×œ ×©×¤×”:
        - ××—×§ ×›×œ ×©×•×¨×•×ª ×—×“×©×•×ª
        - ××—×§ ×˜××‘×™×
        - ×¨×•×•×—×™× ××¨×•×‘×™× â†’ ×¨×•×•×— ×™×—×™×“
        - ×©××•×¨ ×¨×§ ××ª ×”×ª×•×›×Ÿ ×”×××™×ª×™
        """
        if not text or not isinstance(text, str):
            return text

        # ××—×§ ×©×•×¨×•×ª ×—×“×©×•×ª ×•-\r
        text = text.replace('\n', ' ')
        text = text.replace('\r', ' ')
        text = text.replace('\t', ' ')

        # ××—×§ ×›×œ ×ª×•×•×™ ×‘×¨×™×—×” ××—×¨×™×
        text = text.replace('\\n', ' ')
        text = text.replace('\\t', ' ')
        text = text.replace('\\r', ' ')
        text = text.replace('\\"', '"')
        text = text.replace("\\'", "'")
        text = text.replace('\\\\', '\\')

        # ×¨×•×•×—×™× ××¨×•×‘×™× â†’ ×¨×•×•×— ×™×—×™×“
        text = re.sub(r'\s+', ' ', text)

        # × ×§×” ×¨×•×•×—×™× ×‘×”×ª×—×œ×”/×¡×•×£
        text = text.strip()

        return text

    def clean_content(self, wikicode):
        """× ×™×§×•×™ ×ª×•×›×Ÿ - ×’×¨×¡×” ×× ×•×¨××œ×ª ×œ××™××•×Ÿ"""
        try:
            # ×”×¡×¨×ª ×ª×‘× ×™×•×ª
            templates_to_remove = []
            for template in wikicode.filter_templates():
                template_name = str(template.name).strip().lower()
                # ×ª×‘× ×™×•×ª ×œ×”×¡×¨×” ××œ××”
                remove_patterns = [
                    'cite', '×¦-', '×”×¢×¨×”', '××§×•×¨', 'reflist', '××§×•×¨×•×ª',
                    '×¦×™×•×Ÿ', 'ref', 'citation', 'web', 'news', 'book', 'journal'
                ]
                if any(pattern in template_name for pattern in remove_patterns):
                    templates_to_remove.append(template)

            # ×”×¡×¨×ª ×”×ª×‘× ×™×•×ª
            for template in templates_to_remove:
                try:
                    wikicode.remove(template)
                except:
                    pass

            # ×”××¨×ª ×§×™×©×•×¨×™× ×¤× ×™××™×™× ×œ×˜×§×¡×˜
            for link in wikicode.filter_wikilinks():
                try:
                    if link.text:
                        wikicode.replace(link, str(link.text))
                    else:
                        title = str(link.title)
                        if '|' in title:
                            title = title.split('|')[0]
                        wikicode.replace(link, title)
                except:
                    pass

            # ×”×¡×¨×ª ×ª×’×™×•×ª
            for tag in wikicode.filter_tags():
                try:
                    if tag.tag.lower() in ['math', 'chem']:
                        wikicode.replace(tag, f"[× ×•×¡×—×”: {tag.contents}]")
                    elif tag.tag.lower() in ['ref', 'references']:
                        wikicode.remove(tag)
                except:
                    pass

            # ×”××¨×ª ×”×›×œ ×œ×˜×§×¡×˜
            content = str(wikicode.strip_code())

            # × ×™×§×•×™ regex
            content = self.apply_regex_cleaning(content)

            # ×–×™×”×•×™ ×›×•×ª×¨×•×ª
            content = self.identify_headers(content)

            # × ×¨××•×œ ×”×˜×§×¡×˜ ×œ××™××•×Ÿ - ×–×” ×”×—×œ×§ ×”×—×“×©!
            content = self.normalize_text_for_training(content)

            return content

        except Exception as e:
            print(f"Error in clean_content: {e}")
            # ×‘××§×¨×” ×©×œ ×©×’×™××”
            basic_clean = str(wikicode)[:2000]
            return self.normalize_text_for_training(basic_clean)

    def apply_regex_cleaning(self, content):
        """× ×™×§×•×™ regex ××¨×•×›×–"""
        # ×”×¡×¨×ª ×ª×‘× ×™×•×ª ×©× ×©××¨×•
        content = re.sub(r'\{\{[^}]*\}\}', '', content)
        # ×”×¡×¨×ª ×§×™×©×•×¨×™× ×©× ×©××¨×•
        content = re.sub(r'\[\[[^]]*\]\]', '', content)
        # ×”×¡×¨×ª ×ª×’×™×•×ª HTML
        content = re.sub(r'<[^>]*>', '', content)

        # ×”×¡×¨×ª ×ª×™××•×¨×™ ×ª××•× ×•×ª ×•××“×™×”
        content = re.sub(r'^(×©×××œ|×™××™×Ÿ|××¨×›×–|×××•×–×¢×¨)\|.*$', '', content, flags=re.MULTILINE)
        content = re.sub(r'^\s*(×©×××œ|×™××™×Ÿ|××¨×›×–|×××•×–×¢×¨)\|.*$', '', content, flags=re.MULTILINE)

        # ×”×¡×¨×ª ×”×¡×‘×¨×™ ×©×¤×•×ª ×–×¨×•×ª ×‘×¡×•×’×¨×™×™×
        foreign_languages = ['×‘×’×¨×× ×™×ª', '×‘×”×•× ×’×¨×™×ª', '×‘×¢×¨×‘×™×ª', '×‘×›×•×¨×“×™×ª', '×‘×× ×’×œ×™×ª',
                             '×‘×¦×¨×¤×ª×™×ª', '×‘××™×˜×œ×§×™×ª', '×‘×¨×•×¡×™×ª', '×‘×™×•×•× ×™×ª', '×‘×œ×˜×™× ×™×ª']
        for lang in foreign_languages:
            pattern = r'\(' + lang + r':.*?\)'
            content = re.sub(pattern, '', content)

        # ×”×¡×¨×ª ×”×¤× ×™×•×ª ×œ×ª××•× ×•×ª
        content = re.sub(r'×¨××• [A-Za-z\s,]+\.', '', content)

        # × ×™×§×•×™ ×›×œ×œ×™ - × ×©××™×¨ ××ª ×–×” ×›×™ ×”× ×¨××•×œ ×™×˜×¤×œ ×‘×”××©×š
        content = re.sub(r'\n{3,}', '\n\n', content)  # ×©×•×¨×•×ª ×¨×™×§×•×ª ××¨×•×‘×•×ª
        content = re.sub(r' {2,}', ' ', content)  # ×¨×•×•×—×™× ××¨×•×‘×™×
        content = re.sub(r'^\s*=+.*?=+\s*$', '', content, flags=re.MULTILINE)  # ×›×•×ª×¨×•×ª ×•×™×§×™

        return content

    def identify_headers(self, content):
        """×–×™×”×•×™ ×•×¡×™××•×Ÿ ×›×•×ª×¨×•×ª"""
        lines = content.split('\n')
        processed_lines = []

        for i, line in enumerate(lines):
            line = line.strip()

            # ×–×™×”×•×™ ×›×•×ª×¨×ª ×¤×©×•×˜
            if (line and len(line) < 100 and
                    i < len(lines) - 1 and
                    len(lines[i + 1].strip()) > 50 and
                    line.count('.') <= 1 and
                    line.count(',') <= 2):

                # ×‘×“×™×§×•×ª × ×•×¡×¤×•×ª ×œ×•×•×“× ×©×–×• ×›×•×ª×¨×ª
                header_keywords = ['×”×™×¡×˜×•×¨×™×”', '×‘×™×•×’×¨×¤×™×”', '×¨×§×¢', '×ª×•×œ×“×•×ª', '××•×¦×', '×ª×¨×‘×•×ª', '××©×¤×—×ª×•', '×™×œ×“×•×ª×•',
                                   '× ×¢×•×¨×™×•', '×”×ª×¤×ª×—×•×ª', '××©×™××•×ª']
                if (not line.endswith('.') or
                        any(keyword in line for keyword in header_keywords)):
                    processed_lines.append(f"## {line}")
                else:
                    processed_lines.append(line)
            else:
                processed_lines.append(line)

        return '\n'.join(processed_lines)

    def extract_summary(self, content):
        """×—×™×œ×•×¥ ×¡×™×›×•× - ×¢×›×©×™×• ×’× ×× ×•×¨××œ"""
        # ×”×¡×™×›×•× ×™×™×¦×•×¨ ××”×ª×•×›×Ÿ ×©×›×‘×¨ × ×•×§×” ××‘×œ ×¢×•×“ ×œ× × ×•×¨××œ
        # ××– × ×—×œ×¥ ×¡×™×›×•× ×¨×’×™×œ ×•××– × × ×¨××œ ××•×ª×•
        paragraphs = content.split('\n\n')
        summary_text = ""

        for paragraph in paragraphs:
            if len(paragraph.strip()) > 100:
                summary_text = paragraph.strip()[:500]
                break

        # × ×¨××•×œ ×”×¡×™×›×•×
        return self.normalize_text_for_training(summary_text)

    def extract_categories(self, wikicode):
        """×—×™×œ×•×¥ ×§×˜×’×•×¨×™×•×ª"""
        categories = []
        try:
            for link in wikicode.filter_wikilinks():
                title = str(link.title)
                if title.startswith('×§×˜×’×•×¨×™×”:'):
                    category = title.replace('×§×˜×’×•×¨×™×”:', '').strip()
                    if category:
                        categories.append(category)
        except Exception as e:
            print(f"Error extracting categories: {e}")
        return categories

    def extract_infobox(self, wikicode):
        """×—×™×œ×•×¥ ××™× ×¤×•×‘×•×§×¡ - ×’× ×× ×•×¨××œ"""
        try:
            for template in wikicode.filter_templates():
                template_name = str(template.name).lower().strip()

                # ×“×¤×•×¡×™× ×œ×–×™×”×•×™ ××™× ×¤×•×‘×•×§×¡
                infobox_patterns = [
                    '××™× ×¤×•', 'infobox', '×ª×™×‘×ª ××™×“×¢', '××™×“×¢', '×ª×™×‘×”',
                    '×× ×”×™×’', '××“×', '×¢×™×¨', '××“×™× ×”', '×¡×¤×¨', '×¡×¨×˜',
                    '×—×‘×¨×”', '××¨×’×•×Ÿ', '××•× ×™×‘×¨×¡×™×˜×”', '×‘× ×™×™×Ÿ'
                ]

                if any(pattern in template_name for pattern in infobox_patterns):
                    # ×œ× ×ª×‘× ×™×•×ª × ×™×•×•×˜
                    exclude_patterns = ['× ×™×•×•×˜', 'nav', 'citation', 'cite', 'ref', '×”×¢×¨×”']
                    if any(exclude in template_name for exclude in exclude_patterns):
                        continue

                    infobox_text = f"×ª×™×‘×ª ××™×“×¢ - {template.name}: "  # ×©×™× ×•×™: ×¨×•×•×— ×‘××§×•× \n
                    param_count = 0

                    for param in template.params:
                        try:
                            param_name = str(param.name).strip()
                            param_value = str(param.value).strip()

                            if (param_name and param_value and
                                    len(param_value) < 300 and
                                    len(param_value) > 2 and
                                    not param_name.isdigit()):

                                # × ×™×§×•×™ ×‘×¡×™×¡×™ ×©×œ ×”×¢×¨×š
                                param_value = re.sub(r'\[\[([^]|]*)\|?[^]]*\]\]', r'\1', param_value)
                                param_value = re.sub(r'\{\{[^}]*\}\}', '', param_value)
                                param_value = re.sub(r'<[^>]*>', '', param_value)
                                param_value = param_value.strip()

                                if param_value:
                                    infobox_text += f"{param_name}: {param_value} "  # ×¨×•×•×— ×‘××§×•× \n
                                    param_count += 1

                                    if param_count >= 15:
                                        break

                        except Exception:
                            continue

                    if param_count > 0:
                        # × ×¨××•×œ ×”××™× ×¤×•×‘×•×§×¡
                        return self.normalize_text_for_training(infobox_text)

        except Exception as e:
            print(f"Error extracting infobox: {e}")

        return ""

    def extract_tables(self, wikicode):
        """×—×™×œ×•×¥ ×˜×‘×œ××•×ª (×¤×©×•×˜)"""
        tables = []
        try:
            content = str(wikicode)
            table_matches = re.findall(r'\{\|.*?\|\}', content, re.DOTALL)
            for i, table in enumerate(table_matches[:3]):
                if len(table) < 1000:
                    tables.append(f"×˜×‘×œ×” {i + 1}: [××™×“×¢ ××˜×‘×œ×”]")
        except Exception as e:
            print(f"Error extracting tables: {e}")
        return tables

    def calculate_quality(self, processed):
        """×—×™×©×•×‘ ×¦×™×•×Ÿ ××™×›×•×ª ×¤×©×•×˜"""
        score = 30

        # ××•×¨×š ×˜×§×¡×˜ ××¢×•×‘×“
        content_length = len(processed['content'])
        if content_length > 3000:
            score += 30
        elif content_length > 1500:
            score += 20
        elif content_length > 500:
            score += 10

        # ×§×™×•× ×§×˜×’×•×¨×™×•×ª
        if len(processed['categories']) > 0:
            score += 15
        if len(processed['categories']) > 2:
            score += 5

        # ×§×™×•× ××™× ×¤×•×‘×•×§×¡
        if processed['infobox']:
            score += 15

        # ××™×›×•×ª ×¡×™×›×•×
        if len(processed['summary']) > 100:
            score += 5

        return min(100, score)

    def analyze_sample_quality(self):
        """× ×™×ª×•×— ××™×›×•×ª ×”××“×’×"""
        if not self.articles:
            return

        print("\nğŸ“Š Sample Quality Analysis:")
        print("=" * 40)

        # ×¡×˜×˜×™×¡×˜×™×§×•×ª ×‘×¡×™×¡×™×•×ª
        lengths = [a['processed_length'] for a in self.articles]
        qualities = [a['quality_score'] for a in self.articles]

        print(f"ğŸ“ Articles: {len(self.articles)}")
        print(f"ğŸ“ Avg length: {sum(lengths) / len(lengths):.0f} chars")
        print(f"â­ Avg quality: {sum(qualities) / len(qualities):.1f}")

        # ×”×ª×¤×œ×’×•×ª ××™×›×•×ª
        high_quality = len([q for q in qualities if q >= 80])
        med_quality = len([q for q in qualities if 60 <= q < 80])
        low_quality = len([q for q in qualities if q < 60])

        print(f"ğŸŸ¢ High quality (80+): {high_quality}")
        print(f"ğŸŸ¡ Medium quality (60-79): {med_quality}")
        print(f"ğŸ”´ Low quality (<60): {low_quality}")

        # ×‘×“×™×§×ª ×ª×•×›×Ÿ
        with_infobox = len([a for a in self.articles if a.get('infobox')])
        with_categories = len([a for a in self.articles if a.get('categories')])

        print(f"ğŸ“‹ With infobox: {with_infobox}")
        print(f"ğŸ·ï¸ With categories: {with_categories}")

        # ×“×•×’××” ×× ×•×¨××œ×ª
        if self.articles:
            first_article = self.articles[0]
            print(f"\nğŸ“„ ×“×•×’××” ×× ×•×¨××œ×ª (100 ×ª×•×•×™× ×¨××©×•× ×™×):")
            print(f"'{first_article['content'][:100]}...'")

    def save_sample(self):
        """×©××™×¨×ª ×”×“×’×™××”"""
        if not self.articles:
            print("âŒ No articles to save!")
            return

        # ×™×¦×™×¨×ª ××˜×-×“××˜×”
        lengths = [a['processed_length'] for a in self.articles]
        qualities = [a['quality_score'] for a in self.articles]

        metadata = {
            'total_articles': len(self.articles),
            'average_length': sum(lengths) / len(lengths),
            'average_quality': sum(qualities) / len(qualities),
            'total_categories': sum(len(a['categories']) for a in self.articles),
            'articles_with_infobox': sum(1 for a in self.articles if a['infobox']),
            'articles_with_categories': sum(1 for a in self.articles if a['categories']),
            'quality_distribution': {
                'high_80_plus': len([q for q in qualities if q >= 80]),
                'medium_60_79': len([q for q in qualities if 60 <= q < 80]),
                'low_below_60': len([q for q in qualities if q < 60])
            },
            'text_format': 'normalized_for_training'  # ××™×“×¢ ×¢×œ ×”×¤×•×¨××˜
        }

        output_data = {
            'metadata': metadata,
            'articles': self.articles
        }

        with open(self.output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ Sample saved to {self.output_file}")
        print(f"ğŸ“ Total articles: {metadata['total_articles']}")
        print(f"ğŸ“ Average length: {metadata['average_length']:.0f} chars")
        print(f"â­ Average quality: {metadata['average_quality']:.1f}")
        print(f"ğŸ“‹ Articles with infobox: {metadata['articles_with_infobox']}")
        print(f"ğŸ·ï¸ Articles with categories: {metadata['articles_with_categories']}")
        print(f"ğŸ”„ Text format: Normalized for training (no newlines)")

    def run_sample(self):
        """×”×¨×¦×ª ×”××“×’× ×”××œ×"""
        print("ğŸš€ Starting Wikipedia Hebrew sample processing...")
        print("ğŸ”„ Using normalized text format (no newlines) for training")
        print("=" * 60)

        # ×©×œ×‘ 1: ××•×¡×£ ×“×¤×™× ×ª×§×™× ×™×
        print("ğŸ” Step 1: Collecting valid pages...")
        self.collect_valid_pages()

        if len(self.valid_pages) == 0:
            print("âŒ No valid pages found!")
            return

        print(f"âœ… Found {len(self.valid_pages)} valid pages")
        print()

        # ×©×œ×‘ 2: ×™×•×¦×¨ ×“×’×™××” ×•××¢×‘×“
        print("âš™ï¸ Step 2: Creating sample and processing...")
        self.create_sample()

        if not self.articles:
            print("âŒ No articles were successfully processed!")
            return

        print()

        # ×©×œ×‘ 3: × ×™×ª×•×— ××™×›×•×ª
        print("ğŸ“Š Step 3: Analyzing quality...")
        self.analyze_sample_quality()
        print()

        # ×©×œ×‘ 4: ×©×•××¨ ×ª×•×¦××•×ª
        print("ğŸ’¾ Step 4: Saving results...")
        self.save_sample()

        print("\nğŸ‰ Sample processing completed successfully!")
        print("ğŸ“‹ Text is normalized for training - no newlines or escape characters")


# ×©×™××•×©
if __name__ == "__main__":
    processor = WikipediaSampleProcessor(
        dump_path=r'C:\Users\Dan Revital\OneDrive\Documents\gepeta\data\wikipedia\hewiki-latest-pages-articles.xml.bz2',
        output_file=r'C:\Users\Dan Revital\OneDrive\Documents\gepeta\load_wikipedia\hebrew_wiki_sample_100_normalized.json',
        sample_size=100
    )

    processor.run_sample()