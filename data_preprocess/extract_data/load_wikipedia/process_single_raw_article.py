#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import xml.etree.ElementTree as ET
import mwparserfromhell
import re
import bz2
import ipaddress
from pathlib import Path


def find_article_in_dump(dump_path, article_title):
    """
    ××•×¦× ×¢×¨×š ×¡×¤×¦×™×¤×™ ×‘×“×××¤ ×•×™×§×™×¤×“×™×
    """
    print(f"ğŸ” ××—×¤×© ×¢×¨×š: '{article_title}' ×‘×“×××¤...")
    print(f"ğŸ“‚ ×“×××¤: {dump_path}")
    print("â³ ×–×” ×¢×œ×•×œ ×œ×§×—×ª ×–××Ÿ...")

    scanned_pages = 0

    with bz2.open(dump_path, 'rt', encoding='utf-8') as file:
        for event, elem in ET.iterparse(file, events=('start', 'end')):
            if event == 'end' and elem.tag.endswith('page'):
                scanned_pages += 1

                # ×”×“×¤×¡×ª ×”×ª×§×“××•×ª ×›×œ 10,000 ×“×¤×™×
                if scanned_pages % 10000 == 0:
                    print(f"   ×¡×¨×§×ª×™ {scanned_pages:,} ×“×¤×™×...")

                # ×—×™×œ×•×¥ ×”×›×•×ª×¨×ª
                title_elem = elem.find('.//{*}title')
                if title_elem is not None and title_elem.text == article_title:

                    print(f"âœ… × ××¦× ×¢×¨×š: '{article_title}' ××—×¨×™ {scanned_pages:,} ×“×¤×™×!")

                    # ×—×™×œ×•×¥ ×”×˜×§×¡×˜
                    revision = elem.find('.//{*}revision')
                    text_elem = revision.find('.//{*}text') if revision is not None else None

                    if text_elem is not None and text_elem.text:
                        raw_wikitext = text_elem.text
                        print(f"ğŸ“ ××•×¨×š ×•×™×§×™-×˜×§×¡×˜ ×’×•×œ××™: {len(raw_wikitext):,} ×ª×•×•×™×")

                        elem.clear()
                        return raw_wikitext
                    else:
                        print(f"âŒ ×œ× × ××¦× ×ª×•×›×Ÿ ×‘×¢×¨×š")
                        elem.clear()
                        return None

                elem.clear()

    print(f"âŒ ×œ× × ××¦× ×¢×¨×š: '{article_title}' (×¡×¨×§×ª×™ {scanned_pages:,} ×“×¤×™×)")
    return None


class SingleArticleProcessor:
    """××—×œ×§×” ×œ×¢×™×‘×•×“ ×¢×¨×š ×™×—×™×“ - ×‘×“×™×•×§ ×›××• ×‘×§×•×“ ×”××¢×•×“×›×Ÿ"""

    def clean_html_escape_codes(self, text):
        """×›×œ×œ 1: ×”×—×œ×¤×ª HTML escape codes"""
        # ×”×—×œ×¤×ª escape codes ×©×•× ×™×
        text = text.replace('&quot;', '"')
        text = text.replace('&#34;', '"')
        text = text.replace('&#39;', "'")
        return text

    def clean_newlines_and_spaces(self, text):
        """×›×œ×œ 2: ×˜×™×¤×•×œ ×‘×©×•×¨×•×ª ×—×“×©×•×ª ×•×¨×•×•×—×™×"""
        # ×”×¡×¨×ª carriage return
        text = text.replace('\r', '')
        # ×”×—×œ×¤×ª ×™×•×ª×¨ ×-3 ×©×•×¨×•×ª ×—×“×©×•×ª ×¨×¦×•×¤×•×ª ×‘××§×¡×™××•× 3
        text = re.sub(r'\n{4,}', '\n\n\n', text)
        return text

    def clean_multiple_spaces(self, text):
        """×›×œ×œ 3: ×˜×™×¤×•×œ ×‘×¨×•×•×—×™× ××¨×•×‘×™×"""

        def replace_spaces(match):
            space_count = len(match.group(0))
            # ×× ××ª×—×œ×§ ×‘-4, ×œ×”×©××™×¨ ×¢×“ ××§×¡×™××•× 16
            if space_count % 4 == 0:
                return ' ' * min(space_count, 16)
            else:
                # ×× ×œ× ××ª×—×œ×§ ×‘-4, ×œ×¦××¦× ×œ×¨×•×•×— ××—×“
                return ' '

        # ××¦×™××ª ×¨×¦×¤×™× ×©×œ 2+ ×¨×•×•×—×™×
        text = re.sub(r' {2,}', replace_spaces, text)
        return text

    def clean_whitespace_start_end(self, text):
        """×›×œ×œ 4: ×”×¡×¨×ª ×¨×•×•×—×™× ××ª×—×™×œ×ª ×•×¡×•×£ ×”×˜×§×¡×˜"""
        return text.strip()

    def is_localhost_ip(self, ip_str):
        """×‘×“×™×§×” ×× IP ×”×•× localhost (127.0.0.0/8)"""
        try:
            ip = ipaddress.ip_address(ip_str)
            localhost_network = ipaddress.ip_network('127.0.0.0/8')
            return ip in localhost_network
        except:
            return False

    def clean_pii(self, text):
        """×›×œ×œ 6: ××—×™×§×ª PII - IP ×•××™×™×œ (×©××™×¨×ª localhost)"""
        # ××—×™×§×ª ×›×ª×•×‘×•×ª ××™×™×œ
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        text = re.sub(email_pattern, '[EMAIL_REMOVED]', text)

        # ××—×™×§×ª IP addresses (×—×•×¥ ×-localhost)
        ip_pattern = r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b'

        def replace_ip(match):
            ip = match.group(0)
            if self.is_localhost_ip(ip):
                return ip  # ×©××™×¨×ª localhost
            else:
                return '[IP_REMOVED]'

        text = re.sub(ip_pattern, replace_ip, text)
        return text

    def clean_empty_bullet_lines(self, text):
        """×›×œ×œ 7: ×”×¡×¨×ª ×©×•×¨×•×ª ×¨×™×§×•×ª ×¢× bullet points"""
        # ×”×¡×¨×ª ×©×•×¨×•×ª ×©××›×™×œ×•×ª ×¨×§ bullet points ×•×¨×•×•×—×™×
        bullet_pattern = r'^\s*[â€¢â—â– â—¦â–ªâ—†]+\s*$'
        lines = text.split('\n')
        cleaned_lines = []

        for line in lines:
            if not re.match(bullet_pattern, line):
                cleaned_lines.append(line)

        return '\n'.join(cleaned_lines)

    def clean_separator_lines(self, text):
        """×›×œ×œ 8: ×”×¡×¨×ª ×§×•×•×™ ×”×¤×¨×“×” ××¨×•×›×™×"""
        # ×”×¡×¨×ª ×©×•×¨×•×ª ×¢× ×”×¤×¨×“×•×ª ××¨×•×›×•×ª - ×¨×§ ×ª×•×•×™ ×”×¤×¨×“×” ×‘××•×¨×š 4+
        separator_patterns = [
            r'^[-]{4,}$',  # ××§×¤×™× ×‘×œ×‘×“
            r'^[=]{4,}$',  # ×¡×™×× ×™ ×©×•×•×” ×‘×œ×‘×“
            r'^[_]{4,}$',  # ×§×• ×ª×—×ª×•×Ÿ ×‘×œ×‘×“
            r'^[~]{4,}$',  # ×˜×™×œ×“×” ×‘×œ×‘×“
            r'^[*]{4,}$',  # ×›×•×›×‘×™×•×ª ×‘×œ×‘×“
            r'^[#]{4,}$',  # ×¤××•× ×“ ×‘×œ×‘×“
        ]

        lines = text.split('\n')
        cleaned_lines = []

        for line in lines:
            line_stripped = line.strip()
            should_remove = False

            # ×‘×“×™×§×” ×× ×”×©×•×¨×” ××›×™×œ×” ×¨×§ ×ª×•×•×™ ×”×¤×¨×“×” (4 ××• ×™×•×ª×¨)
            for pattern in separator_patterns:
                if re.match(pattern, line_stripped):
                    should_remove = True
                    break

            # ×¨×§ ×× ×”×©×•×¨×” ×œ× ×¦×¨×™×›×” ×œ×”×™×•×ª ××•×¡×¨×ª, × ×›×œ×•×œ ××•×ª×”
            if not should_remove:
                cleaned_lines.append(line)

        return '\n'.join(cleaned_lines)

    def clean_css_from_tables(self, text):
        """×›×œ×œ 9: ×”×¡×¨×ª CSS ××˜×‘×œ××•×ª"""
        # ×”×¡×¨×ª style attributes
        text = re.sub(r'style="[^"]*"', '', text)
        text = re.sub(r'cellspacing="[^"]*"', '', text)
        text = re.sub(r'cellpadding="[^"]*"', '', text)
        text = re.sub(r'class="[^"]*"', '', text)
        text = re.sub(r'width="[^"]*"', '', text)
        text = re.sub(r'height="[^"]*"', '', text)
        return text

    def convert_tables_to_markdown(self, text):
        """×›×œ×œ 10: ×”××¨×ª ×˜×‘×œ××•×ª ×•×™×§×™ ×œ××¨×§×“××•×Ÿ"""

        def process_wiki_table(match):
            table_content = match.group(0)
            table_inner = table_content[2:-2]  # ×”×¡×¨ {| ×• |}

            rows = re.split(r'\|-', table_inner)
            markdown_rows = []

            for i, row in enumerate(rows):
                row = row.strip()
                if not row:
                    continue

                # ×¤×™×¦×•×œ ×ª××™×
                cells = re.split(r'\|\||\|', row)
                clean_cells = []

                for cell in cells:
                    cell = cell.strip()
                    if cell and not re.match(r'^\d+px$', cell):
                        # × ×™×§×•×™ ×ª×
                        cell = re.sub(r'^[\|\s]+', '', cell)
                        cell = re.sub(r'[\|\s]+$', '', cell)
                        if cell and len(cell) > 1:
                            clean_cells.append(cell)

                if clean_cells:
                    # ×™×¦×™×¨×ª ×©×•×¨×ª ××¨×§×“××•×Ÿ
                    markdown_row = "| " + " | ".join(clean_cells) + " |"
                    markdown_rows.append(markdown_row)

                    # ×”×•×¡×¤×ª ×©×•×¨×ª ×”×¤×¨×“×” ××—×¨×™ ×”×©×•×¨×” ×”×¨××©×•× ×”
                    if i == 0 and len(clean_cells) > 0:
                        separator = "|" + "---|" * len(clean_cells)
                        markdown_rows.append(separator)

            return "\n".join(markdown_rows) if markdown_rows else ""

        # ×”××¨×ª ×˜×‘×œ××•×ª ×•×™×§×™
        table_pattern = r'\{\|.*?\|\}'
        text = re.sub(table_pattern, process_wiki_table, text, flags=re.DOTALL)
        return text

    def clean_wiki_templates_and_markup(self, text):
        """×›×œ×œ 11a,b: ×”×¡×¨×ª ×ª×‘× ×™×•×ª ×•-markup ×©×œ ×•×™×§×™"""
        # ×”×¡×¨×ª ×ª×‘× ×™×•×ª ××•×¨×›×‘×•×ª ×¢× ×ª×‘× ×™×•×ª ××§×•× × ×•×ª {{}}
        # ×¢×•×©×” ×–××ª ××¡×¤×¨ ×¤×¢××™× ×›×“×™ ×œ×”×ª××•×“×“ ×¢× ×§×™× ×•×Ÿ ×¢××•×§
        for _ in range(5):  # ××§×¡×™××•× 5 ××™×˜×¨×¦×™×•×ª
            old_text = text
            text = re.sub(r'\{\{[^{}]*\}\}', '', text)
            if text == old_text:  # ×× ×œ× ×”×©×ª× ×”, ××¤×¡×™×§
                break

        # ×”×¡×¨×ª ×§×™×©×•×¨×™ ×§×‘×¦×™×/×ª××•× ×•×ª ×œ×¤× ×™ ×§×™×©×•×¨×™× ×¨×’×™×œ×™×
        media_link_patterns = [
            r'\[\[×§×•×‘×¥:.*?\]\]',
            r'\[\[File:.*?\]\]',
            r'\[\[Image:.*?\]\]',
            r'\[\[×ª××•× ×”:.*?\]\]'
        ]

        for pattern in media_link_patterns:
            text = re.sub(pattern, '', text, flags=re.DOTALL | re.IGNORECASE)

        # ×”×¡×¨×ª ×§×™×©×•×¨×™× ×¤× ×™××™×™× [[]] ×¢× ×ª×™××•×¨×™×
        # ××˜×¤×œ ×‘×§×™×©×•×¨×™× ××”×¦×•×¨×” [[×§×™×©×•×¨|×ª×™××•×¨]] â†’ ×ª×™××•×¨
        text = re.sub(r'\[\[([^|\]]+)\|([^\]]+)\]\]', r'\2', text)

        # ×”×¡×¨×ª ×§×™×©×•×¨×™× ×¤×©×•×˜×™× [[×§×™×©×•×¨]] â†’ ×§×™×©×•×¨
        text = re.sub(r'\[\[([^\]]+)\]\]', r'\1', text)

        # ×”×¡×¨×ª ×ª×’×™×•×ª HTML (×›×•×œ×œ ×ª×’×™×•×ª ×¢× ×ª×›×•× ×•×ª)
        text = re.sub(r'<[^>]*>', '', text)

        # × ×™×§×•×™ ×¨×•×•×—×™× ××™×•×ª×¨×™× ×•×¤×¡×™×§×™× ×©× ×•×ª×¨×•
        # ×”×¡×¨×ª ×¤×¡×™×§×™× ××™×•×ª×¨×™× ×©× ×•×ª×¨×• ××—×¨×™ ×”×¡×¨×ª ×§×™×©×•×¨×™×
        text = re.sub(r',\s*,', ',', text)  # ×¤×¡×™×§×™× ×›×¤×•×œ×™×
        text = re.sub(r',\s*\]', ']', text)  # ×¤×¡×™×§ ×œ×¤× ×™ ×¡×’×™×¨×ª ×¡×•×’×¨×™×™×
        text = re.sub(r'\[\s*,', '[', text)  # ×¤×¡×™×§ ××—×¨×™ ×¤×ª×™×—×ª ×¡×•×’×¨×™×™×

        # ×”×¡×¨×ª ×¨×•×•×—×™× ××¨×•×‘×™× (×¨×§ ×¨×•×•×—×™× ×•×˜××‘×™×, ×œ× ×©×•×¨×•×ª ×—×“×©×•×ª)
        text = re.sub(r'[ \t]+', ' ', text)

        # ×”×¡×¨×ª ×¤×¡×™×§×™× ×‘×ª×—×™×œ×ª ××• ×¡×•×£ ××©×¤×˜×™×
        text = re.sub(r'^\s*,\s*', '', text, flags=re.MULTILINE)
        text = re.sub(r'\s*,\s*$', '', text, flags=re.MULTILINE)

        return text

    def clean_wiki_media_descriptions(self, text):
        """×›×œ×œ 11b: ×”×¡×¨×ª ×ª×™××•×¨×™ ××“×™×” ×•×ª××•× ×•×ª"""

        # ×”×¡×¨×ª ×ª×™××•×¨×™ ×§×‘×¦×™ ××“×™×” (File:, ×§×•×‘×¥:, Image:)
        media_patterns = [
            r'\[\[×§×•×‘×¥:.*?\]\]',
            r'\[\[File:.*?\]\]',
            r'\[\[Image:.*?\]\]',
            r'\[\[×ª××•× ×”:.*?\]\]'
        ]

        for pattern in media_patterns:
            text = re.sub(pattern, '', text, flags=re.DOTALL | re.IGNORECASE)

        # ×”×¡×¨×ª ×ª×™××•×¨×™ ××™×§×•× ×ª××•× ×•×ª - ×‘××•×¤×Ÿ ×¡×¤×¦×™×¤×™
        location_keywords = ['×©×××œ', '×™××™×Ÿ', '××¨×›×–', '×××•×–×¢×¨', 'thumb', 'thumbnail', 'frame', 'framed', 'left', 'right',
                             'center']

        for keyword in location_keywords:
            # ×”×¡×¨×” ×¡×¤×¦×™×¤×™×ª ×©×œ ×ª×™××•×¨×™ ××™×§×•× (×›××• ×××•×–×¢×¨|300px|)
            pattern = rf'\b{keyword}\|[^|]*?\|'
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)

        # ×”×¡×¨×ª ×”×¤× ×™×•×ª "×¨××•" - ×ª××™×›×” ×‘×¢×‘×¨×™×ª ×•×× ×’×œ×™×ª
        see_patterns = [
            r'×¨××• [A-Za-z\s,×-×ª\.]+',
            r'×¨××” [A-Za-z\s,×-×ª\.]+',
            r'see [A-Za-z\s,\.]+',
            r'See [A-Za-z\s,\.]+',
            r'×¨××• ×’× [A-Za-z\s,×-×ª\.]+',
            r'×¨××” ×’× [A-Za-z\s,×-×ª\.]+',
            r'see also [A-Za-z\s,\.]+',
            r'See also [A-Za-z\s,\.]+',
        ]

        for pattern in see_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)

        # ×”×¡×¨×ª ×ª×™××•×¨×™× ×‘×¡×•×’×¨×™×™× ×©××›×™×œ×™× ××™×“×¢ ×¢×œ ××“×™×”
        text = re.sub(r'\([^)]*(?:px|×××•×–×¢×¨|thumb|frame)[^)]*\)', '', text, flags=re.IGNORECASE)

        # ×”×¡×¨×ª ××™×“×¢ ×¢×œ ×’×•×“×œ ×ª××•× ×•×ª
        text = re.sub(r'\b\d+px\b', '', text)

        # × ×™×§×•×™ ×©×•×¨×•×ª ×¨×™×§×•×ª ×¢×•×“×¤×•×ª ×©× ×•×¦×¨×•
        text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)

        # × ×™×§×•×™ ×¨×•×•×—×™× ××™×•×ª×¨×™× (×¨×§ ×¨×•×•×—×™× ×•×˜××‘×™×, ×œ× ×©×•×¨×•×ª ×—×“×©×•×ª)
        text = re.sub(r'[ \t]+', ' ', text)
        text = re.sub(r'^[ \t]+', '', text, flags=re.MULTILINE)
        text = re.sub(r'[ \t]+$', '', text, flags=re.MULTILINE)

        return text

    def clean_wiki_headers(self, text):
        """×›×œ×œ 11d: ×”××¨×ª ×›×•×ª×¨×•×ª ×•×™×§×™ ×œ×¤×•×¨××˜ ××¨×§×“××•×Ÿ"""

        # ×”××¨×ª ×›×•×ª×¨×•×ª ×•×™×§×™ (== ×›×•×ª×¨×ª ==) ×œ××¨×§×“××•×Ÿ (## ×›×•×ª×¨×ª)
        def convert_header(match):
            # ××¡×¤×¨ ×”-= ×§×•×‘×¢ ××ª ×¨××ª ×”×›×•×ª×¨×ª
            equals_prefix = match.group(1)
            header_text = match.group(2).strip()
            equals_suffix = match.group(3)

            # ×¡×¤×™×¨×ª ××¡×¤×¨ ×”-= (×œ×•×§×— ××ª ×”××™× ×™××•× ×‘×™×Ÿ ×”×ª×—×œ×” ×•×¡×•×£)
            level = min(len(equals_prefix), len(equals_suffix))

            # ×”××¨×” ×œ-markdown headers (××§×¡×™××•× 6 ×¨××•×ª)
            markdown_level = min(level, 6)
            return '#' * markdown_level + ' ' + header_text

        # ×“×¤×•×¡ ×¨×’×•×œ×¨×™ ××©×•×¤×¨ ×œ×–×™×”×•×™ ×›×•×ª×¨×•×ª ×•×™×§×™
        # ××—×¤×©: (=+) + ×¨×•×•×—×™× ××•×¤×¦×™×•× ×œ×™× + ×ª×•×›×Ÿ + ×¨×•×•×—×™× ××•×¤×¦×™×•× ×œ×™× + (=+)
        header_pattern = r'^(={2,6})\s*([^=\r\n]+?)\s*(={2,6})\s*$'

        text = re.sub(header_pattern, convert_header, text, flags=re.MULTILINE)

        return text

    def clean_wiki_citations(self, text):
        """×›×œ×œ 11e: ×”×¡×¨×ª citations"""
        # ×”×¡×¨×ª citations ×©×•× ×™×
        citation_patterns = [
            r'<ref[^>]*>.*?</ref>',  # ref tags
            r'<ref[^>]*/>',  # self-closing ref tags
            r'\{\{cite[^}]*\}\}',  # cite templates
            r'\{\{×¦-[^}]*\}\}',  # Hebrew citations
            r'\{\{×”×¢×¨×”[^}]*\}\}',  # Hebrew notes
            r'\{\{××§×•×¨[^}]*\}\}',  # Hebrew sources
        ]

        for pattern in citation_patterns:
            text = re.sub(pattern, '', text, flags=re.DOTALL | re.IGNORECASE)

        return text

    def check_redirecting_article(self, raw_wikitext):
        """×›×œ×œ 11f: ×‘×“×™×§×” ×× ×”××××¨ ×”×•× ×”×¤× ×™×”"""
        is_redirect = raw_wikitext.strip().startswith('#REDIRECT') or raw_wikitext.strip().startswith('#×”×¤× ×™×”')
        return is_redirect

    def apply_all_cleaning_rules(self, title, raw_wikitext):
        """×”×¤×¢×œ×ª ×›×œ ×›×œ×œ×™ ×”× ×™×§×™×•×Ÿ ×¢×œ ×˜×§×¡×˜ - ×‘×“×™×•×§ ×›××• ×‘×§×•×“ ×”××¢×•×“×›×Ÿ"""
        # ×‘×“×™×§×” ×× ×–×” ××××¨ ××¤× ×” (×›×œ×œ 11f)
        if self.check_redirecting_article(raw_wikitext):
            return None

        # ×”××¨×” ×œ××•×‘×™×™×§×˜ wikicode
        try:
            wikicode = mwparserfromhell.parse(raw_wikitext)
            text = str(wikicode)
        except:
            text = raw_wikitext

        # ×”×¤×¢×œ×ª ×›×œ ×”×›×œ×œ×™× ×‘×“×™×•×§ ×‘××•×ª×• ×¡×“×¨
        text = self.clean_html_escape_codes(text)  # ×›×œ×œ 1
        text = self.clean_newlines_and_spaces(text)  # ×›×œ×œ 2
        text = self.clean_multiple_spaces(text)  # ×›×œ×œ 3
        text = self.clean_whitespace_start_end(text)  # ×›×œ×œ 4
        text = self.clean_pii(text)  # ×›×œ×œ 6
        text = self.clean_empty_bullet_lines(text)  # ×›×œ×œ 7
        text = self.clean_separator_lines(text)  # ×›×œ×œ 8
        text = self.clean_css_from_tables(text)  # ×›×œ×œ 9
        text = self.convert_tables_to_markdown(text)  # ×›×œ×œ 10
        text = self.clean_wiki_templates_and_markup(text)  # ×›×œ×œ 11a,b
        text = self.clean_wiki_media_descriptions(text)  # ×›×œ×œ 11b
        text = self.clean_wiki_headers(text)  # ×›×œ×œ 11d
        text = self.clean_wiki_citations(text)  # ×›×œ×œ 11e

        # ×‘×“×™×§×” ×¡×•×¤×™×ª ×©×œ ××™×›×•×ª
        if not text or len(text) < 100:
            return None

        return text


def process_with_updated_method(raw_wikitext, article_title):
    """
    ×¢×™×‘×•×“ ×¢× ×”×©×™×˜×” ×”××¢×•×“×›× ×ª (×‘×“×™×•×§ ×›××• ×‘×§×•×“ ×”××¢×•×“×›×Ÿ)
    """
    print(f"âš™ï¸ ××¢×‘×“ ××ª ×”×•×™×§×™-×˜×§×¡×˜...")

    processor = SingleArticleProcessor()

    # ×¢×™×‘×•×“ ×”×¢×¨×š
    cleaned_text = processor.apply_all_cleaning_rules(article_title, raw_wikitext)

    if cleaned_text:
        print(f"âœ… ×¢×™×‘×•×“ ×”×•×©×œ× ×‘×”×¦×œ×—×”")
        print(f"ğŸ“„ ××•×¨×š ××—×¨×™ ×¢×™×‘×•×“: {len(cleaned_text):,} ×ª×•×•×™×")
        return cleaned_text
    else:
        print(f"âŒ ×”×¢×¨×š × ×¤×¡×œ ×‘×¢×™×‘×•×“ (×™×™×ª×›×Ÿ ×©×”×•× ×”×¤× ×™×” ××• ×§×¦×¨ ××“×™)")
        return None


def main():
    """
    ×”×¤×¢×œ×” ×¨××©×™×ª
    """
    print("ğŸ¯ ××•×¦× ×•××¢×‘×“ ×¢×¨×š ××”×“×××¤ ×¢× ×”×©×™×˜×” ×”××¢×•×“×›× ×ª")
    print("=" * 60)

    # ×”×’×“×¨×•×ª
    dump_path = r'C:\Users\Dan Revital\OneDrive\Documents\gepeta\data\wikipedia\hewiki-latest-pages-articles.xml.bz2'
    article_title = '×”×‘×™× ×•× ×©×œ × ×™×•×˜×•×Ÿ'

    print(f"ğŸ” ××—×¤×© ×¢×¨×š: '{article_title}'")
    print(f"ğŸ“‚ ×‘×“×××¤: {dump_path}")
    print()

    # ×©×œ×‘ 1: ××¦×™××ª ×”×¢×¨×š
    raw_wikitext = find_article_in_dump(dump_path, article_title)

    if not raw_wikitext:
        print(f"âŒ ×œ× × ××¦× ×”×¢×¨×š '{article_title}'")
        return

    print()

    # ×©×œ×‘ 2: ×¢×™×‘×•×“ ×¢× ×”×©×™×˜×” ×”××¢×•×“×›× ×ª
    print("=" * 60)
    processed_content = process_with_updated_method(raw_wikitext, article_title)

    if not processed_content:
        print("âŒ ×”×¢×™×‘×•×“ × ×›×©×œ")
        return

    # ×©×œ×‘ 3: ×©××™×¨×ª ×”×ª×•×¦××•×ª
    print("=" * 60)
    print("ğŸ’¾ ×©×•××¨ ×ª×•×¦××•×ª...")

    # ×©××™×¨×ª ×”×˜×§×¡×˜ ×”×’×•×œ××™
    raw_filename = f"{article_title.replace(' ', '_')}_raw_wikitext.txt"
    with open(raw_filename, 'w', encoding='utf-8') as f:
        f.write(raw_wikitext)
    print(f"âœ… ×•×™×§×™-×˜×§×¡×˜ ×’×•×œ××™ × ×©××¨: {raw_filename}")

    # ×©××™×¨×ª ×”×˜×§×¡×˜ ×”××¢×•×‘×“
    processed_filename = f"{article_title.replace(' ', '_')}_updated_processed.txt"
    with open(processed_filename, 'w', encoding='utf-8') as f:
        f.write(processed_content)
    print(f"âœ… ×˜×§×¡×˜ ××¢×•×‘×“ × ×©××¨: {processed_filename}")

    # ×¡×™×›×•×
    print("\n" + "=" * 60)
    print("ğŸ‰ ×”×•×©×œ× ×‘×”×¦×œ×—×”!")
    print(f"ğŸ“Š ×¡×™×›×•×:")
    print(f"   ×•×™×§×™-×˜×§×¡×˜ ×’×•×œ××™: {len(raw_wikitext):,} ×ª×•×•×™×")
    print(f"   ×˜×§×¡×˜ ××¢×•×‘×“: {len(processed_content):,} ×ª×•×•×™×")
    print(f"   ×“×—×™×¡×”: {(1 - len(processed_content) / len(raw_wikitext)) * 100:.1f}%")

    print(f"\nğŸ“„ ×“×•×’××” ××”×˜×§×¡×˜ ×”××¢×•×‘×“ (200 ×ª×•×•×™× ×¨××©×•× ×™×):")
    print(f"   {processed_content[:200]}...")


if __name__ == "__main__":
    main()