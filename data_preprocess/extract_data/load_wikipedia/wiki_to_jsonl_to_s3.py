#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import xml.etree.ElementTree as ET
import mwparserfromhell
import json
import re
import bz2
import boto3
from pathlib import Path
from tqdm import tqdm
import time
import os


class FullWikipediaProcessor:
    def __init__(self, dump_path, s3_bucket, s3_prefix, chunk_size=50000):
        self.dump_path = dump_path
        self.s3_bucket = s3_bucket
        self.s3_prefix = s3_prefix
        self.chunk_size = chunk_size
        self.output_dir = Path("output")
        self.output_dir.mkdir(exist_ok=True)

        # S3 client
        self.s3_client = boto3.client('s3')

        # ××•× ×™×
        self.total_processed = 0
        self.total_scanned = 0
        self.current_chunk = 1
        self.current_chunk_count = 0
        self.current_file = None
        self.uploaded_files = []

    def normalize_text_for_training(self, text):
        """× ×¨××•×œ ×˜×§×¡×˜ (×–×”×” ×œ×—×œ×•×˜×™×Ÿ ×œ×ª×•×›× ×™×ª ×”××“×’×)"""
        if not text or not isinstance(text, str):
            return text

        # ×ª×™×§×•×Ÿ ×ª×•×•×™ ×‘×¨×™×—×” ×œ×¤× ×™ ×”×›×œ
        text = text.replace('\\"', '"')
        text = text.replace("\\'", "'")
        text = text.replace('\\\\', '\\')

        # ×ª×™×§×•×Ÿ ×’× ×•×¨×™××¦×™×•×ª ×©×œ ×ª×•×•×™ ×‘×¨×™×—×”
        text = text.replace('&quot;', '"')
        text = text.replace('&#34;', '"')
        text = text.replace('&#39;', "'")

        # ××—×§ ×©×•×¨×•×ª ×—×“×©×•×ª ×•-\r
        text = text.replace('\n', ' ')
        text = text.replace('\r', ' ')
        text = text.replace('\t', ' ')

        # ××—×§ ×ª×•×•×™ ×‘×¨×™×—×” ×©× ×©××¨×•
        text = text.replace('\\n', ' ')
        text = text.replace('\\t', ' ')
        text = text.replace('\\r', ' ')

        # ×¨×•×•×—×™× ××¨×•×‘×™× â†’ ×¨×•×•×— ×™×—×™×“
        text = re.sub(r'\s+', ' ', text)

        # × ×§×” ×¨×•×•×—×™× ×‘×”×ª×—×œ×”/×¡×•×£
        text = text.strip()

        return text

    def convert_wiki_tables_to_lists(self, content):
        """×”××¨×ª ×˜×‘×œ××•×ª ×•×™×§×™ ×œ×¨×©×™××•×ª × ×§×™×•×ª"""

        def process_single_table(match):
            table_content = match.group(0)
            table_inner = table_content[2:-2]  # ×”×¡×¨ {| ×• |}

            rows = re.split(r'\|-', table_inner)
            items = []

            for row in rows:
                row = row.strip()
                if not row:
                    continue

                # ×”×¡×¨×ª ×¡×’× ×•× ×•×ª CSS
                row = re.sub(r'style="[^"]*"', '', row)
                row = re.sub(r'cellspacing="[^"]*"', '', row)
                row = re.sub(r'cellpadding="[^"]*"', '', row)
                row = re.sub(r'\|\s*\d+px', '', row)

                # ×¤×™×¦×•×œ ×ª××™×
                cells = re.split(r'\|\||\|', row)

                for cell in cells:
                    cell = cell.strip()
                    if cell and not re.match(r'^\d+px$', cell):
                        cell = re.sub(r'^[\|\s]+', '', cell)
                        cell = re.sub(r'[\|\s]+$', '', cell)
                        if cell and len(cell) > 1:
                            items.append(cell)

            if items:
                if len(items) >= 3:
                    result = "## × ×•×©××™× ×§×©×•×¨×™×:\n"
                    for item in items:
                        result += "- " + item + "\n"
                    return result
                else:
                    return " ".join(items)

            return ""

        table_pattern = r'\{\|.*?\|\}'
        cleaned_content = re.sub(table_pattern, process_single_table, content, flags=re.DOTALL)
        return cleaned_content

    def apply_regex_cleaning(self, content):
        """× ×™×§×•×™ regex (×¢× ×ª×•×¡×¤×ª ×˜×™×¤×•×œ ×‘×˜×‘×œ××•×ª)"""
        # ×”××¨×ª ×˜×‘×œ××•×ª ×•×™×§×™ ×œ×¨×©×™××•×ª
        content = self.convert_wiki_tables_to_lists(content)

        # ×”×¡×¨×ª ×ª×‘× ×™×•×ª ×©× ×©××¨×•
        content = re.sub(r'\{\{[^}]*\}\}', '', content)
        content = re.sub(r'\[\[[^]]*\]\]', '', content)
        content = re.sub(r'<[^>]*>', '', content)

        # ×”×¡×¨×ª ×ª×™××•×¨×™ ×ª××•× ×•×ª ×•××“×™×”
        content = re.sub(r'^(×©×××œ|×™××™×Ÿ|××¨×›×–|×××•×–×¢×¨)\|.*$', '', content, flags=re.MULTILINE)
        content = re.sub(r'^\s*(×©×××œ|×™××™×Ÿ|××¨×›×–|×××•×–×¢×¨)\|.*$', '', content, flags=re.MULTILINE)

        # ×”×¡×¨×ª ×”×¡×‘×¨×™ ×©×¤×•×ª ×–×¨×•×ª
        foreign_languages = ['×‘×’×¨×× ×™×ª', '×‘×”×•× ×’×¨×™×ª', '×‘×¢×¨×‘×™×ª', '×‘×›×•×¨×“×™×ª', '×‘×× ×’×œ×™×ª',
                             '×‘×¦×¨×¤×ª×™×ª', '×‘××™×˜×œ×§×™×ª', '×‘×¨×•×¡×™×ª', '×‘×™×•×•× ×™×ª', '×‘×œ×˜×™× ×™×ª']
        for lang in foreign_languages:
            pattern = r'\(' + lang + r':.*?\)'
            content = re.sub(pattern, '', content)

        # ×”×¡×¨×ª ×”×¤× ×™×•×ª ×œ×ª××•× ×•×ª
        content = re.sub(r'×¨××• [A-Za-z\s,]+\.', '', content)

        # × ×™×§×•×™ ×›×œ×œ×™
        content = re.sub(r'\n{3,}', '\n\n', content)
        content = re.sub(r' {2,}', ' ', content)
        content = re.sub(r'^\s*=+.*?=+\s*$', '', content, flags=re.MULTILINE)

        return content

    def identify_headers(self, content):
        """×–×™×”×•×™ ×•×¡×™××•×Ÿ ×›×•×ª×¨×•×ª"""
        lines = content.split('\n')
        processed_lines = []

        for i, line in enumerate(lines):
            line = line.strip()

            if (line and len(line) < 100 and
                    i < len(lines) - 1 and
                    len(lines[i + 1].strip()) > 50 and
                    line.count('.') <= 1 and
                    line.count(',') <= 2):

                header_keywords = ['×”×™×¡×˜×•×¨×™×”', '×‘×™×•×’×¨×¤×™×”', '×¨×§×¢', '×ª×•×œ×“×•×ª', '××•×¦×', '×ª×¨×‘×•×ª', '××©×¤×—×ª×•', '×™×œ×“×•×ª×•',
                                   '× ×¢×•×¨×™×•', '×”×ª×¤×ª×—×•×ª', '××©×™××•×ª']
                if (not line.endswith('.') or
                        any(keyword in line for keyword in header_keywords)):
                    processed_lines.append("## " + line)
                else:
                    processed_lines.append(line)
            else:
                processed_lines.append(line)

        return '\n'.join(processed_lines)

    def clean_content(self, wikicode):
        """× ×™×§×•×™ ×ª×•×›×Ÿ (×–×”×” ×œ×—×œ×•×˜×™×Ÿ ×œ×ª×•×›× ×™×ª ×”××“×’×)"""
        try:
            # ×”×¡×¨×ª ×ª×‘× ×™×•×ª
            templates_to_remove = []
            for template in wikicode.filter_templates():
                template_name = str(template.name).strip().lower()
                remove_patterns = [
                    'cite', '×¦-', '×”×¢×¨×”', '××§×•×¨', 'reflist', '××§×•×¨×•×ª',
                    '×¦×™×•×Ÿ', 'ref', 'citation', 'web', 'news', 'book', 'journal'
                ]
                if any(pattern in template_name for pattern in remove_patterns):
                    templates_to_remove.append(template)

            for template in templates_to_remove:
                try:
                    wikicode.remove(template)
                except:
                    pass

            # ×”××¨×ª ×§×™×©×•×¨×™× ×¤× ×™××™×™× ×œ×˜×§×¡×˜
            for link in wikicode.filter_wikilinks():
                try:
                    if link.text:
                        wikicode.replace(link, str(link.text))
                    else:
                        title = str(link.title)
                        if '|' in title:
                            title = title.split('|')[0]
                        wikicode.replace(link, title)
                except:
                    pass

            # ×”×¡×¨×ª ×ª×’×™×•×ª
            for tag in wikicode.filter_tags():
                try:
                    if tag.tag.lower() in ['math', 'chem']:
                        wikicode.replace(tag, "[× ×•×¡×—×”: " + str(tag.contents) + "]")
                    elif tag.tag.lower() in ['ref', 'references']:
                        wikicode.remove(tag)
                except:
                    pass

            # ×”××¨×ª ×”×›×œ ×œ×˜×§×¡×˜
            content = str(wikicode.strip_code())

            # × ×™×§×•×™ regex
            content = self.apply_regex_cleaning(content)

            # ×–×™×”×•×™ ×›×•×ª×¨×•×ª
            content = self.identify_headers(content)

            # × ×¨××•×œ ×”×˜×§×¡×˜ ×œ××™××•×Ÿ
            content = self.normalize_text_for_training(content)

            return content.strip()

        except Exception as e:
            print("Error in clean_content: " + str(e))
            basic_clean = str(wikicode)[:2000]
            return self.normalize_text_for_training(basic_clean)

    def count_words(self, text):
        """×¡×¤×™×¨×ª ××™×œ×™×"""
        if not text:
            return 0
        words = text.split()
        return len(words)

    def count_bytes(self, text):
        """×¡×¤×™×¨×ª ×‘×™×™×˜×™× ×‘-UTF-8"""
        if not text:
            return 0
        return len(text.encode('utf-8'))

    def process_article(self, title, raw_wikitext):
        """×¢×™×‘×•×“ ×¢×¨×š ×™×—×™×“ ×œ×™×¦×™×¨×ª ×¤×¨×™×˜ JSONL"""
        try:
            wikicode = mwparserfromhell.parse(raw_wikitext)
            processed_text = self.clean_content(wikicode)

            if not processed_text or len(processed_text) < 100:
                return None

            article_item = {
                "text": processed_text,
                "word_count": self.count_words(processed_text),
                "byte_count": self.count_bytes(processed_text)
            }

            return article_item

        except Exception as e:
            return None

    def is_valid_article(self, page_elem):
        """×‘×“×™×§×” ×× ×”×“×£ ×ª×§×™×Ÿ ×œ×¢×™×‘×•×“"""
        namespace_elem = page_elem.find('.//{*}ns')
        if namespace_elem is None or namespace_elem.text != '0':
            return False

        revision = page_elem.find('.//{*}revision')
        if revision is None:
            return False

        text_elem = revision.find('.//{*}text')
        if text_elem is None or not text_elem.text:
            return False

        if len(text_elem.text) < 500:
            return False

        if text_elem.text.strip().startswith('#REDIRECT'):
            return False

        return True

    def get_current_filename(self):
        """×™×•×¦×¨ ×©× ×§×•×‘×¥ ×œchunk ×”× ×•×›×—×™"""
        return self.output_dir / f"wikipedia_he_part_{self.current_chunk:03d}.jsonl"

    def open_new_chunk_file(self):
        """×¤×•×ª×— ×§×•×‘×¥ ×—×“×© ×œchunk"""
        if self.current_file:
            self.current_file.close()

        filename = self.get_current_filename()
        self.current_file = open(filename, 'w', encoding='utf-8')
        self.current_chunk_count = 0

        print(f"\nğŸ“‚ ×¤×•×ª×— chunk ×—×“×©: {filename}")

    def write_article_to_current_chunk(self, article_item):
        """×›×•×ª×‘ ×¢×¨×š ×œchunk ×”× ×•×›×—×™"""
        if self.current_file is None or self.current_chunk_count >= self.chunk_size:
            if self.current_file:
                self.current_file.close()
                # ×”×¢×œ××” ×©×œ ×”chunk ×©×”×¡×ª×™×™×
                self.upload_chunk_to_s3(self.get_current_filename())
                self.current_chunk += 1

            self.open_new_chunk_file()

        json.dump(article_item, self.current_file, ensure_ascii=False)
        self.current_file.write('\n')
        self.current_chunk_count += 1

    def upload_chunk_to_s3(self, local_file):
        """××¢×œ×” chunk ×œ-S3"""
        s3_key = f"{self.s3_prefix}{local_file.name}"

        try:
            print(f"â¬†ï¸ ××¢×œ×” ×œ-S3: {s3_key}")

            # ×‘×“×™×§×ª ×’×•×“×œ ×§×•×‘×¥
            file_size_mb = local_file.stat().st_size / (1024 * 1024)

            start_time = time.time()
            self.s3_client.upload_file(
                str(local_file),
                self.s3_bucket,
                s3_key
            )
            upload_time = time.time() - start_time

            # ××™××•×ª ×©×”×”×¢×œ××” ×”×¦×œ×™×—×”
            try:
                self.s3_client.head_object(Bucket=self.s3_bucket, Key=s3_key)
                print(f"âœ… ×”×•×¢×œ×” ×‘×”×¦×œ×—×”: {file_size_mb:.1f}MB ×‘-{upload_time:.1f}s")
                self.uploaded_files.append(s3_key)

                # ××—×™×§×ª ×”×§×•×‘×¥ ×”××§×•××™ ×œ×—×¡×›×•×Ÿ ×‘××§×•×
                local_file.unlink()
                print(f"ğŸ—‘ï¸ × ××—×§ ×§×•×‘×¥ ××§×•××™: {local_file.name}")

            except:
                print(f"âŒ ×©×’×™××” ×‘××™××•×ª ×”×¢×œ××”: {s3_key}")

        except Exception as e:
            print(f"âŒ ×©×’×™××” ×‘×”×¢×œ××”: {e}")

    def estimate_total_articles(self):
        """××•××“×Ÿ ××¡×¤×¨ ×”×¢×¨×›×™× ×”×›×•×œ×œ ×‘×“×××¤"""
        print("ğŸ“Š ××¢×¨×™×š ××ª ××¡×¤×¨ ×”×¢×¨×›×™× ×”×›×•×œ×œ...")

        sample_size = 10000  # ×“×’×™××” ×©×œ 10K ×“×¤×™×
        valid_count = 0
        total_scanned = 0

        try:
            with bz2.open(self.dump_path, 'rt', encoding='utf-8') as dump_file:
                for event, elem in ET.iterparse(dump_file, events=('start', 'end')):
                    if event == 'end' and elem.tag.endswith('page'):
                        total_scanned += 1

                        if self.is_valid_article(elem):
                            valid_count += 1

                        elem.clear()

                        if total_scanned >= sample_size:
                            break

            # ×—×™×©×•×‘ ××•××“×Ÿ ×¢×œ ×‘×¡×™×¡ ×”×“×’×™××”
            if total_scanned > 0:
                success_rate = valid_count / total_scanned

                # ××•××“×Ÿ ×’×¡ ×©×œ ×¡×š ×”×“×¤×™× ×‘×•×™×§×™×¤×“×™×” ×”×¢×‘×¨×™×ª
                estimated_total_pages = 1500000  # ××•××“×Ÿ ×‘×¡×™×¡
                estimated_valid_articles = int(estimated_total_pages * success_rate)

                print(f"âœ… ×“×’×™××” ×©×œ {total_scanned:,} ×“×¤×™×:")
                print(f"   ğŸ“ˆ ×©×™×¢×•×¨ ×”×¦×œ×—×”: {success_rate * 100:.1f}%")
                print(f"   ğŸ¯ ××•××“×Ÿ ×¢×¨×›×™× ×ª×§×™× ×™×: {estimated_valid_articles:,}")

                return estimated_valid_articles

        except Exception as e:
            print(f"âš ï¸ ×œ× ×”×¦×œ×—×ª×™ ×œ×”×¢×¨×™×š ××ª ×”××¡×¤×¨ ×”×›×•×œ×œ: {e}")

        # fallback ×× ×”××•××“×Ÿ × ×›×©×œ
        return 800000  # ××•××“×Ÿ ×‘×¨×™×¨×ª ××—×“×œ

    def process_full_dump(self):
        """×¢×™×‘×•×“ ×”×“×××¤ ×”××œ×"""
        print("ğŸš€ ××ª×—×™×œ ×¢×™×‘×•×“ ××œ× ×©×œ ×•×™×§×™×¤×“×™×”")
        print("ğŸ“‚ ×“×××¤: " + self.dump_path)
        print("â˜ï¸ S3: s3://" + self.s3_bucket + "/" + self.s3_prefix)
        print("ğŸ“¦ ×’×•×“×œ chunk: " + str(self.chunk_size) + " ×¢×¨×›×™×")
        print("=" * 60)

        # ××•××“×Ÿ ××¡×¤×¨ ×”×¢×¨×›×™× ×”×›×•×œ×œ
        estimated_total = self.estimate_total_articles()
        print("=" * 60)

        start_time = time.time()

        # ×¤×ª×™×—×ª ×”chunk ×”×¨××©×•×Ÿ
        self.open_new_chunk_file()

        try:
            with bz2.open(self.dump_path, 'rt', encoding='utf-8') as dump_file:

                # ×™×¦×™×¨×ª progress bar ×¢× ××¡×¤×¨ ×›×•×œ×œ ××©×•×¢×¨
                with tqdm(total=estimated_total, desc="ğŸ”„ ×¢×™×‘×•×“ ×¢×¨×›×™×", unit="articles") as pbar:

                    for event, elem in ET.iterparse(dump_file, events=('start', 'end')):
                        if event == 'end' and elem.tag.endswith('page'):
                            self.total_scanned += 1

                            # ×¢×“×›×•×Ÿ ×”×ª×§×“××•×ª ×›×œ 500 ×“×¤×™× (×™×•×ª×¨ ×ª×›×•×£ ×œ×¢×“×›×•×Ÿ ×–××Ÿ ××“×•×™×§ ×™×•×ª×¨)
                            if self.total_scanned % 500 == 0:
                                # ×—×™×©×•×‘ ×”×¢×¨×›×ª ×–××Ÿ
                                elapsed_time = time.time() - start_time
                                if self.total_processed > 0:
                                    articles_per_second = self.total_processed / elapsed_time
                                    remaining_articles = estimated_total - self.total_processed
                                    eta_seconds = remaining_articles / articles_per_second if articles_per_second > 0 else 0
                                    eta_hours = eta_seconds / 3600

                                    pbar.set_postfix_str(
                                        f"×¡×¨×§×ª×™: {self.total_scanned:,} | "
                                        f"chunk: {self.current_chunk} ({self.current_chunk_count:,}/{self.chunk_size:,}) | "
                                        f"ETA: {eta_hours:.1f}h | "
                                        f"×§×¦×‘: {articles_per_second:.1f} ×¢×¨×›×™×/×©× "
                                    )
                                else:
                                    pbar.set_postfix_str(
                                        f"×¡×¨×§×ª×™: {self.total_scanned:,} | "
                                        f"chunk: {self.current_chunk} ({self.current_chunk_count:,}/{self.chunk_size:,})"
                                    )

                            # ×‘×“×™×§×” ×× ×”×“×£ ×ª×§×™×Ÿ
                            if self.is_valid_article(elem):

                                # ×—×™×œ×•×¥ ××™×“×¢
                                title_elem = elem.find('.//{*}title')
                                revision = elem.find('.//{*}revision')
                                text_elem = revision.find('.//{*}text')

                                title = title_elem.text if title_elem is not None else ""
                                raw_wikitext = text_elem.text if text_elem is not None else ""

                                # ×¢×™×‘×•×“ ×”×¢×¨×š
                                article_item = self.process_article(title, raw_wikitext)

                                if article_item:
                                    # ×›×ª×™×‘×” ×œchunk ×”× ×•×›×—×™
                                    self.write_article_to_current_chunk(article_item)

                                    self.total_processed += 1
                                    pbar.update(1)

                                    # ×¢×“×›×•×Ÿ ×”×ª×™××•×¨ ×”× ×•×›×—×™
                                    pbar.set_description(f"ğŸ”„ ×¢×™×‘×•×“: {title[:25]}...")

                            elem.clear()

            # ×¡×’×™×¨×ª ×”chunk ×”××—×¨×•×Ÿ
            if self.current_file:
                self.current_file.close()
                if self.current_chunk_count > 0:  # ×¨×§ ×× ×™×© ×ª×•×›×Ÿ
                    self.upload_chunk_to_s3(self.get_current_filename())

        except KeyboardInterrupt:
            print("\nâš ï¸ ×”×¢×™×‘×•×“ ×”×•×¤×¡×§ ×¢×œ ×™×“×™ ×”××©×ª××©")
            if self.current_file:
                self.current_file.close()
                if self.current_chunk_count > 0:
                    self.upload_chunk_to_s3(self.get_current_filename())

        except Exception as e:
            print(f"\nâŒ ×©×’×™××” ×‘×¢×™×‘×•×“: {e}")
            if self.current_file:
                self.current_file.close()

        # ×¡×™×›×•× ×¡×•×¤×™
        self.print_final_summary(start_time)

    def print_final_summary(self, start_time):
        """×”×“×¤×¡×ª ×¡×™×›×•× ×¡×•×¤×™"""
        total_time = time.time() - start_time

        print("\n" + "=" * 60)
        print("ğŸ‰ ×¢×™×‘×•×“ ×”×•×©×œ×!")
        print("ğŸ“Š ×¡×˜×˜×™×¡×˜×™×§×•×ª ×¡×•×¤×™×•×ª:")
        print(f"   â±ï¸ ×–××Ÿ ×›×•×œ×œ: {total_time / 3600:.1f} ×©×¢×•×ª")
        print(f"   ğŸ“– ×“×¤×™× ×©× ×¡×¨×§×•: {self.total_scanned:,}")
        print(f"   âœ… ×¢×¨×›×™× ×©×¢×•×‘×“×•: {self.total_processed:,}")
        print(f"   ğŸ“ˆ ×©×™×¢×•×¨ ×”×¦×œ×—×”: {(self.total_processed / self.total_scanned) * 100:.2f}%")
        print(f"   ğŸ“¦ chunks ×©× ×•×¦×¨×•: {len(self.uploaded_files)}")
        print(f"   âš¡ ××”×™×¨×•×ª ×¢×™×‘×•×“: {self.total_processed / (total_time / 3600):.0f} ×¢×¨×›×™×/×©×¢×”")

        print(f"\nâ˜ï¸ ×§×‘×¦×™× ×‘-S3:")
        for i, s3_key in enumerate(self.uploaded_files, 1):
            print(f"   {i:2d}. s3://{self.s3_bucket}/{s3_key}")

        print(f"\nâœ… ×”×“××˜××¡×˜ ×–××™×Ÿ ×‘: s3://{self.s3_bucket}/{self.s3_prefix}")


def main():
    """×”×¤×¢×œ×” ×¨××©×™×ª"""
    print("ğŸ¯ ×¢×™×‘×•×“ ××œ× ×©×œ ×•×™×§×™×¤×“×™×” ×œ-JSONL + ×”×¢×œ××” ×œ-S3")
    print("=" * 60)

    # ×”×’×“×¨×•×ª
    dump_path = r'C:\Users\Dan Revital\OneDrive\Documents\gepeta\data\wikipedia\hewiki-latest-pages-articles.xml.bz2'
    s3_bucket = 'gepeta-datasets'
    s3_prefix = 'processed/wikipedia/'
    chunk_size = 50000

    # ×™×¦×™×¨×ª ××¢×‘×“
    processor = FullWikipediaProcessor(
        dump_path=dump_path,
        s3_bucket=s3_bucket,
        s3_prefix=s3_prefix,
        chunk_size=chunk_size
    )

    # ×‘×“×™×§×ª ×§×™×©×•×¨×™×ª S3
    try:
        processor.s3_client.head_bucket(Bucket=s3_bucket)
        print(f"âœ… ×§×™×©×•×¨×™×ª S3 ×ª×§×™× ×”: s3://{s3_bucket}")
    except Exception as e:
        print(f"âŒ ×‘×¢×™×” ×‘×§×™×©×•×¨×™×ª S3: {e}")
        return

    # ×”×¤×¢×œ×ª ×”×¢×™×‘×•×“
    print(f"\nğŸš€ ××ª×—×™×œ ×¢×™×‘×•×“...")
    processor.process_full_dump()


if __name__ == "__main__":
    main()