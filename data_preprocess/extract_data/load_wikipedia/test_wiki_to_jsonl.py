#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import xml.etree.ElementTree as ET
import mwparserfromhell
import json
import re
import bz2
from pathlib import Path
from tqdm import tqdm


class WikipediaJSONLProcessor:
    def __init__(self, dump_path, output_file, max_articles=100):
        self.dump_path = dump_path
        self.output_file = output_file
        self.max_articles = max_articles
        self.processed_articles = []

    def normalize_text_for_training(self, text):
        """
        × ×¨××•×œ ×˜×§×¡×˜ (×–×”×” ×œ×—×œ×•×˜×™×Ÿ ×œ×ª×•×›× ×™×ª ×”××“×’×)
        """
        if not text or not isinstance(text, str):
            return text

        # ×ª×™×§×•×Ÿ ×ª×•×•×™ ×‘×¨×™×—×” ×œ×¤× ×™ ×”×›×œ
        text = text.replace('\\"', '"')
        text = text.replace("\\'", "'")
        text = text.replace('\\\\', '\\')

        # ×ª×™×§×•×Ÿ ×’× ×•×¨×™××¦×™×•×ª ×©×œ ×ª×•×•×™ ×‘×¨×™×—×”
        text = text.replace('&quot;', '"')
        text = text.replace('&#34;', '"')
        text = text.replace('&#39;', "'")

        # ××—×§ ×©×•×¨×•×ª ×—×“×©×•×ª ×•-\r
        text = text.replace('\n', ' ')
        text = text.replace('\r', ' ')
        text = text.replace('\t', ' ')

        # ××—×§ ×ª×•×•×™ ×‘×¨×™×—×” ×©× ×©××¨×•
        text = text.replace('\\n', ' ')
        text = text.replace('\\t', ' ')
        text = text.replace('\\r', ' ')

        # ×¨×•×•×—×™× ××¨×•×‘×™× â†’ ×¨×•×•×— ×™×—×™×“
        text = re.sub(r'\s+', ' ', text)

        # × ×§×” ×¨×•×•×—×™× ×‘×”×ª×—×œ×”/×¡×•×£
        text = text.strip()

        return text

    def convert_wiki_tables_to_lists(self, content):
        """
        ×”××¨×ª ×˜×‘×œ××•×ª ×•×™×§×™ ×œ×¨×©×™××•×ª × ×§×™×•×ª
        """

        def process_single_table(match):
            table_content = match.group(0)

            # ×”×¡×¨×ª ×ª×’×™×ª ×”×¤×ª×™×—×” ×•×”×¡×’×™×¨×”
            table_inner = table_content[2:-2]  # ×”×¡×¨ {| ×• |}

            # ×¤×™×¦×•×œ ×œ×©×•×¨×•×ª
            rows = re.split(r'\|-', table_inner)

            items = []

            for row in rows:
                row = row.strip()
                if not row:
                    continue

                # ×”×¡×¨×ª ×¡×’× ×•× ×•×ª CSS ××”×©×•×¨×” ×”×¨××©×•× ×”
                row = re.sub(r'style="[^"]*"', '', row)
                row = re.sub(r'cellspacing="[^"]*"', '', row)
                row = re.sub(r'cellpadding="[^"]*"', '', row)
                row = re.sub(r'\|\s*\d+px', '', row)  # ×”×¡×¨ ××™×“×•×ª ×›××• 96px

                # ×¤×™×¦×•×œ ×ª××™× - ×™×›×•×œ ×œ×”×™×•×ª || ××• |
                cells = re.split(r'\|\||\|', row)

                for cell in cells:
                    cell = cell.strip()
                    if cell and not re.match(r'^\d+px$', cell):  # ×œ× ×ª× ×©×œ ××™×“×•×ª
                        # × ×§×” ×¨×•×•×—×™× ×•×¡×™×× ×™× ××™×•×ª×¨×™×
                        cell = re.sub(r'^[\|\s]+', '', cell)
                        cell = re.sub(r'[\|\s]+$', '', cell)
                        if cell and len(cell) > 1:  # ×¨×§ ×ª××™× ××©××¢×•×ª×™×™×
                            items.append(cell)

            # ×× × ××¦××• ×¤×¨×™×˜×™×, ×”××¨ ×œ×¨×©×™××”
            if items:
                # × ×¡×” ×œ×–×”×•×ª ×× ×™×© ×›×•×ª×¨×ª ×”×’×™×•× ×™×ª
                if len(items) >= 3:
                    result = "## × ×•×©××™× ×§×©×•×¨×™×:\n"
                    for item in items:
                        result += "- " + item + "\n"
                    return result
                else:
                    # ×× ×™×© ××¢×˜ ×¤×¨×™×˜×™×, ×¨×§ ×¨×©×™××” ×¤×©×•×˜×”
                    return " ".join(items)

            return ""  # ×× ×œ× × ××¦× ×ª×•×›×Ÿ ××©××¢×•×ª×™

        # ××¦×™××” ×•×”×—×œ×¤×” ×©×œ ×›×œ ×”×˜×‘×œ××•×ª
        table_pattern = r'\{\|.*?\|\}'
        cleaned_content = re.sub(table_pattern, process_single_table, content, flags=re.DOTALL)

        return cleaned_content

    def apply_regex_cleaning(self, content):
        """
        × ×™×§×•×™ regex (×¢× ×ª×•×¡×¤×ª ×˜×™×¤×•×œ ×‘×˜×‘×œ××•×ª)
        """
        # ×”××¨×ª ×˜×‘×œ××•×ª ×•×™×§×™ ×œ×¨×©×™××•×ª - ×œ×¤× ×™ × ×™×§×•×™×™× ××—×¨×™×
        content = self.convert_wiki_tables_to_lists(content)

        # ×”×¡×¨×ª ×ª×‘× ×™×•×ª ×©× ×©××¨×•
        content = re.sub(r'\{\{[^}]*\}\}', '', content)
        # ×”×¡×¨×ª ×§×™×©×•×¨×™× ×©× ×©××¨×•
        content = re.sub(r'\[\[[^]]*\]\]', '', content)
        # ×”×¡×¨×ª ×ª×’×™×•×ª HTML
        content = re.sub(r'<[^>]*>', '', content)

        # ×”×¡×¨×ª ×ª×™××•×¨×™ ×ª××•× ×•×ª ×•××“×™×”
        content = re.sub(r'^(×©×××œ|×™××™×Ÿ|××¨×›×–|×××•×–×¢×¨)\|.*$', '', content, flags=re.MULTILINE)
        content = re.sub(r'^\s*(×©×××œ|×™××™×Ÿ|××¨×›×–|×××•×–×¢×¨)\|.*$', '', content, flags=re.MULTILINE)

        # ×”×¡×¨×ª ×”×¡×‘×¨×™ ×©×¤×•×ª ×–×¨×•×ª ×‘×¡×•×’×¨×™×™×
        foreign_languages = ['×‘×’×¨×× ×™×ª', '×‘×”×•× ×’×¨×™×ª', '×‘×¢×¨×‘×™×ª', '×‘×›×•×¨×“×™×ª', '×‘×× ×’×œ×™×ª',
                             '×‘×¦×¨×¤×ª×™×ª', '×‘××™×˜×œ×§×™×ª', '×‘×¨×•×¡×™×ª', '×‘×™×•×•× ×™×ª', '×‘×œ×˜×™× ×™×ª']
        for lang in foreign_languages:
            pattern = r'\(' + lang + r':.*?\)'
            content = re.sub(pattern, '', content)

        # ×”×¡×¨×ª ×”×¤× ×™×•×ª ×œ×ª××•× ×•×ª
        content = re.sub(r'×¨××• [A-Za-z\s,]+\.', '', content)

        # × ×™×§×•×™ ×›×œ×œ×™
        content = re.sub(r'\n{3,}', '\n\n', content)  # ×©×•×¨×•×ª ×¨×™×§×•×ª ××¨×•×‘×•×ª
        content = re.sub(r' {2,}', ' ', content)  # ×¨×•×•×—×™× ××¨×•×‘×™×
        content = re.sub(r'^\s*=+.*?=+\s*$', '', content, flags=re.MULTILINE)  # ×›×•×ª×¨×•×ª ×•×™×§×™

        return content

    def identify_headers(self, content):
        """
        ×–×™×”×•×™ ×•×¡×™××•×Ÿ ×›×•×ª×¨×•×ª (×–×”×” ×œ×—×œ×•×˜×™×Ÿ ×œ×ª×•×›× ×™×ª ×”××“×’×)
        """
        lines = content.split('\n')
        processed_lines = []

        for i, line in enumerate(lines):
            line = line.strip()

            # ×–×™×”×•×™ ×›×•×ª×¨×ª ×¤×©×•×˜
            if (line and len(line) < 100 and
                    i < len(lines) - 1 and
                    len(lines[i + 1].strip()) > 50 and
                    line.count('.') <= 1 and
                    line.count(',') <= 2):

                # ×‘×“×™×§×•×ª × ×•×¡×¤×•×ª ×œ×•×•×“× ×©×–×• ×›×•×ª×¨×ª
                header_keywords = ['×”×™×¡×˜×•×¨×™×”', '×‘×™×•×’×¨×¤×™×”', '×¨×§×¢', '×ª×•×œ×“×•×ª', '××•×¦×', '×ª×¨×‘×•×ª', '××©×¤×—×ª×•', '×™×œ×“×•×ª×•',
                                   '× ×¢×•×¨×™×•', '×”×ª×¤×ª×—×•×ª', '××©×™××•×ª']
                if (not line.endswith('.') or
                        any(keyword in line for keyword in header_keywords)):
                    processed_lines.append("## " + line)
                else:
                    processed_lines.append(line)
            else:
                processed_lines.append(line)

        return '\n'.join(processed_lines)

    def clean_content(self, wikicode):
        """
        × ×™×§×•×™ ×ª×•×›×Ÿ (×–×”×” ×œ×—×œ×•×˜×™×Ÿ ×œ×ª×•×›× ×™×ª ×”××“×’×)
        """
        try:
            # ×”×¡×¨×ª ×ª×‘× ×™×•×ª
            templates_to_remove = []
            for template in wikicode.filter_templates():
                template_name = str(template.name).strip().lower()
                # ×ª×‘× ×™×•×ª ×œ×”×¡×¨×” ××œ××”
                remove_patterns = [
                    'cite', '×¦-', '×”×¢×¨×”', '××§×•×¨', 'reflist', '××§×•×¨×•×ª',
                    '×¦×™×•×Ÿ', 'ref', 'citation', 'web', 'news', 'book', 'journal'
                ]
                if any(pattern in template_name for pattern in remove_patterns):
                    templates_to_remove.append(template)

            # ×”×¡×¨×ª ×”×ª×‘× ×™×•×ª
            for template in templates_to_remove:
                try:
                    wikicode.remove(template)
                except:
                    pass

            # ×”××¨×ª ×§×™×©×•×¨×™× ×¤× ×™××™×™× ×œ×˜×§×¡×˜
            for link in wikicode.filter_wikilinks():
                try:
                    if link.text:
                        wikicode.replace(link, str(link.text))
                    else:
                        title = str(link.title)
                        if '|' in title:
                            title = title.split('|')[0]
                        wikicode.replace(link, title)
                except:
                    pass

            # ×”×¡×¨×ª ×ª×’×™×•×ª
            for tag in wikicode.filter_tags():
                try:
                    if tag.tag.lower() in ['math', 'chem']:
                        # ×–×”×” ×‘×“×™×•×§ ×œ×ª×•×›× ×™×ª ×”××“×’× - ×œ×œ× .strip()
                        wikicode.replace(tag, "[× ×•×¡×—×”: " + str(tag.contents) + "]")
                    elif tag.tag.lower() in ['ref', 'references']:
                        wikicode.remove(tag)
                except:
                    pass

            # ×”××¨×ª ×”×›×œ ×œ×˜×§×¡×˜
            content = str(wikicode.strip_code())

            # × ×™×§×•×™ regex
            content = self.apply_regex_cleaning(content)

            # ×–×™×”×•×™ ×›×•×ª×¨×•×ª
            content = self.identify_headers(content)

            # × ×¨××•×œ ×”×˜×§×¡×˜ ×œ××™××•×Ÿ
            content = self.normalize_text_for_training(content)

            return content.strip()

        except Exception as e:
            print("Error in clean_content: " + str(e))
            # ×‘××§×¨×” ×©×œ ×©×’×™××”
            basic_clean = str(wikicode)[:2000]
            return self.normalize_text_for_training(basic_clean)

    def count_words(self, text):
        """
        ×¡×¤×™×¨×ª ××™×œ×™×
        """
        if not text:
            return 0
        # ×¡×¤×™×¨×ª ××™×œ×™× ×¤×©×•×˜×” - ×—×œ×•×§×” ×œ×¤×™ ×¨×•×•×—×™×
        words = text.split()
        return len(words)

    def count_bytes(self, text):
        """
        ×¡×¤×™×¨×ª ×‘×™×™×˜×™× ×‘-UTF-8
        """
        if not text:
            return 0
        return len(text.encode('utf-8'))

    def process_article(self, title, raw_wikitext):
        """
        ×¢×™×‘×•×“ ×¢×¨×š ×™×—×™×“ ×œ×™×¦×™×¨×ª ×¤×¨×™×˜ JSONL
        """
        try:
            # ×¢×™×‘×•×“ ×”×•×™×§×™-×˜×§×¡×˜
            wikicode = mwparserfromhell.parse(raw_wikitext)
            processed_text = self.clean_content(wikicode)

            # ×•×™×“×•× ×©×”×˜×§×¡×˜ ×œ× ×¨×™×§ ××“×™
            if not processed_text or len(processed_text) < 100:
                return None

            # ×™×¦×™×¨×ª ×¤×¨×™×˜ JSONL
            article_item = {
                "text": processed_text,
                "word_count": self.count_words(processed_text),
                "byte_count": self.count_bytes(processed_text)
            }

            return article_item

        except Exception as e:
            print("Error processing article '" + title + "': " + str(e))
            return None

    def is_valid_article(self, page_elem):
        """
        ×‘×“×™×§×” ×× ×”×“×£ ×ª×§×™×Ÿ ×œ×¢×™×‘×•×“ (×–×”×” ×œ×ª×•×›× ×™×ª ×”××“×’×)
        """
        # ×¨×§ namespace 0 (×¢×¨×›×™× ×¨××©×™×™×)
        namespace_elem = page_elem.find('.//{*}ns')
        if namespace_elem is None or namespace_elem.text != '0':
            return False

        # ×‘×“×™×§×ª ×ª×•×›×Ÿ ×‘×¡×™×¡×™×ª
        revision = page_elem.find('.//{*}revision')
        if revision is None:
            return False

        text_elem = revision.find('.//{*}text')
        if text_elem is None or not text_elem.text:
            return False

        # ×‘×“×™×§×ª ××•×¨×š ××™× ×™××œ×™
        if len(text_elem.text) < 500:
            return False

        # ×“×™×œ×•×’ ×¢×œ ×”×¤× ×™×•×ª
        if text_elem.text.strip().startswith('#REDIRECT'):
            return False

        return True

    def process_dump(self):
        """
        ×¢×™×‘×•×“ ×”×“×××¤ ×•×›×ª×™×‘×” ×œ-JSONL
        """
        print("ğŸš€ ××ª×—×™×œ ×¢×™×‘×•×“ ×•×™×§×™×¤×“×™×” ×œ-JSONL")
        print("ğŸ“‚ ×“×××¤: " + self.dump_path)
        print("ğŸ’¾ ×¤×œ×˜: " + self.output_file)
        print("ğŸ¯ ××˜×¨×”: " + str(self.max_articles) + " ×¢×¨×›×™×")
        print("=" * 60)

        processed_count = 0
        scanned_count = 0

        # ×¤×ª×™×—×ª ×§×•×‘×¥ ×”×¤×œ×˜
        with open(self.output_file, 'w', encoding='utf-8') as output:

            # ×¢×™×‘×•×“ ×”×“×××¤
            with bz2.open(self.dump_path, 'rt', encoding='utf-8') as dump_file:

                with tqdm(total=self.max_articles, desc="ğŸ”„ ×¢×™×‘×•×“ ×¢×¨×›×™×", unit="articles") as pbar:

                    for event, elem in ET.iterparse(dump_file, events=('start', 'end')):
                        if event == 'end' and elem.tag.endswith('page'):
                            scanned_count += 1

                            # ×”×“×¤×¡×ª ×”×ª×§×“××•×ª ×›×œ 1000 ×“×¤×™×
                            if scanned_count % 1000 == 0:
                                pbar.set_postfix_str(
                                    "×¡×¨×§×ª×™: " + str(scanned_count) + ", ×¢×™×‘×“×ª×™: " + str(processed_count))

                            # ×‘×“×™×§×” ×× ×”×“×£ ×ª×§×™×Ÿ
                            if self.is_valid_article(elem):

                                # ×—×™×œ×•×¥ ××™×“×¢
                                title_elem = elem.find('.//{*}title')
                                revision = elem.find('.//{*}revision')
                                text_elem = revision.find('.//{*}text')

                                title = title_elem.text if title_elem is not None else ""
                                raw_wikitext = text_elem.text if text_elem is not None else ""

                                # ×¢×™×‘×•×“ ×”×¢×¨×š
                                article_item = self.process_article(title, raw_wikitext)

                                if article_item:
                                    # ×›×ª×™×‘×” ×œ-JSONL
                                    json.dump(article_item, output, ensure_ascii=False)
                                    output.write('\n')

                                    processed_count += 1
                                    pbar.update(1)
                                    pbar.set_postfix_str("× ×•×›×—×™: " + title[:30] + "...")

                                    # ×”×¤×¡×§×” ×× ×”×’×¢× ×• ×œ××˜×¨×”
                                    if processed_count >= self.max_articles:
                                        break

                            elem.clear()

                            # ×”×¤×¡×§×” ×× ×”×’×¢× ×• ×œ××˜×¨×”
                            if processed_count >= self.max_articles:
                                break

        # ×¡×™×›×•×
        print("\n" + "=" * 60)
        print("ğŸ‰ ×¢×™×‘×•×“ ×”×•×©×œ×!")
        print("ğŸ“Š ×¡×˜×˜×™×¡×˜×™×§×•×ª:")
        print("   ×“×¤×™× ×©× ×¡×¨×§×•: " + str(scanned_count))
        print("   ×¢×¨×›×™× ×©×¢×•×‘×“×•: " + str(processed_count))
        print("   ×©×™×¢×•×¨ ×”×¦×œ×—×”: " + str(round((processed_count / scanned_count) * 100, 2)) + "%")

        # × ×™×ª×•×— ×”×§×•×‘×¥ ×©× ×•×¦×¨
        self.analyze_output()

    def analyze_output(self):
        """
        × ×™×ª×•×— ×”×§×•×‘×¥ ×©× ×•×¦×¨
        """
        print("\nğŸ“Š × ×™×ª×•×— ×”×§×•×‘×¥: " + self.output_file)
        print("-" * 40)

        total_items = 0
        total_words = 0
        total_bytes = 0
        word_counts = []
        byte_counts = []

        try:
            with open(self.output_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        item = json.loads(line)
                        total_items += 1
                        total_words += item['word_count']
                        total_bytes += item['byte_count']
                        word_counts.append(item['word_count'])
                        byte_counts.append(item['byte_count'])

            print("ğŸ“ ×¡×”\"×› ×¢×¨×›×™×: " + str(total_items))
            print("ğŸ“ ×¡×”\"×› ××™×œ×™×: " + str(total_words))
            print("ğŸ’¾ ×¡×”\"×› ×‘×™×™×˜×™×: " + str(total_bytes) + " (" + str(round(total_bytes / 1024 / 1024, 2)) + " MB)")

            if word_counts:
                print("ğŸ“Š ×××•×¦×¢ ××™×œ×™× ×œ×¢×¨×š: " + str(round(sum(word_counts) / len(word_counts), 1)))
                print("ğŸ“Š ×××•×¦×¢ ×‘×™×™×˜×™× ×œ×¢×¨×š: " + str(round(sum(byte_counts) / len(byte_counts), 1)))
                print("ğŸ“Š ×¢×¨×š ×”×§×¦×¨ ×‘×™×•×ª×¨: " + str(min(word_counts)) + " ××™×œ×™×")
                print("ğŸ“Š ×¢×¨×š ×”××¨×•×š ×‘×™×•×ª×¨: " + str(max(word_counts)) + " ××™×œ×™×")

            # ×“×•×’××” ××”×§×•×‘×¥
            print("\nğŸ“„ ×“×•×’××” ××”×§×•×‘×¥:")
            with open(self.output_file, 'r', encoding='utf-8') as f:
                first_line = f.readline()
                if first_line:
                    example = json.loads(first_line)
                    print("   ××™×œ×™×: " + str(example['word_count']))
                    print("   ×‘×™×™×˜×™×: " + str(example['byte_count']))
                    print("   ×˜×§×¡×˜ (100 ×ª×•×•×™×): " + example['text'][:100] + "...")

        except Exception as e:
            print("âŒ ×©×’×™××” ×‘× ×™×ª×•×—: " + str(e))


def main():
    """
    ×”×¤×¢×œ×” ×¨××©×™×ª
    """
    print("ğŸ¯ ×¢×™×‘×•×“ ×“×’×™××” ×©×œ ×•×™×§×™×¤×“×™×” ×œ-JSONL")
    print("=" * 60)

    # ×”×’×“×¨×•×ª
    dump_path = r'C:\Users\Dan Revital\OneDrive\Documents\gepeta\data\wikipedia\hewiki-latest-pages-articles.xml.bz2'
    output_file = 'wikipedia_he_sample_100.jsonl'
    max_articles = 100

    # ×™×¦×™×¨×ª ××¢×‘×“
    processor = WikipediaJSONLProcessor(
        dump_path=dump_path,
        output_file=output_file,
        max_articles=max_articles
    )

    # ×”×¤×¢×œ×ª ×”×¢×™×‘×•×“
    processor.process_dump()

    print("\nâœ… ×”×§×•×‘×¥ × ×•×¦×¨: " + output_file)
    print("ğŸ’¡ ××¤×©×¨ ×œ×‘×“×•×§ ××•×ª×• ×¢×:")
    print("   head -n 3 " + output_file)


if __name__ == "__main__":
    main()