{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "from google import genai\n",
    "from google.genai.types import CreateBatchJobConfig\n",
    "from google.cloud import storage\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from logging.handlers import RotatingFileHandler\n",
    "import logging\n",
    "import boto3\n",
    "import fsspec\n",
    "import jsonlines\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
    "\n",
    "print(PROJECT_ID, LOCATION)\n",
    "\n",
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching for gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "bucket = storage_client.bucket(os.getenv(\"BUCKET_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define jsonl filaname and where it willl be stored locally\n",
    "JSON_FILENAME = \"tau_ocr_batch1.jsonl\"\n",
    "jsonl_fn = f\"outputs/gcp_batches/{JSON_FILENAME}\"\n",
    "\n",
    "# model to use for \"OCR\"\n",
    "MODEL_ID = \"gemini-2.5-flash\"\n",
    "\n",
    "# prefix in the gcp bucket where the pages are stored\n",
    "prefix = \"data_HQ_wOcr_img_chunks/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by dsparse\n",
    "system_prompt = None\n",
    "prompt = \"\"\"\n",
    "You are a PDF -> MD file parser. Your task is to analyze the provided PDF page (provided as an image) and return a structured JSON response containing all of the elements on the page. Each element must be represented using Markdown formatting.\n",
    "\n",
    "There are three categories of elements you need to identify: \"Text\", \"Table\" and \"Figure\". \n",
    "- Text and table elements are those that can be accurately represented using Markdown. For such elements, you must provide the exact text content. \n",
    "- Figure elements are those that need to be represented as images to fully capture their content. For Figure elements, you must provide a detailed description of the content in *Hebrew*.\n",
    "\n",
    "**Crucially, page numbers, often found in headers or footers, are considered document metadata and should be explicitly excluded from *any* extracted content for *any* element type.**\n",
    "\n",
    "Every element on the page should be classified as one of these types. There should be no overlap between elements. You should use the smallest number of elements possible while still accurately representing and categorizing the content on the page. For example, if the page contains a couple paragraphs of text, followed by a large figure, followed by a few more paragraphs of text, you should use three elements: Text, Figure, and Text. With that said, you should never combine two different types of elements into a single element.\n",
    "\n",
    "**Key Instruction for Footnotes:**\n",
    "**Footnotes must adhere to the standard Markdown footnote syntax.** This means:\n",
    "1. For references within the main body of text, use the format `[^N]` (e.g., `[^1]`, `[^2]`), where `N` is an integer sequentially assigned to that footnote on the page.\n",
    "2. For the footnote definition itself, found typically at the bottom of the page, use the format `[^N]: <full footnote text>`.\n",
    "3. The `N` in the reference `[^N]` and its corresponding definition `[^N]:` *must* match. **Footnote numbering should reflect the original numbering from the document, which may be continuous across pages rather than restarting each page. Therefore, transcribe the footnote number precisely as it appears in the PDF content on the current page.** Do not attempt to re-number or infer a sequence if the original numbering is already present.\n",
    "\n",
    "Here are detailed descriptions of the element types you can use:\n",
    "- Text: This is the main text content of the page, including paragraphs, lists, titles, and any other text content that is not part of one of the other more specialized element types. Not all pages have narrative text, but most do. Be sure to use Markdown formatting for the text content. This includes using tags like # for headers, * for lists, etc. Make sure your header tags are properly nested and that your lists are properly formatted.\n",
    "    When a footnote reference appears in the text, transcribe it using the Markdown `[^N]` format as described above.\n",
    "    When encountering a Table of Contents (TOC), transcribe each entry as a list item. For each entry, include only the main text of the entry and its corresponding page number. Explicitly omit any connecting leader characters (e.g., dots, dashes, spaces) that typically appear between the entry text and the page number (e.g., \"Chapter 1 Introduction 1\" not \"Chapter 1 Introduction . . . . . . . . . 1\").\n",
    "- Table: This covers any tabular data arrangement on the page, including simple and complex tables. Any titles, captions, or notes associated with the table should be considered part of the table element.\n",
    "- Figure: This covers charts, graphs, diagrams, etc. Associated titles, legends, axis titles, etc. should be considered to be part of the figure.\n",
    "- Footnote: This covers any explanatory notes typically found at the bottom of the page, marked by a reference in the main text. Their purpose is to provide supplementary information or citations for specific points in the main text. Include the footnote marker (e.g., *, ยน, a) along with its associated text content. Markdown formatting should be used for the footnote's text. Footnotes should be treated as distinct elements from the main 'Text' content.\n",
    "    For the content of a `Footnote` element, you *must* use the Markdown definition syntax `[^N]: <full footnote text>`. *Ensure `N` matches the corresponding reference in the main document*.\n",
    "    Crucially, if a footnote contains structured data (e.g., citations with multiple fields, statistical breakdowns, short lists, key-value pairs), you must represent this data using appropriate Markdown structures within the `<full footnote text>` part. Prioritize Markdown lists (ordered or unordered), simple Markdown tables, or inline code blocks/fenced code blocks if the content warrants it.\n",
    "\n",
    "For Figure elements, you must provide a detailed description of the element in the \"content\" field. Do not just transcribe the actual text contained in the element. For textual elements (Text, Table), you must provide the exact text content of the element in the Markdown format.\n",
    "\n",
    "Output format\n",
    "- Your output should be an ordered (from top to bottom) list of elements on the page, where each element is a dictionary with the following keys:\n",
    "    - type: str - the type of the element\n",
    "    - content: str - the content of the element. For Figure elements, this should be a detailed description of the visual content, rather than a transcription of the actual text contained in the element. You can use Markdown formatting for text content.\n",
    "\n",
    "Complex and multi-part figures or images should be represented as a single element. For example, if a figure consists of a main chart and a smaller inset chart, these should be described together in a single Figure element. If there are two separate graphs side by side, these should be represented as a single Figure element with a bounding box that encompasses both graphs. DO NOT create separate elements for each part of a complex figure or image.\n",
    "\"\"\"\n",
    "\n",
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"type\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"Text\", \"Table\", \"Figure\", \"Footnote\"]\n",
    "            },\n",
    "            \"content\": {\n",
    "                \"type\": \"string\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"type\", \"content\"],\n",
    "    } \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generationConfig = {\"temperature\": 0, \"maxOutputTokens\": 8192}\n",
    "if response_schema:\n",
    "    generationConfig[\"response_schema\"] = response_schema\n",
    "    generationConfig[\"response_mime_type\"] = \"application/json\"\n",
    "if MODEL_ID==\"gemini-2.5-flash\":\n",
    "    generationConfig[\"thinkingConfig\"] = {\"thinkingBudget\": 0}\n",
    "print(generationConfig)\n",
    "\n",
    "print(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the jsonl locally\n",
    "blobs = bucket.list_blobs(prefix=prefix)\n",
    "\n",
    "counter = 0\n",
    "with jsonlines.open(jsonl_fn, mode=\"w\") as writer:\n",
    "    for blob in tqdm(blobs):\n",
    "        # if not blob.name.lower().endswith('.pdf'): continue\n",
    "        if not blob.name.lower().endswith('.png'): continue\n",
    "        blob_id = blob.id\n",
    "        blob_generation = str(blob.generation)\n",
    "        blob_uri = \"gs://\" + blob_id.strip(\"/\"+blob_generation)\n",
    "        chunk_id = int(blob_uri.strip(\".png\").split(\"_chunk_\")[1])\n",
    "        gem_request = {\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": prompt}, \n",
    "                                                                            {\"file_data\": {\"file_uri\": blob_uri, \n",
    "                                                                                        \"mime_type\": \"image/png\"\n",
    "                                                                                           }}]}],\n",
    "                                    \"generationConfig\":generationConfig}}\n",
    "        if system_prompt:\n",
    "            gem_request[\"request\"][\"system_instruction\"] = {\"parts\": [{\"text\": system_prompt}]}\n",
    "        writer.write(gem_request)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the local jsonl to the bucket \n",
    "blob = bucket.blob(JSON_FILENAME)\n",
    "blob.upload_from_filename(jsonl_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the variable definitions for passing to the batch API\n",
    "INPUT_DATA = f\"gs://{os.getenv(\"BUCKET_NAME\")}/{JSON_FILENAME}\"  \n",
    "DEST_BUKCET_URI = \"gs://tau_ocr_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the batch\n",
    "gcs_batch_job = client.batches.create(\n",
    "    model=MODEL_ID,\n",
    "    src=INPUT_DATA,\n",
    "    config=CreateBatchJobConfig(dest=DEST_BUKCET_URI)\n",
    ")\n",
    "gcs_batch_job.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally cancel the job - good for quick fixes\n",
    "# client.batches.cancel(name=gcs_batch_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally get a print once the batch succeeds\n",
    "# Alternatively check through gcp console\n",
    "gcs_batch_job = client.batches.get(name=gcs_batch_job.name)\n",
    "# Refresh the job until complete\n",
    "while not gcs_batch_job.state in [\"JOB_STATE_SUCCEEDED\", \"JOB_STATE_FAILED\", \"JOB_STATE_UNEXECUTED\"]:\n",
    "    time.sleep(5)\n",
    "    gcs_batch_job = client.batches.get(name=gcs_batch_job.name)\n",
    "\n",
    "if gcs_batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "# Check if the job succeeds\n",
    "    print(\"Job succeeded!\")\n",
    "else:\n",
    "    print(f\"Job failed: {gcs_batch_job.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if the batch fails at large, likely something is wrong with the configuration. In that case the results will have errors loading and no response_text will be available.\n",
    "\n",
    "Occasional fails, ~1%, are expected due to the model wrongly trying to parse a figure or table of contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"gemini_ocr\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create rotating file handler (max 10MB, keep 5 backups)\n",
    "handler = RotatingFileHandler(\n",
    "    './logs/text_uploading_batch.log',\n",
    "    maxBytes=10*1024*1024,  # 10MB\n",
    "    backupCount=5\n",
    ")\n",
    "\n",
    "# Create formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "# Add handler to logger\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_candidates(response):\n",
    "    if not isinstance(response, dict):\n",
    "        return {'candidates': []}\n",
    "    if 'candidates' not in response:\n",
    "        response['candidates'] = []\n",
    "    return response\n",
    "\n",
    "def extract_file_uri(request):\n",
    "    try:\n",
    "        contents = request.get('contents', [])\n",
    "        for content in contents:\n",
    "            parts = content.get('parts', [])\n",
    "            for part in parts:\n",
    "                file_data = part.get('file_data')\n",
    "                if file_data and 'file_uri' in file_data:\n",
    "                    return file_data['file_uri']\n",
    "        return None\n",
    "    except (AttributeError, TypeError):\n",
    "        return None\n",
    "    \n",
    "def extract_response_text(response):\n",
    "    try:\n",
    "        return response[0]['text']\n",
    "    except (AttributeError, TypeError):\n",
    "        return None\n",
    "def extract_source_file_from_uri(file_uri):\n",
    "    \"\"\"Extract source file name from file URI\"\"\"\n",
    "    return file_uri.split(\"/\")[-1].split(\"_chunk_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem(\"gcs\")\n",
    "\n",
    "file_paths = fs.glob(f\"{gcs_batch_job.dest.gcs_uri}/*/predictions.jsonl\")\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the batch succeeded (don't forget to read the latest status)\n",
    "# store only the important parts of the result\n",
    "# within dictionary\n",
    "if gcs_batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    logger.info(\"Accumulating all requests by source file...\")\n",
    "    \n",
    "    # Dictionary to accumulate all chunks for each file\n",
    "    file_accumulator = defaultdict(list)\n",
    "    processed_count = 0\n",
    "    \n",
    "    # Stream through the JSONL file and accumulate by source file\n",
    "    chunk_reader = pd.read_json(f\"gs://{file_paths[-1]}\", lines=True, chunksize=1_000)\n",
    "    \n",
    "    for chunk_df in chunk_reader:\n",
    "        logger.info(f\"Processing batch with {len(chunk_df)} records...\")\n",
    "        \n",
    "        # Process each row in this chunk\n",
    "        for _, row in chunk_df.iterrows():\n",
    "            try:\n",
    "                file_uri = extract_file_uri(row['request'])\n",
    "                if file_uri == \"gs://tau_ocr/data_HQwOcr_img_chunks\":\n",
    "                    continue\n",
    "                    \n",
    "                source_file = extract_source_file_from_uri(file_uri)\n",
    "                \n",
    "                # Extract chunk ID for proper ordering later\n",
    "                chunk_id = int(file_uri.strip(\".png\").split('_chunk_')[1]) if '_chunk_' in file_uri else 0\n",
    "                \n",
    "                # Process the response immediately to extract just the text\n",
    "                try:\n",
    "                    response = ensure_candidates(row['response'])\n",
    "                    normalized = pd.json_normalize([response], \"candidates\")\n",
    "                    response_text = extract_response_text(normalized.iloc[0][\"content.parts\"])\n",
    "                    \n",
    "                    # Store only the essentials: filename, chunk_id, and processed text\n",
    "                    file_accumulator[source_file].append({\n",
    "                        'chunk_id': chunk_id,\n",
    "                        'text': response_text\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.info(f\"Error processing response for {source_file}, chunk {chunk_id}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                processed_count += 1\n",
    "                if processed_count % 10000 == 0:\n",
    "                    logger.info(f\"Processed {processed_count} records, tracking {len(file_accumulator)} files\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.info(f\"Error processing row: {e}\")\n",
    "                continue\n",
    "    \n",
    "    logger.info(f\"Finished accumulating. Found {len(file_accumulator)} unique files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving full docs locally within jsonl\n",
    "\n",
    "import json\n",
    "\n",
    "results_fn = \"./outputs/data_HQ_WOcr_text_batch.jsonl\"\n",
    "with jsonlines.open(results_fn, mode=\"w\") as writer:\n",
    "    for key, text in file_accumulator.items():\n",
    "        \n",
    "        text.sort(key=lambda x: x[\"chunk_id\"])\n",
    "\n",
    "        final_text = \"\"\n",
    "        for ele_type,ele in enumerate(text):\n",
    "            try:\n",
    "                texts = json.loads(ele[\"text\"])\n",
    "                for el in texts:\n",
    "                    if el[\"type\"] == \"Table\":\n",
    "                        final_text += \"<MARKDOWN_TABLE>\" + el[\"content\"] + \"</MARKDOWN_TABLE>\" + \"\\n\"\n",
    "                    else: \n",
    "                        final_text += el[\"content\"]+\"\\n\"\n",
    "                final_text += \"\\n\\n\"\n",
    "            except:\n",
    "                logger.info(f\"Couldn't load row {ele[\"chunk_id\"]} for {key}\")\n",
    "\n",
    "        entry = {\"filename\": key, \"text\": final_text}\n",
    "        writer.write(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_key = 'tau_clean/HQ_WOcr_Markdown_Tables_batch3.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the jsonl with results to s3\n",
    " \n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "try:\n",
    "    s3_client.upload_file(\n",
    "        Filename=results_fn,  # local file path\n",
    "        Bucket=os.getenv(\"AWS_BUCKET_NAME\"),\n",
    "        Key=s3_key  # S3 object key (path in bucket)\n",
    "    )\n",
    "    logger.info(\"Upload to s3 was successful\")\n",
    "except ClientError as e:\n",
    "    logger.info(f\"Upload to s3 failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up the batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.batches.delete(name=gcs_batch_job.name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
