{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from google import genai\n",
    "from google.genai.types import CreateBatchJobConfig, GenerateContentConfig, Part\n",
    "from google.cloud import storage\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "import csv\n",
    "import fsspec\n",
    "import json\n",
    "import jsonlines\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "raw_bucket_name = \"israllm-datasets\"\n",
    "clean_bucket_name = \"gepeta-datasets\"\n",
    "prefix = \"processed_and_cleaned\"\n",
    "n_samples = 1_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_clean_map = {\"AllHebNLI\": {\"raw\": \"raw-datasets/nli/csv_output/\", \"clean\": f\"{prefix}/AllHebNLI/\", 'source_name': 'AllHebNLI'},\n",
    "                 \"AllOfHEOscarData-Combined-Deduped-DC4.forgpt\": {\"raw\": \"raw-datasets/rar/csv_output/\", \"clean\": f\"{prefix}/AllOfHEOscarData\", 'source_name': 'AllOfHEOscarData-Combined-Deduped-DC4.forgpt'},\n",
    "                 \"AllTzenzuraData-Combined-Deduped-DC4.forgpt\": {\"raw\": \"raw-datasets/rar/csv_output/\", \"clean\": f\"{prefix}/AllTzenzuraData\", 'source_name': 'AllTzenzuraData-Combined-Deduped-DC4.forgpt'},\n",
    "                 \"BooksNLI2-Combined-Deduped.forgpt\": {\"raw\": \"raw-datasets/rar/csv_output/\", \"clean\": f\"{prefix}/BooksNLI2\", 'source_name': 'BooksNLI2-Combined-Deduped.forgpt'},\n",
    "                 \"GeektimeCorpus-Combined-Deduped.forgpt\": {\"raw\": \"raw-datasets/rar/csv_output/\", \"clean\": f\"{prefix}/GeektimeCorpus\", 'source_name': 'GeektimeCorpus-Combined-Deduped.forgpt'},\n",
    "                 \"hebrew_tweets_text_clean_full-Deduped.forgpt\": {\"raw\": \"raw-datasets/rar/csv_output/\", \"clean\": f\"{prefix}/hebrew_tweets\", 'source_name': 'hebrew_tweets_text_clean_full-Deduped.forgpt'},\n",
    "                 \"HeC4DictaCombined-Clean-Deduped.forgpt\": {\"raw\": \"raw-datasets/rar/csv_output/\", \"clean\": f\"{prefix}/HeC4DictaCombined\", 'source_name': 'HeC4DictaCombined-Clean-Deduped.forgpt'},\n",
    "                 \"YifatDataBatch2-Round3-Deduped.forgpt\": {\"raw\": \"raw-datasets/rar/csv_output/\", \"clean\": f\"{prefix}/YifatDataBatch2/\", 'source_name': 'YifatDataBatch2-Round3-Deduped.forgpt'},\n",
    "                 \"YifatDataRound2-Deduped.forgpt\": {\"raw\": \"raw-datasets/rar/csv_output/\", \"clean\": f\"{prefix}/YifatDataRound2\", 'source_name': 'YifatDataRound2-Deduped.forgpt'},\n",
    "                 \"YifatToCombine-Deduped.forgpt\": {\"raw\": \"raw-datasets/rar/csv_output/\", \"clean\": f\"{prefix}/YifatToCombine\", 'source_name': 'YifatToCombine-Deduped.forgpt'},\n",
    "                 \"YisraelHayomData-Combined-Deduped.forgpt\": {\"raw\": \"raw-datasets/rar/csv_output/\", \"clean\": f\"{prefix}/YisraelHayomData\", 'source_name': 'YisraelHayomData-Combined-Deduped.forgpt'},\n",
    "                 \"FineWeb2\": {\"raw\": \"raw-datasets/fineweb2\", \"clean\": f\"{prefix}/FineWeb2/\", 'source_name': 'batch'},\n",
    "                 \"HeC4\": {\"raw\": \"raw-datasets/HeC4\", \"clean\": f\"{prefix}/HeC4-HF\", 'source_name': 'part'},\n",
    "                 \"SupremeCourtOfIsrael\": {\"raw\": \"raw-datasets/SupremeCourtOfIsrael/text_extraction/\", \"clean\": f\"{prefix}/SupremeCourtOfIsrael\", 'source_name': 'batch'},\n",
    "                 \"YifatDataBatch2-Round4\": {\"raw\": \"raw-datasets/Yifat4+5/csv_output\", \"clean\": f\"{prefix}/YifatDataBatch2-Round4\", 'source_name': 'YifatDataBatch2-Round4'},\n",
    "                 \"YifatDataBatch3-Round5\": {\"raw\": \"raw-datasets/Yifat4+5/csv_output\", \"clean\": f\"{prefix}/YifatDataBatch2-Round5/\", 'source_name': 'YifatDataBatch3-Round5'},\n",
    "                 \"OcrTau\": {\"raw\": \"tau_clean/\", \"clean\": f\"{prefix}/TauOCR\", 'source_name': [\"HQ\", 'Markdown_Table']},\n",
    "                 \"TauDigital\": {\"raw\": \"tau_clean/\", \"clean\": f\"{prefix}/TauDigital\", 'source_name': 'DigitalMarkdown_Tables'},\n",
    "                 \"BIU\": {\"raw\": \"raw-datasets/biu-drive/\", \"clean\": f\"{prefix}/BIU\", 'source_name': 'AllBIUDriveDocs-MD-Deduped.forgpt'},\n",
    "                 \"sefaria\": {\"raw\": \"raw-datasets/sefaria/\", \"clean\": f\"{prefix}/sefaria\", 'source_name': 'AllOfSefariaTexts'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_clean_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_s3(bucket_name, object_key):\n",
    "    \"\"\"\n",
    "    Read CSV or JSONL file from S3 into a pandas DataFrame\n",
    "    \n",
    "    Args:\n",
    "        object_key (str): S3 object key\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3.get_object(\n",
    "            Bucket=bucket_name,\n",
    "            Key=object_key\n",
    "        )\n",
    "        \n",
    "        content = response['Body'].read().decode('utf-8')\n",
    "        \n",
    "        # Determine file type and read accordingly\n",
    "        if object_key.lower().endswith('.csv'):\n",
    "            df = pd.read_csv(StringIO(content))\n",
    "        elif object_key.lower().endswith('.jsonl'):\n",
    "            lines = content.strip().split('\\n')\n",
    "            data = [json.loads(line) for line in lines if line.strip()]\n",
    "            df = pd.DataFrame(data)\n",
    "        else:\n",
    "            print(f\"Unsupported file type for {object_key}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(f\"Successfully read {object_key} with {len(df)} rows\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {object_key}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "\n",
    "def get_random_samples(df, n_samples=1):\n",
    "        \"\"\"\n",
    "        Get a random row from the DataFrame\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            pd.Series: Random row from the DataFrame\n",
    "        \"\"\"\n",
    "        if df.empty or n_samples <= 0:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # If requesting more samples than available, return all rows\n",
    "        if n_samples >= len(df):\n",
    "            print(f\"Requested {n_samples} samples but only {len(df)} rows available\")\n",
    "            return df.copy()\n",
    "        \n",
    "        # Sample without replacement\n",
    "        return df.sample(n=n_samples, random_state=random.randint(0, 10000))\n",
    "def distribute_samples_across_files( data_objects, n_samples, distribution_method='uniform'):\n",
    "    \"\"\"\n",
    "    Determine how many samples to collect from each file\n",
    "    \n",
    "    Args:\n",
    "        data_objects (list): List of data file keys\n",
    "        n_samples (int): Total number of samples to collect\n",
    "        distribution_method (str): How to distribute samples across files\n",
    "            - 'uniform': Equal samples from each file\n",
    "            - 'random': Random allocation across files\n",
    "            - 'weighted_random': Weighted random (files can get 0 to multiple samples)\n",
    "            \n",
    "    Returns:\n",
    "        dict: Mapping of file_key -> number_of_samples\n",
    "    \"\"\"\n",
    "    if not data_objects:\n",
    "        return {}\n",
    "        \n",
    "    sample_distribution = {}\n",
    "    \n",
    "    if distribution_method == 'uniform':\n",
    "        # Distribute samples as evenly as possible across all files\n",
    "        samples_per_file = n_samples // len(data_objects)\n",
    "        remainder = n_samples % len(data_objects)\n",
    "        \n",
    "        for i, file_key in enumerate(data_objects):\n",
    "            # Give each file the base amount, plus 1 extra for the first 'remainder' files\n",
    "            sample_distribution[file_key] = samples_per_file + (1 if i < remainder else 0)\n",
    "            \n",
    "    elif distribution_method == 'random':\n",
    "        # Randomly assign each sample to a file (some files may get 0, others multiple)\n",
    "        for file_key in data_objects:\n",
    "            sample_distribution[file_key] = 0\n",
    "            \n",
    "        for _ in range(n_samples):\n",
    "            random_file = random.choice(data_objects)\n",
    "            sample_distribution[random_file] += 1\n",
    "            \n",
    "    elif distribution_method == 'weighted_random':\n",
    "        # Use random weights to distribute samples\n",
    "        weights = [random.random() for _ in data_objects]\n",
    "        total_weight = sum(weights)\n",
    "        \n",
    "        allocated = 0\n",
    "        for i, file_key in enumerate(data_objects[:-1]):  # Handle last file separately\n",
    "            file_samples = int((weights[i] / total_weight) * n_samples)\n",
    "            sample_distribution[file_key] = file_samples\n",
    "            allocated += file_samples\n",
    "        \n",
    "        # Give remaining samples to the last file\n",
    "        sample_distribution[data_objects[-1]] = n_samples - allocated\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown distribution method: {distribution_method}\")\n",
    "        \n",
    "    # Log the distribution\n",
    "    print(f\"Sample distribution ({distribution_method}):\")\n",
    "    for file_key, count in sample_distribution.items():\n",
    "        if count > 0:\n",
    "            print(f\"  {file_key}: {count} samples\")\n",
    "            \n",
    "    return sample_distribution\n",
    "def sample_dataset(dataset_name, n_samples, both=False, max_retries=3):\n",
    "    \"\"\"\n",
    "    Sample n_samples from the dataset.\n",
    "    If both is True, then return couples - from raw directory and from cleaned.\n",
    "    \"\"\"\n",
    "\n",
    "    if both:\n",
    "        raw_response = s3.list_objects_v2(Bucket=raw_bucket_name, Prefix=raw_clean_map[dataset_name][\"raw\"])\n",
    "        if \"Contents\" not in raw_response:\n",
    "            print(f\"Didn't find raw for {dataset_name}\")\n",
    "            return\n",
    "        raw_response = [ele[\"Key\"] for ele in raw_response[\"Contents\"] if raw_clean_map[dataset_name][\"source_name\"] in ele[\"Key\"]]\n",
    "\n",
    "    clean_response = s3.list_objects_v2(Bucket=clean_bucket_name, Prefix=raw_clean_map[dataset_name][\"clean\"])\n",
    "    if \"Contents\" not in clean_response:\n",
    "        print(f\"Didn't find clean for {dataset_name}\")\n",
    "        return\n",
    "    \n",
    "    if isinstance(raw_clean_map[dataset_name][\"source_name\"], list):\n",
    "        clean_response = [ele[\"Key\"] for ele in clean_response[\"Contents\"] if all(source in ele[\"Key\"] for source in raw_clean_map[dataset_name][\"source_name\"])]\n",
    "    else:\n",
    "        clean_response = [ele[\"Key\"] for ele in clean_response[\"Contents\"] if raw_clean_map[dataset_name][\"source_name\"] in ele[\"Key\"]]\n",
    "    \n",
    "    if both:\n",
    "        if len(raw_response) == len(clean_response):\n",
    "            print(f\"Same number of files for raw and clean for {dataset_name}\")\n",
    "        else:\n",
    "            print(f\"The number of files for raw and clean in {dataset_name} differs\")\n",
    "\n",
    "    # Determine sample distribution across files\n",
    "    # sample_distribution = distribute_samples_across_files(\n",
    "    #     clean_response, n_samples\n",
    "    # )\n",
    "    sample_distribution = {random.choice(clean_response): n_sample}\n",
    "    sampled_data = []\n",
    "    sample_id = 1\n",
    "    \n",
    "    print(f\"Starting to collect {n_samples} samples...\")\n",
    "    \n",
    "    for file_key, target_samples in tqdm(sample_distribution.items()):\n",
    "        if target_samples == 0:\n",
    "            continue\n",
    "            \n",
    "        print(f\"Collecting {target_samples} samples from: {file_key}\")\n",
    "        \n",
    "        retries = 0\n",
    "        collected_from_file = 0\n",
    "        \n",
    "        while collected_from_file < target_samples and retries < max_retries:\n",
    "            try:\n",
    "                # Read the data file\n",
    "                df = read_data_from_s3(clean_bucket_name, file_key)\n",
    "                \n",
    "                if df.empty:\n",
    "                    print(f\"File {file_key} is empty, skipping...\")\n",
    "                    break\n",
    "                \n",
    "                # Calculate how many samples we still need from this file\n",
    "                remaining_samples = target_samples - collected_from_file\n",
    "                \n",
    "                # Get the samples\n",
    "                sampled_rows = get_random_samples(df, remaining_samples)\n",
    "                \n",
    "                if not sampled_rows.empty:\n",
    "                    # Add metadata to each sample\n",
    "                    for _, row in sampled_rows.iterrows():\n",
    "                        row_dict = row.to_dict()\n",
    "                        row_dict['_source_file'] = file_key\n",
    "                        row_dict['_sample_id'] = sample_id\n",
    "                        \n",
    "                        sampled_data.append(row_dict)\n",
    "                        sample_id += 1\n",
    "                        collected_from_file += 1\n",
    "                    \n",
    "                    print(f\"Successfully collected {len(sampled_rows)} samples from {file_key}\")\n",
    "                    break  # Successfully got samples, move to next file\n",
    "                else:\n",
    "                    print(f\"No samples collected from {file_key} on attempt {retries + 1}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error sampling from {file_key}: {e}\")\n",
    "            \n",
    "            retries += 1\n",
    "        \n",
    "        if collected_from_file < target_samples:\n",
    "            print(f\"Only collected {collected_from_file}/{target_samples} samples from {file_key}\")\n",
    "    \n",
    "    if len(sampled_data) == 0:\n",
    "        print(\"No valid samples collected\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    result_df = pd.DataFrame(sampled_data)\n",
    "    print(f\"Successfully collected {len(result_df)}/{n_samples} total samples\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in raw_clean_map.keys():\n",
    "    print(f\"Dataset: {key}\")\n",
    "    sample_df = sample_dataset(key, n_samples=n_samples, both=False)\n",
    "    sample_df.to_csv(f\"./outputs/super_benchmark/{key}_random_{n_samples}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "LOCATION = os.getenv(\"LOCATION\")\n",
    "\n",
    "print(PROJECT_ID, LOCATION)\n",
    "\n",
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "BUCKET_NAME = os.getenv(\"BUCKET_NAME\")\n",
    "bucket = storage_client.bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_jsonl_lines(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return sum(1 for line in f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an AI assistant tasked with analyzing the results of a text cleaning process for Hebrew language datasets.\n",
    "We are building an LLM for Hebrew, and our current phase involves cleaning training data using a specific set of rules implemented in our code. \n",
    "Your role is to evaluate the outcome of this cleaning process by evaluating a sample of a cleaned text.\n",
    "\n",
    "### Your Evaluation Task:\n",
    "For the provided clean_text (text after cleaning), you must perform a thorough analysis. Please assess the following:\n",
    "\n",
    "1. Overall Quality Score: Provide a general quality score for the clean_text after all the applied cleanings. [Score]\n",
    "2. Comments: Offer specific observations, insights, and notes regarding the cleaning outcome.\n",
    "3. Best Practice Recommendations: Suggest what appears to be most appropriate or effective, considering the cleaning rules.\n",
    "\n",
    "### Data Cleaning Rules to Verify (as implemented by our code):\n",
    "- HTML Escape Codes Replacement:\n",
    "-- Replaces HTML escape codes like &quot;, &#34; with \".\n",
    "-- Replaces &#39; with '.\n",
    "-- Note: These replacements are explicitly excluded from occurring within <MARKDOWN_TABLE> tags.\n",
    "\n",
    "- HTML Tags and CSS Removal:\n",
    "-- Removes <style>...</style> blocks (content and tags).\n",
    "-- Removes common HTML tags (e.g., <html>, <body>, <div>, <p>, <a>, <table>, <img>, headings, list items, etc.).\n",
    "-- Note: This removal is explicitly excluded from occurring within <MARKDOWN_TABLE> tags.\n",
    "\n",
    "- Newline and Whitespace Handling:\n",
    "-- Removes empty lines (lines containing only whitespace) at the start/end of the line.\n",
    "-- Replaces carriage returns (\\r or \\r\\n) with a single newline (\\n).\n",
    "-- Replaces more than 3 consecutive newlines (\\n{4,}) with exactly 3 newlines (\\n\\n\\n).\n",
    "-- Note: These operations are explicitly excluded from occurring within <MARKDOWN_TABLE> tags.\n",
    "\n",
    "- Multiple Space Normalization:\n",
    "-- Adjusts sequences of multiple spaces. While the code is complex, the info suggests it replaces multiple spaces to their \"lower 4 multiplier\" (e.g., 10 spaces to 8, 7 spaces to 4) up to a maximum of 16 spaces.\n",
    "-- It also strips leading/trailing spaces from lines and removes extra spaces between words (reducing multiple spaces to single spaces).\n",
    "-- Note: This normalization is explicitly excluded from occurring within <MARKDOWN_TABLE> tags.\n",
    "\n",
    "- PII Deletion (IP and Email Addresses):\n",
    "-- IP Addresses: Replaces IP addresses (e.g., 192.168.1.1) with <IP>, except for 127.0.0.1 and 0.0.0.0.\n",
    "-- Email Addresses: Replaces email addresses (e.g., user@example.com) with <EMAIL>.\n",
    "-- Note: These operations are explicitly excluded from occurring within <MARKDOWN_TABLE> tags.\n",
    "\n",
    "- Long Separator Line Removal:\n",
    "-- Removes entire lines that consist only of hyphens (-), equals signs (=), asterisks (*), underscores (_), tildes (~), or common bullet/block symbols (like •, ●, ■, ▪, ◆, ◦), followed by whitespace.\n",
    "-- Note: This removal is explicitly excluded from occurring within <MARKDOWN_TABLE> tags.\n",
    "\n",
    "- Hebrew Diacritics (Nikud) and Formatting Character Removal:\n",
    "-- Removes Hebrew diacritics (Nikud) from U+0591 to U+05C7, specifically excluding U+05BE (Makaf, the Hebrew hyphen).\n",
    "-- Replaces non-breaking spaces (\\u00A0) with regular spaces ().\n",
    "-- Removes unicode control characters for text directionality (Left-to-Right Mark, Right-to-Left Mark, Pop Directional Formatting).\n",
    "-- Note: These operations are explicitly excluded from occurring within <MARKDOWN_TABLE> tags.\n",
    "\n",
    "- Internal Markdown Table Tag Removal:\n",
    "-- Removes the internal <MARKDOWN_TABLE> and </MARKDOWN_TABLE> tags that are used to protect table content from other cleaning rules.\n",
    "-- Note: This is the final step for tables, implying that tables themselves are converted to markdown before these tags are removed, and protected during the process.\n",
    "\n",
    "For each Score use the following rubric:\n",
    "5 - Perfect cleaning. The text is cleaned fully according to the rules, is coherent and fully ready for LLM training.\n",
    "4 - Very good cleaning. The text is cleaned mostly according to the rules, is coherent with minor disruprencies, but is ready for LLM training.\n",
    "3 - Good cleaning. The text is cleaned according to the rules, but some rules have been ommited. The text is mostly coherent with some issues. The text is permissible for LLM training overall.\n",
    "2 - OK cleaning. The text is cleaned according to some rules. It's somewhat coherent but should be used with caution.\n",
    "1 - Unacceptable cleaning. The text is not cleaned according to the rules. The text is not coherent and is not acceptable for LLM training as is.\n",
    "\n",
    "You will be provided with a clean_text. Analyze it against these rules and provide your feedback in the requested format.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Provide the answer in a JSON format.\n",
    "Note: some samples are too large and were truncated at the end.\n",
    "\n",
    "Here is a sample to evaluate:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"score\": {\n",
    "                \"type\": \"INTEGER\",\n",
    "                \"minimum\": 1,\n",
    "                \"maximum\": 5,\n",
    "                \"description\": \"Rating score from 1 (unacceptable) to 5 (perfect)\"\n",
    "            },\n",
    "            \"comments\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Detailed feedback and observations\"\n",
    "                },\n",
    "            \"recommendations\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Actionable suggestions for improvement\"},\n",
    "        },\n",
    "    \"required\": [\"score\", \"comments\", \"recommendations\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = os.listdir(\"./outputs/benchmark\")\n",
    "print(len(keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_FILENAME = f\"overall_benchmark_evaluation.jsonl\"\n",
    "jsonl_fn = f\"./outputs/jsonls/{JSON_FILENAME}\"\n",
    "print(jsonl_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.5-flash\"\n",
    "thresh = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generationConfig = {\"temperature\": 0, \"maxOutputTokens\": 8192,\n",
    "                    \"response_schema\": response_schema, \"response_mime_type\":\"application/json\",\n",
    "                    \"thinkingConfig\": {\"thinkingBudget\": 1_000}}\n",
    "\n",
    "for key in tqdm(keys):\n",
    "    df = pd.read_csv(f\"./outputs/benchmark/{key}\")\n",
    "\n",
    "    with jsonlines.open(jsonl_fn, mode=\"a\") as writer:\n",
    "        \n",
    "        for row_index, row in df.iterrows():\n",
    "            # Extract data from CSV row\n",
    "            sample_id = row.get('_sample_id', f\"{row_index}\")\n",
    "            text = row.get('text', '')  # Adjust column name as needed\n",
    "\n",
    "            if len(text) > thresh:\n",
    "                print(f\"{row_index} of {key} is too large ({len(text)}). Truncating to {thresh}\")\n",
    "                text = text[:thresh]\n",
    "\n",
    "            gem_request = {\n",
    "                            \"custom_id\": f\"{key}_{sample_id}\",\n",
    "                            \"request\":{\n",
    "                                \"contents\": [\n",
    "                                    {\n",
    "                                        \"role\": \"user\", \n",
    "                                        \"parts\": [\n",
    "                                            {\"text\": prompt},\n",
    "                                            {\"text\": text}\n",
    "                                            ]\n",
    "                                    }\n",
    "                                ], \n",
    "                                \"generationConfig\":generationConfig,\n",
    "                                \"system_instruction\": {\"parts\": [{\"text\": system_prompt}]}\n",
    "                            }\n",
    "                            }\n",
    "            writer.write(gem_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_jsonl_lines(jsonl_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = bucket.blob(JSON_FILENAME)\n",
    "blob.upload_from_filename(jsonl_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA = f\"gs://{BUCKET_NAME}/{JSON_FILENAME}\"\n",
    "DEST_BUKCET_URI = \"gs://tau_ocr_results\"\n",
    "\n",
    "gcs_batch_job = client.batches.create(\n",
    "    model=MODEL_ID,\n",
    "    src=INPUT_DATA,\n",
    "    config=CreateBatchJobConfig(dest=DEST_BUKCET_URI),\n",
    ")\n",
    "gcs_batch_job.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_batch_job = client.batches.get(name=gcs_batch_job.name)\n",
    "# Refresh the job until complete\n",
    "while not gcs_batch_job.state in [\"JOB_STATE_SUCCEEDED\", \"JOB_STATE_FAILED\", \"JOB_STATE_UNEXECUTED\"]:\n",
    "    time.sleep(5)\n",
    "    gcs_batch_job = client.batches.get(name=gcs_batch_job.name)\n",
    "\n",
    "if gcs_batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "# Check if the job succeeds\n",
    "    print(\"Job succeeded!\")\n",
    "else:\n",
    "    print(f\"Job failed: {gcs_batch_job.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting batch results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_candidates(response):\n",
    "    if not isinstance(response, dict):\n",
    "        return {'candidates': []}\n",
    "    if 'candidates' not in response:\n",
    "        response['candidates'] = []\n",
    "    return response\n",
    "    \n",
    "def extract_response_text(response):\n",
    "    try:\n",
    "        return response[0]['text']\n",
    "    except (AttributeError, TypeError):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_response(json_string):\n",
    "    \"\"\"Parse JSON response string and extract key fields\"\"\"\n",
    "    try:\n",
    "        data = json.loads(json_string)\n",
    "        return pd.Series({\n",
    "            'score': data.get('score'),\n",
    "            'comments': data.get('comments'),\n",
    "            'recommendations': data.get('recommendations'),\n",
    "        })\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return pd.Series({\n",
    "            'score': None,\n",
    "            'comments': None,\n",
    "            'recommendations': None,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_request(row):\n",
    "    \"\"\"Safely extract text from nested request structure\"\"\"\n",
    "    try:\n",
    "        return row[\"request\"][\"contents\"][0][\"parts\"][1][\"text\"]\n",
    "    except (KeyError, IndexError, TypeError):\n",
    "        return None  # or return \"\" for empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem(\"gcs\")\n",
    "\n",
    "file_paths = fs.glob(f\"{gcs_batch_job.dest.gcs_uri}/*/predictions.jsonl\")\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gcs_batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    # Load the JSONL file into a DataFrame\n",
    "    df = pd.read_json(f\"gs://{file_paths[-1]}\", lines=True)\n",
    "    df['response'] = df['response'].apply(ensure_candidates)\n",
    "    df = df.join(pd.json_normalize(df[\"response\"], \"candidates\"))\n",
    "\n",
    "    df[\"response_text\"] = df[\"content.parts\"].apply(extract_response_text)\n",
    "\n",
    "    # Apply the parsing function to create new columns\n",
    "    df[['score', 'comments', 'recommendations']] = df['response_text'].apply(parse_json_response)\n",
    "\n",
    "    df[\"source\"] = df[\"custom_id\"].apply(lambda x: x.split(\"_random_\")[0])\n",
    "    df[\"sample_id\"] = df[\"custom_id\"].apply(lambda x: x.split(\"_\")[-1])\n",
    "\n",
    "    df[\"sample_text\"] = df.apply(extract_text_from_request, axis=1)\n",
    "    \n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_save = [\"sample_id\", \"sample_text\", \"score\", \"comments\", \"recommendations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_excel(df, output_excel_path):\n",
    "    \"\"\"Save DataFrame to Excel with separate sheets for each source\"\"\"\n",
    "    \n",
    "    with pd.ExcelWriter(output_excel_path, engine='xlsxwriter') as writer:\n",
    "        \n",
    "        # Group by source and save each group to a separate sheet\n",
    "        for source, group_df in df.groupby('source'):\n",
    "            \n",
    "            # Clean sheet name (Excel has restrictions)\n",
    "            clean_sheet_name = str(source).replace('/', '_').replace('\\\\', '_')[:31]\n",
    "            print(clean_sheet_name)\n",
    "\n",
    "            # Save to sheet\n",
    "            selected_df = group_df[columns_to_save].reset_index(drop=True)\n",
    "            selected_df.to_excel(writer, sheet_name=clean_sheet_name, index=False)\n",
    "            \n",
    "            print(f\"Saved {len(group_df)} records to sheet '{clean_sheet_name}'\")\n",
    "\n",
    "# Usage\n",
    "save_results_to_excel(df, './outputs/results/overall_benchmark_results.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
